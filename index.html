<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="aiping.liang s home">
<meta property="og:type" content="website">
<meta property="og:title" content="Aiping.LAP">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Aiping.LAP">
<meta property="og:description" content="aiping.liang s home">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Aiping.LAP">
<meta name="twitter:description" content="aiping.liang s home">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>Aiping.LAP</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Aiping.LAP</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">认真工作，快乐生活</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/22/Apache-Eagle-基本介绍/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/22/Apache-Eagle-基本介绍/" itemprop="url">Apache Eagle 基本介绍</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-21T14:23:35-08:00">
                2017-11-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/12/Ranger对HSFS实现控制管理的流程分析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/12/Ranger对HSFS实现控制管理的流程分析/" itemprop="url">Ranger对HDFS实现管理和控制的流程分析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-11T14:23:35-08:00">
                2017-10-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一、Apache-Ranger简介"><a href="#一、Apache-Ranger简介" class="headerlink" title="一、Apache Ranger简介"></a>一、Apache Ranger简介</h1><p>​       Apache Ranger提供一个集中式安全管理框架, 并解决授权和审计。它可以对Hadoop生态的组件如HDFS、Yarn、Hive、Hbase等进行细粒度的数据访问控制。通过操作Ranger控制台,管理员可以轻松的通过配置策略来控制用户访问权限。在2017年2月左右，Apache 软件基金会<a href="https://blogs.apache.org/foundation/entry/the-apache-software-foundation-announces3" target="_blank" rel="external">宣布</a>，Apache Ranger 已经成功地从孵化毕业，成为基金会的一个新的顶级项目</p>
<h1 id="二、Instrumentation-简介"><a href="#二、Instrumentation-简介" class="headerlink" title="二、Instrumentation 简介"></a>二、Instrumentation 简介</h1><p>​      利用 Java 代码，即 java.lang.instrument 做动态 Instrumentation 是 Java SE 5 的新特性，它把 Java 的 instrument 功能从本地代码中解放出来，使之可以用 Java 代码的方式解决问题。使用 Instrumentation，开发者可以构建一个独立于应用程序的代理程序（Agent），用来监测和协助运行在 JVM 上的程序，甚至能够替换和修改某些类的定义。有了这样的功能，开发者就可以实现更为灵活的运行时虚拟机监控和 Java 类操作了，这样的特性实际上提供了一种虚拟机级别支持的 AOP 实现方式，使得开发者无需对 JDK 做任何升级和改动，就可以实现某些 AOP 的功能。</p>
<h1 id="三、Ranger利用Instrumentation实现对HDFS的权限控制和管理"><a href="#三、Ranger利用Instrumentation实现对HDFS的权限控制和管理" class="headerlink" title="三、Ranger利用Instrumentation实现对HDFS的权限控制和管理"></a>三、Ranger利用Instrumentation实现对HDFS的权限控制和管理</h1><h2 id="1、策略的编辑和管理"><a href="#1、策略的编辑和管理" class="headerlink" title="1、策略的编辑和管理"></a>1、策略的编辑和管理</h2><p>​       Ranger对于用户策略的控制是通过控制台完成的，首先是通过ServiceREST的updatePolicy方法，</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@PUT</span></div><div class="line"><span class="meta">@Path</span>(<span class="string">"/services/&#123;id&#125;"</span>)</div><div class="line"><span class="meta">@Produces</span>(&#123; <span class="string">"application/json"</span>, <span class="string">"application/xml"</span> &#125;)</div><div class="line"><span class="meta">@PreAuthorize</span>(<span class="string">"@rangerPreAuthSecurityHandler.isAPIAccessible(\""</span> + RangerAPIList.UPDATE_SERVICE + <span class="string">"\")"</span>) <span class="comment">//这里会对界面的操作者鉴权，确定是否有相应的管理员权限编辑当前的策略</span></div><div class="line"><span class="function"><span class="keyword">public</span> RangerService <span class="title">updateService</span><span class="params">(RangerService service)</span> </span>&#123;</div><div class="line">     <span class="comment">//...省略上下文</span></div><div class="line">     </div><div class="line">      <span class="comment">//这里主要检查输入的参数中，是否缺少了必须要有的参数</span></div><div class="line">		RangerPolicyValidator validator = validatorFactory.getPolicyValidator(svcStore);</div><div class="line">		validator.validate(policy, Action.UPDATE, bizUtil.isAdmin());</div><div class="line"></div><div class="line">		ensureAdminAccess(policy.getService(), policy.getResources());</div><div class="line"></div><div class="line">		ret = svcStore.updatePolicy(policy);</div><div class="line">      <span class="comment">//...省略上下文</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>然后调用ServiceDBStore的updatePolicy的方法把更新的策略直接写入到数据库中：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> RangerPolicy <span class="title">updatePolicy</span><span class="params">(RangerPolicy policy)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">  <span class="comment">//...省略上下文</span></div><div class="line"></div><div class="line">   XXPolicy xxExisting = daoMgr.getXXPolicy().getById(policy.getId());</div><div class="line">   <span class="comment">//确定是一个存在的服务类型，比如HDFS</span></div><div class="line">   RangerPolicy existing = policyService.getPopulatedViewObject(xxExisting);</div><div class="line">   <span class="comment">//确定是已经创建的一个服务，例如Sandbox_hadoop</span></div><div class="line">   RangerService service = getServiceByName(policy.getService());</div><div class="line">   <span class="comment">//..这里省略了一些和上面类似的判断，都是在完成写入前不为空的一些逻辑检查</span></div><div class="line">   </div><div class="line">   <span class="comment">//这里是目前支持的四种类型的策略，多个策略组成了策略组，也就是一条policy</span></div><div class="line">   List&lt;RangerPolicyItem&gt; policyItems     = policy.getPolicyItems();</div><div class="line">   List&lt;RangerPolicyItem&gt; denyPolicyItems = policy.getDenyPolicyItems();</div><div class="line">   List&lt;RangerPolicyItem&gt; allowExceptions = policy.getAllowExceptions();</div><div class="line">   List&lt;RangerPolicyItem&gt; denyExceptions  = policy.getDenyExceptions();</div><div class="line">   <span class="comment">//..skip</span></div><div class="line">   <span class="comment">//写入相应的策略到数据库中</span></div><div class="line">   createNewPolicyItemsForPolicy(policy, newUpdPolicy, policyItems, xServiceDef, RangerPolicyItemEvaluator.POLICY_ITEM_TYPE_ALLOW);</div><div class="line"></div><div class="line">  <span class="comment">//很重要的一步，更新策略的版本号，方便后续根据版本号确定客户端执行最新的策略</span></div><div class="line">   handlePolicyUpdate(service, isTagVersionUpdateNeeded);</div><div class="line">   <span class="comment">//把上一步的策略存储起来，方便后续的回滚和查询</span></div><div class="line">   RangerPolicy updPolicy = policyService.getPopulatedViewObject(newUpdPolicy);</div><div class="line">   dataHistService.createObjectDataHistory(updPolicy, RangerDataHistService.ACTION_UPDATE);</div><div class="line">   <span class="comment">//所有的transaction Log (XXTrxLog)都是用于审计 </span></div><div class="line">   bizUtil.createTrxLog(trxLogList);</div><div class="line">   </div><div class="line">   <span class="keyword">return</span> updPolicy;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="2、触发策略的更新"><a href="#2、触发策略的更新" class="headerlink" title="2、触发策略的更新"></a>2、触发策略的更新</h2><p> Ranger服务端对于策略更新后，客户端获取更新后的策略的方式比较简单，就是通过一个异步的类PolicyRefresher来不断的检查策略的id是否发生了更新，然后就调用更新，默认更新的间隔是pollingIntervalMs   = 30 * 1000。更新主要调用的方法如下:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">loadPolicy</span><span class="params">()</span> </span>&#123;</div><div class="line"></div><div class="line">   RangerPerfTracer perf = <span class="keyword">null</span>;</div><div class="line"></div><div class="line">   <span class="keyword">try</span> &#123;</div><div class="line">      <span class="comment">//获取需要更新的策略列表</span></div><div class="line">      ServicePolicies svcPolicies = loadPolicyfromPolicyAdmin();</div><div class="line"></div><div class="line">      <span class="comment">//如果获取到的策略列表为空，则直接从上次的缓存中读取，否则把结果写入到缓存</span></div><div class="line">      <span class="keyword">if</span> (svcPolicies == <span class="keyword">null</span>) &#123;</div><div class="line">         <span class="keyword">if</span> (!policiesSetInPlugin) &#123;</div><div class="line">            svcPolicies = loadFromCache();</div><div class="line">         &#125;</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">         saveToCache(svcPolicies);</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">if</span> (svcPolicies != <span class="keyword">null</span>) &#123;</div><div class="line">        <span class="comment">//注意，这里的RangerHdfsPlugin是后续鉴权中使用的插件</span></div><div class="line">         plugIn.setPolicies(svcPolicies);</div><div class="line">         policiesSetInPlugin = <span class="keyword">true</span>;</div><div class="line">         <span class="comment">//这两部很重要，每次更新完之后，一定要设置上一次更新的时间和版本号</span></div><div class="line">         setLastActivationTimeInMillis(System.currentTimeMillis());</div><div class="line">         lastKnownVersion = svcPolicies.getPolicyVersion();</div><div class="line">      &#125; <span class="keyword">else</span> &#123;</div><div class="line">         <span class="keyword">if</span> (!policiesSetInPlugin &amp;&amp; !serviceDefSetInPlugin) &#123;</div><div class="line">            plugIn.setPolicies(<span class="keyword">null</span>);</div><div class="line">            serviceDefSetInPlugin = <span class="keyword">true</span>;</div><div class="line">         &#125;</div><div class="line">      &#125;</div><div class="line">   &#125; <span class="keyword">catch</span> (RangerServiceNotFoundException snfe) &#123;</div><div class="line">      <span class="comment">//一旦更新失败，则回滚所有的版本号到最原始的版本，然后下一次会下载所有的版本</span></div><div class="line">      <span class="keyword">if</span> (disableCacheIfServiceNotFound) &#123;</div><div class="line">         disableCache();</div><div class="line">         plugIn.setPolicies(<span class="keyword">null</span>);</div><div class="line">         setLastActivationTimeInMillis(System.currentTimeMillis());</div><div class="line">         lastKnownVersion = -<span class="number">1</span>;</div><div class="line">         serviceDefSetInPlugin = <span class="keyword">true</span>;</div><div class="line">      &#125;</div><div class="line">   &#125; <span class="keyword">catch</span> (Exception excp) &#123;</div><div class="line">      LOG.error(<span class="string">"Encountered unexpected exception, ignoring.."</span>, excp);</div><div class="line">   &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>获取需要更新的策略的逻辑如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">private</span> ServicePolicies <span class="title">loadPolicyfromPolicyAdmin</span><span class="params">()</span> <span class="keyword">throws</span> RangerServiceNotFoundException </span>&#123;</div><div class="line">  <span class="comment">//...省略一些无关内容</span></div><div class="line"></div><div class="line">   ServicePolicies svcPolicies = <span class="keyword">null</span>;</div><div class="line"></div><div class="line">   RangerPerfTracer perf = <span class="keyword">null</span>;</div><div class="line"></div><div class="line">   <span class="keyword">try</span> &#123;</div><div class="line">      <span class="comment">//根据上次更新完以后的版本号，以及上次更新的时间，来获取所有需要更新的内容</span></div><div class="line">      svcPolicies = rangerAdmin.getServicePoliciesIfUpdated(lastKnownVersion, lastActivationTimeInMillis);</div><div class="line"></div><div class="line">      <span class="keyword">boolean</span> isUpdated = svcPolicies != <span class="keyword">null</span>;</div><div class="line">     <span class="comment">//如果获取到的更新内容不为空，则把所有的更新内容</span></div><div class="line">      <span class="keyword">if</span>(isUpdated) &#123;</div><div class="line">         <span class="keyword">long</span> newVersion = svcPolicies.getPolicyVersion() == <span class="keyword">null</span> ? -<span class="number">1</span> : svcPolicies.getPolicyVersion().longValue();</div><div class="line"></div><div class="line">         <span class="keyword">if</span>(!StringUtils.equals(serviceName, svcPolicies.getServiceName())) &#123;</div><div class="line">            LOG.warn(<span class="string">"PolicyRefresher(serviceName="</span> + serviceName + <span class="string">"): ignoring unexpected serviceName '"</span> + svcPolicies.getServiceName() + <span class="string">"' in service-store"</span>);</div><div class="line"></div><div class="line">            svcPolicies.setServiceName(serviceName);</div><div class="line">         &#125;</div><div class="line">    <span class="keyword">return</span> svcPolicies;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="3、通过插件最终实现对hdfs的权限控制"><a href="#3、通过插件最终实现对hdfs的权限控制" class="headerlink" title="3、通过插件最终实现对hdfs的权限控制"></a>3、通过插件最终实现对hdfs的权限控制</h2><p>   在上文中，我们介绍了Instrumentation的技术，在ranger中，通过HadoopAuthClassTransformer类来实现注入，并且检查所有访问org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker方法的请求。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">byte</span>[] transform(ClassLoader aClassLoader, String aClassName, Class&lt;?&gt; aClassBeingRedefined, ProtectionDomain aProtectionDomain, <span class="keyword">byte</span>[] aClassFileBuffer) <span class="keyword">throws</span> IllegalClassFormatException &#123;</div><div class="line">   <span class="keyword">byte</span>[] ret = aClassFileBuffer;</div><div class="line"></div><div class="line">   <span class="comment">//拦截Hadoop原始的文件权限鉴权类FSPermissionChecker</span></div><div class="line">   <span class="keyword">if</span> (aClassName.equals(<span class="string">"org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker"</span>)) &#123;</div><div class="line">           <span class="keyword">byte</span>[] result = transformedClassByteCode;</div><div class="line">           <span class="keyword">if</span> (result == <span class="keyword">null</span>) &#123;</div><div class="line"></div><div class="line">         <span class="keyword">byte</span>[] injectedClassCode = injectFSPermissionCheckerHooks(aClassName);</div><div class="line"></div><div class="line">         <span class="keyword">if</span>(injectedClassCode != <span class="keyword">null</span>) &#123;</div><div class="line">                   <span class="comment">//通过HadoopAuthClassTransformer来实现控制</span></div><div class="line">                   <span class="keyword">synchronized</span> (HadoopAuthClassTransformer.class) &#123;</div><div class="line">                       result = transformedClassByteCode;</div><div class="line">                       <span class="keyword">if</span> (result == <span class="keyword">null</span>) &#123;</div><div class="line">                           transformedClassByteCode = result = injectedClassCode;</div><div class="line">                       &#125;</div><div class="line">                   &#125;</div><div class="line">               &#125;</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      <span class="keyword">if</span>(result != <span class="keyword">null</span>) &#123;</div><div class="line">         ret = result;</div><div class="line">      &#125;</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   <span class="keyword">return</span> ret;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>   HDFS默认使用的权限检查策略为FSPermissionChecker，这部分的检查是在namenode中，当用户在获取要访问数据的元数据的时候，进行检查，在org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker中</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">checkPermission</span><span class="params">(INodesInPath inodesInPath, <span class="keyword">boolean</span> doCheckOwner,</span></span></div><div class="line">  //...skip</div><div class="line">  //获取acl的控制对象,这里通过调用了RangerHdfsAuthorizer来生成一个内部类RangerAccessControlEnforcer对象</div><div class="line">  AccessControlEnforcer enforcer =</div><div class="line">      getAttributesProvider().<span class="title">getExternalAccessControlEnforcer</span><span class="params">(<span class="keyword">this</span>)</span>;</div><div class="line">   <span class="comment">//权限检查,这里调用了RangerAccessControlEnforcer来进行实际的检查</span></div><div class="line">  enforcer.checkPermission(fsOwner, supergroup, callerUgi, inodeAttrs, inodes,</div><div class="line">      pathByNameArr, snapshotId, path, ancestorIndex, doCheckOwner,</div><div class="line">      ancestorAccess, parentAccess, access, subAccess, ignoreEmptyDir);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在RangerHdfsAuthorizer中的内部类getExternalAccessControlEnforcer实现了INodeAttributeProvider的权限检查接口AccessControlEnforcer，替代了默认的FSPermissionChecker提供的checkPermission方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">checkPermission</span><span class="params">(String fsOwner, String superGroup, UserGroupInformation ugi,</span></span></div><div class="line">                     INodeAttributes[] inodeAttrs, INode[] inodes, <span class="keyword">byte</span>[][] pathByNameArr,</div><div class="line">                     <span class="keyword">int</span> snapshotId, String path, <span class="keyword">int</span> ancestorIndex, <span class="keyword">boolean</span> doCheckOwner,</div><div class="line">                     FsAction ancestorAccess, FsAction parentAccess, FsAction access,</div><div class="line">                     FsAction subAccess, <span class="keyword">boolean</span> ignoreEmptyDir) <span class="keyword">throws</span> AccessControlException &#123;</div><div class="line">  <span class="comment">//...skip</span></div><div class="line"></div><div class="line">   <span class="keyword">try</span> &#123;</div><div class="line">      <span class="comment">//先确定RangerHdfsPlugin不为空</span></div><div class="line">      <span class="keyword">if</span>(plugin != <span class="keyword">null</span> &amp;&amp; !ArrayUtils.isEmpty(inodes)) &#123;</div><div class="line">         <span class="keyword">if</span>(ancestorIndex &gt;= inodes.length) &#123;</div><div class="line">            ancestorIndex = inodes.length - <span class="number">1</span>;</div><div class="line">         &#125;</div><div class="line"></div><div class="line">         <span class="comment">//轮训所有的要访问的节点</span></div><div class="line">         <span class="keyword">for</span>(; ancestorIndex &gt;= <span class="number">0</span> &amp;&amp; inodes[ancestorIndex] == <span class="keyword">null</span>; ancestorIndex--);</div><div class="line">        <span class="comment">//...skip</span></div><div class="line"></div><div class="line">         <span class="comment">// 跳过白名单用户和owner的检查</span></div><div class="line">         <span class="comment">// 判断是否有父路径的权限</span></div><div class="line">         <span class="comment">//检查当前节点的路径权限</span></div><div class="line">         <span class="comment">//检查访问的类型</span></div><div class="line">         <span class="comment">//...经过一系列检查后，如果不抛出异常，则认为当前用户可以访问指定的资源</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>参考文章：</p>
<p><a href="https://www.ibm.com/developerworks/cn/java/j-lo-jse61/index.html" target="_blank" rel="external">https://www.ibm.com/developerworks/cn/java/j-lo-jse61/index.html</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/03/23/Hadoop开启LZO后的压测/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2016/03/23/Hadoop开启LZO后的压测/" itemprop="url">Hadoop开启LZO后的压测</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2016-03-22T19:54:05-08:00">
                2016-03-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="压测的方法"><a href="#压测的方法" class="headerlink" title="压测的方法"></a>压测的方法</h1><h2 id="通过Straming把压缩普通文件成为LZO的格式"><a href="#通过Straming把压缩普通文件成为LZO的格式" class="headerlink" title="通过Straming把压缩普通文件成为LZO的格式"></a>通过Straming把压缩普通文件成为LZO的格式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">hadoop jar /opt/hadoop/hadoop-2.6.0/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar`</div><div class="line">`-Dmapred.min.split.size=$[1024*1024*1024]`</div><div class="line">`-D mapred.output.compress=true`</div><div class="line">`-D mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec`</div><div class="line">`-D mapred.job.name=lap_lzo_compress`</div><div class="line">`-D mapred.reduce.tasks=0`</div><div class="line">`-mapper /bin/cat`</div><div class="line">`-input /raw_data/kafka/sourcedata/input/2016-03-02`</div><div class="line">`-output /raw_data/kafka/sourcedata/input/2016-03-02_tmp</div></pre></td></tr></table></figure>
<p>当然也可以map的时候做一些处理，然后在输出的时候压缩</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">`hadoop jar /opt/hadoop/hadoop-2.6.0/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar -Dmapred.min.split.size=$[1024*1024*1024]`</div><div class="line">`-D mapreduce.output.fileoutputformat.compress=true`</div><div class="line">`-D mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzopCodec`</div><div class="line">`-D mapred.job.name=lap_lzo_compress`</div><div class="line">`-D stream.non.zero.exit.is.failure=false`</div><div class="line">`-mapper /bin/cat`</div><div class="line">`-reducer /bin/cat`</div><div class="line">`-input /user/lap/SecurityAuth-hadoop.audit`</div><div class="line">`-output /user/lap/output4`</div></pre></td></tr></table></figure>
<h2 id="通过Streaming读取压缩文件"><a href="#通过Streaming读取压缩文件" class="headerlink" title="通过Streaming读取压缩文件"></a>通过Streaming读取压缩文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">hadoop jar /opt/hadoop/hadoop-2.6.0/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar -Dmapred.min.split.size=$[1024*1024*1024]`</div><div class="line"></div><div class="line">`-D mapred.job.name=lap_lzo_compress`</div><div class="line"></div><div class="line">`-D mapred.reduce.tasks=0`</div><div class="line"></div><div class="line">`-D stream.non.zero.exit.is.failure=false`</div><div class="line"></div><div class="line">`-inputformat com.hadoop.mapred.DeprecatedLzoTextInputFormat`</div><div class="line"></div><div class="line">`-mapper /bin/cat`</div><div class="line"></div><div class="line">`-input /user/lap/lzo_output/part-00000.lzo`</div><div class="line"></div><div class="line">`-output /user/lap/lzooutput/outtest2</div></pre></td></tr></table></figure>
<h2 id="MapReduce任务中，读取和输出压缩类型的文件"><a href="#MapReduce任务中，读取和输出压缩类型的文件" class="headerlink" title="MapReduce任务中，读取和输出压缩类型的文件"></a>MapReduce任务中，读取和输出压缩类型的文件</h2><p>首先，编译的时候需要获取到LZO的压缩的包。 在线上，这个包一般的存放路径为：    ${HADOOP_HOME}/share/hadoop/common/hadoop-lzo-0.4.20-SNAPSHOT.jar</p>
<p>写一个简单的java版的示例代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">lzoMapperOutput</span> </span>&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TokenizerMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span></span>&#123;</div><div class="line">        <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> IntWritable one = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</div><div class="line">        <span class="keyword">private</span> Text word = <span class="keyword">new</span> Text();</div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">            StringTokenizer itr = <span class="keyword">new</span> StringTokenizer(value.toString());</div><div class="line">            <span class="keyword">while</span> (itr.hasMoreTokens()) &#123;</div><div class="line">                word.set(itr.nextToken());</div><div class="line">                context.write(word, one);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">IntSumReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span> </span>&#123;</div><div class="line">        <span class="keyword">private</span> IntWritable result = <span class="keyword">new</span> IntWritable();</div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</div><div class="line">            <span class="keyword">int</span> sum = <span class="number">0</span>;</div><div class="line">            <span class="keyword">for</span> (IntWritable val : values) &#123;</div><div class="line">                sum += val.get();</div><div class="line">            &#125;</div><div class="line">            result.set(sum);</div><div class="line">            context.write(key, result);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String [] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</div><div class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line">        Job job = Job.getInstance(conf);</div><div class="line">        job.setJarByClass(lzoMapperOutput.class);</div><div class="line">        job.setMapperClass(TokenizerMapper.class);</div><div class="line">        job.setCombinerClass(IntSumReducer.class);</div><div class="line">        job.setReducerClass(IntSumReducer.class);</div><div class="line">        job.setOutputKeyClass(Text.class);</div><div class="line">        job.setOutputValueClass(IntWritable.class);</div><div class="line"></div><div class="line">        <span class="comment">//需要修改的地方，这里是重点</span></div><div class="line">        <span class="comment">//读取lzo类型的压缩的时候，进行以下的设置，或者通过conf的变量进行设置，或者通过Job提供的方法进行设置</span></div><div class="line">        <span class="comment">//注意conf的方式设置的话，需要在Job.getInstance(conf)初始化Job这一步之前</span></div><div class="line">        <span class="comment">// conf.setClass(MRJobConfig.INPUT_FORMAT_CLASS_ATTR, com.hadoop.mapreduce.LzoTextInputFormat.class,InputFormat.class);</span></div><div class="line">        job.setInputFormatClass(com.hadoop.mapreduce.LzoTextInputFormat.class);</div><div class="line">        <span class="comment">//也可以设置单独的shuffle为LZO，但是线上默认开启的是snappy压缩，所以建议不要覆盖</span></div><div class="line"></div><div class="line">        <span class="comment">//设置输出类型为LZO</span></div><div class="line">        FileOutputFormat.setCompressOutput(job, <span class="keyword">true</span>);</div><div class="line">        FileOutputFormat.setOutputCompressorClass(job, com.hadoop.compression.lzo.LzopCodec.class);</div><div class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</div><div class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</div><div class="line">        System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Hive使用LZO的压缩"><a href="#Hive使用LZO的压缩" class="headerlink" title="Hive使用LZO的压缩"></a>Hive使用LZO的压缩</h2><p>Hive表使用LZO压缩的时候，需要在创建表的时候把压缩属性更改成lzo的格式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">CREATE` `TABLE` ``raw_kafka_input_dt3`(`</div><div class="line">`   ```customer` string,`</div><div class="line">`   ```uid` ``int``)`</div><div class="line">`ROW FORMAT DELIMITED`</div><div class="line">`    ``FIELDS TERMINATED ``BY` `&apos;\t&apos;`</div><div class="line">`STORED ``AS` `INPUTFORMAT`</div><div class="line">`    ``&apos;com.hadoop.mapred.DeprecatedLzoTextInputFormat&apos;`</div><div class="line">`OUTPUTFORMAT`</div><div class="line">`     ``&apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;`</div></pre></td></tr></table></figure>
<p>注意，以上的inputformat在使用alter的时候会导致历史数据被删除，所以在更改属性之前，备份一下数据，也不建议使用这种方式申明。 仅仅把表的InputFormat修改为lzo的压缩还不够，因为压缩完后的文件，用传统的方式无法被切分，所以map就只能是一个，所以需要为压缩文件创建一个索引，这样才可以使用分布式的压缩，创建方法如下： hadoop jar ${HADOOP_HOME}/share/hadoop/common/hadoop-lzo-0.4.20-SNAPSHOT.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/库名/表名 创建完索引后，以上的任务就是分布式的了。</p>
<h2 id="HBase表使用LZO压缩"><a href="#HBase表使用LZO压缩" class="headerlink" title="HBase表使用LZO压缩"></a>HBase表使用LZO压缩</h2><p>修改表开启LZO压缩： alter ‘表名’,{NAME=&gt;’列簇名’,COMPRESSION=&gt;’LZO’} </p>
<h1 id="测试总结"><a href="#测试总结" class="headerlink" title="测试总结"></a>测试总结</h1><p>其实一直有种误解,就是以为lzo本身是支持分布式的,也就是支持压缩后的数据可以分片.我们提供给它分片的逻辑,由lzo本身控制.但看了Hadoop lzo源码才发现,lzo只是个压缩和解压缩的工具,如何分片,是由Hadoop lzo(Javad代码里)控制.具体的分片算法写得也很简单,就是在内存中开一块大大的缓存,默认是256K,缓存可以在通过io.compression.codec.lzo.buffersize参数指定.数据读入缓存(实际上要更小一些),如果缓存满了,则交给lzo压缩,获取压缩后的数据,同时在lzo文件中写入压缩前后的大小以及压缩后的数据.所以这里,一个分片,其实就是&lt;=缓存大小.也就是我们常常使用的lzo索引机制。</p>
<h1 id="遇到的问题以及解决办法"><a href="#遇到的问题以及解决办法" class="headerlink" title="遇到的问题以及解决办法"></a>遇到的问题以及解决办法</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">`Error: java.io.IOException: No LZO codec found, cannot run.`</div><div class="line"></div><div class="line">`at com.hadoop.mapred.DeprecatedLzoLineRecordReader.(DeprecatedLzoLineRecordReader.java:53)`</div><div class="line"></div><div class="line">`at com.hadoop.mapred.DeprecatedLzoTextInputFormat.getRecordReader(DeprecatedLzoTextInputFormat.java:156)`</div><div class="line"></div><div class="line">`at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.(MapTask.java:169)`</div><div class="line"></div><div class="line">`at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)`</div><div class="line"></div><div class="line">`at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)`</div><div class="line"></div><div class="line">`at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)`</div><div class="line"></div><div class="line">`at java.security.AccessController.doPrivileged(Native Method)`</div><div class="line"></div><div class="line">`at javax.security.auth.Subject.doAs(Subject.java:415)`</div><div class="line"></div><div class="line">`at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)`</div><div class="line"></div><div class="line">`at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)</div></pre></td></tr></table></figure>
<p>这个是因为在查找split的压缩类型的时候，返回空了，根本原因是没有在core-site.xml中正确配置集群中所有支持的压缩类型，key 是io.compression.codecs value是org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec</p>
<h1 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h1><p>1、如果集群中没有在默认配置core-site.xml中加载lzo的压缩的话，使用hadoop fs -text 无法读取lzo、snappy压缩后的文件 </p>
<p>2、快速的检测一个文件是否是lzo，且集群支持lzo的方法 hadoop org.apache.hadoop.io.compress.CompressionCodecFactory $filepath </p>
<p>3、对于所有的流式的任务，InputFormat的类型都应该是 com.hadoop.mapred.DeprecatedLzoTextInputFormat 对于普通的MR任务，InputFormat的类型是: com.hadoop.mapreduce.LzoTextInputFormat.class</p>
<h1 id="测试效果"><a href="#测试效果" class="headerlink" title="测试效果"></a>测试效果</h1><p>1、存储对比 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">359809200161 /raw_data/kafka/sourcedata/input/2016-03-02 </div><div class="line"></div><div class="line">133941395118 /raw_data/kafka/sourcedata/input/2016-03-02_lzo_tmp </div><div class="line"></div><div class="line">142546974490 /raw_data/kafka/sourcedata/input/2016-03-02_snappy_tmp </div><div class="line"></div><div class="line">54598916288 /raw_data/kafka/sourcedata/input/2016-03-02_tmp</div></pre></td></tr></table></figure>
<p>2、计算对比 </p>
<p>以线上的Input_Format为例，开启shuffle的snappy压缩后，效果明显：<br>从开启的压缩后的一段时间内，最多时候能到50%的性能优化， 从每天整体的性能看，大约能节约30%左右(以前每天总的执行时间(单位秒)6700W，到现在5000W左右)</p>
<h1 id="LZO加载"><a href="#LZO加载" class="headerlink" title="LZO加载"></a>LZO加载</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">`2016-03-17 14:53:16,192 DEBUG [main] com.hadoop.compression.lzo.GPLNativeCodeLoader: location: /native/Linux-amd64-64/lib`</div><div class="line">`2016-03-17 14:53:16,193 DEBUG [main] com.hadoop.compression.lzo.GPLNativeCodeLoader: temporary unpacked path: /opt/data7/yarn_dir/local/usercache/hadoop/appcache/application_1458035422051_0621/container_1458035422051_0621_01_000279/tmp/unpacked-7314810008241798412-libgplcompression.so`</div><div class="line">`2016-03-17 14:53:16,194 INFO [main] com.hadoop.compression.lzo.GPLNativeCodeLoader: Loaded native gpl library from the embedded binaries`</div><div class="line">`2016-03-17 14:53:16,197 INFO [main] com.hadoop.compression.lzo.LzoCodec: Successfully loaded &amp; initialized native-lzo library [hadoop-lzo rev d62701d4d05dfa6115bbaf8d9dff002df142e62d]</div></pre></td></tr></table></figure>
<p>`</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2015/10/27/HBase0-98以后的版本直接读取HFile文件/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2015/10/27/HBase0-98以后的版本直接读取HFile文件/" itemprop="url">HBase0.98以后的版本直接读取HFile文件</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2015-10-26T21:58:32-08:00">
                2015-10-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在一些特殊的场景下，我们可能需要直接去读取HBase的HFile文件来替代通过HBase Coonection的方式读取Hbase的表信息。</p>
<h1 id="主要的好处是"><a href="#主要的好处是" class="headerlink" title="主要的好处是"></a>主要的好处是</h1><p>1、可以减少对Server的压力</p>
<p>2、在一些特殊的场景下，如果服务出现问题，需要恢复数据的时候，只能通过这种方式</p>
<p>3、对于一些离线的读操作(保证这个时间段没有大量的写操作,且能容忍 一定的错误比率)，完全可以通过直接读HFIle来替代原来的方式</p>
<p>总的来说，这种方式比较适合在写入量极少(最好没有写入)的需求下，需要扫描全表，或者一定范围的需求</p>
<h1 id="实现方式"><a href="#实现方式" class="headerlink" title="实现方式"></a>实现方式</h1><p>1、为了降低读取成本，一般来说，在读取之前可以先进行一个compact，或者进行一个major compact</p>
<p>2、读取的代码如下</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//获取到HBase的根目录，这个不需要用户自己制定，只需要使用默认的配置生成即可</span></div><div class="line"></div><div class="line">Path rootDir = FSUtils.getRootDir(conf);</div><div class="line">LOG.info(<span class="string">"Get Root HBase dir "</span> + rootDir);</div><div class="line"></div><div class="line"><span class="comment">//制定要读取的表，然后获取表在hdfs上的绝对路径</span></div><div class="line">Path tablePath = FSUtils.getTableDir(rootDir, TableName.valueOf(tableName));</div><div class="line">LOG.info(<span class="string">"Get Table dir "</span> + tablePath);</div><div class="line">FileSystem fs = tablePath.getFileSystem(conf);</div><div class="line"></div><div class="line"><span class="comment">//获取当前表的所有region list，如果有特定的范围需求，可以通过一个filter，过滤需要的region</span></div><div class="line">List allRegionPath = FSUtils.getRegionDirs(fs, tablePath);</div><div class="line">List storeFilePaths = <span class="keyword">new</span> ArrayList();</div><div class="line"></div><div class="line"><span class="comment">//循环所有的region路径</span></div><div class="line"><span class="keyword">for</span> (Path regionPath: allRegionPath)&#123;</div><div class="line">	LOG.info(<span class="string">"Get region path "</span> + regionPath);</div><div class="line">	</div><div class="line">	<span class="comment">//指定过滤特殊的region过滤</span></div><div class="line">	PathFilter dirFilter = <span class="keyword">new</span> FSUtils.DirFilter(fs);</div><div class="line">    <span class="comment">//获取指定的region下所有的store file文件，如果进行major compact，这个文件个数为1  </span></div><div class="line">  	FileStatus[] familyStatus = fs.listStatus(regionPath,dirFilter);</div><div class="line">    <span class="comment">//循环所有的sotrefile</span></div><div class="line">	<span class="keyword">for</span> (FileStatus fileStatus: familyStatus)&#123;</div><div class="line">    	FileStatus[] storeFiles = fs.listStatus(fileStatus.getPath());</div><div class="line">        <span class="comment">//记录所有已经读取的rowkey</span></div><div class="line">        List rowKeyList = <span class="keyword">new</span> ArrayList(); </div><div class="line">        <span class="keyword">if</span> (HConstants.RECOVERED_EDITS_DIR.equals(fileStatus.getPath().getName()))&#123;</div><div class="line">             <span class="comment">// if it is wal edits file,skip!</span></div><div class="line">			<span class="keyword">continue</span>;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">        <span class="comment">//遍历sotrefile</span></div><div class="line">		<span class="keyword">for</span> (FileStatus storeFile: storeFiles)&#123;</div><div class="line">             <span class="comment">//默认的情况，在storefile的同一层有一些元数据的文件，这些文件不包含有要读取的数据，所以直接跳过，这个方法比较暴力，但是目前没有看到HBase提供了判断一个路径是否是storefile的方法，只能这么干</span></div><div class="line">			<span class="keyword">if</span> (!storeFile.isDirectory())&#123;</div><div class="line">				LOG.info(<span class="string">"get store file path:"</span>+storeFile.getPath().getName());</div><div class="line">				storeFilePaths.add(storeFile.getPath());</div><div class="line">                 <span class="comment">//构造HFile.Reader对象，读取storefile路径下的所有文件</span></div><div class="line">				HFile.Reader hfileReader = HFile.createReader(fs, storeFile.getPath(), <span class="keyword">new</span> CacheConfig(conf), conf);</div><div class="line">				<span class="comment">// Map fileInfo = hfileReader.loadFileInfo();</span></div><div class="line">                 </div><div class="line">                  <span class="comment">//获取scanner对象，开始读取</span></div><div class="line">				 HFileScanner hFileScanner = hfileReader.getScanner(<span class="keyword">false</span>, <span class="keyword">false</span>, <span class="keyword">false</span>);</div><div class="line">                  hFileScanner.seekTo();</div><div class="line"></div><div class="line">				<span class="keyword">do</span>&#123;</div><div class="line">                      <span class="comment">//读取文件中的一行记录</span></div><div class="line">					Cell cell = hFileScanner.getKeyValue();</div><div class="line">						</div><div class="line">                      <span class="comment">//获取当前行的所有信息，打印</span></div><div class="line">					String rowKey = <span class="keyword">new</span> String(CellUtil.cloneRow(cell));</div><div class="line">					<span class="keyword">if</span> (rowKeyList.contains(rowKey))&#123;</div><div class="line">						<span class="keyword">continue</span>;</div><div class="line">					&#125;</div><div class="line">					rowKeyList.add(rowKey);</div><div class="line">					LOG.info(<span class="string">"count "</span> + rowKeyList.size() + <span class="string">"RowName:"</span> + rowKey+ <span class="string">" "</span>);</div><div class="line">					LOG.debug(<span class="string">"Timetamp:"</span> + cell.getTimestamp() + <span class="string">" "</span>);</div><div class="line">					LOG.debug(<span class="string">"column Family:"</span> + <span class="keyword">new</span> String(CellUtil.cloneFamily(cell)) + <span class="string">" "</span>);</div><div class="line">					LOG.debug(<span class="string">"row Name:"</span> + <span class="keyword">new</span> String(CellUtil.cloneQualifier(cell)) + <span class="string">" "</span>);</div><div class="line">					LOG.debug(<span class="string">"value:"</span> + <span class="keyword">new</span> String(CellUtil.cloneValue(cell)) + <span class="string">" "</span>);</div><div class="line">				&#125; <span class="keyword">while</span>(hFileScanner.next());</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		LOG.info(<span class="string">"Store File "</span>+fileStatus.getPath()+<span class="string">" containes row "</span>+rowKeyList.size()+<span class="string">" lines!"</span>);</div><div class="line">	&#125;</div><div class="line">  &#125;</div><div class="line">&#125; <span class="keyword">catch</span> (Exception e)&#123;</div><div class="line">	e.printStackTrace();</div><div class="line">&#125;</div></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2015/10/24/HBase-Region的状态迁移流程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2015/10/24/HBase-Region的状态迁移流程/" itemprop="url">HBase Region的状态迁移流程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2015-10-23T21:33:34-08:00">
                2015-10-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="http://hbase.apache.org/images/region_states.png" alt="HBase Region状态迁移"></p>
<p>1、master控制把一个region从offline的状态迁移到opening 的状态，并且把该region分配到一个regionServer上。在到达rpc的重试最大次数(11次)之前，master会一直尝试发送打开region的请求。等到regionserver接收到request之后，就会开始打开region</p>
<p>2、如果master超过请求重试次数，master会停止启动region，然后把该region迁移到closing的状态，然后停止该region(注意，这里即使某个regnionserver已经开始打开该region，也会强制再停止的)。</p>
<p>3、等到regionserver打开region，它会一直notify master，直到master把该region的状态更改为open并且notifies所有的regionserver,到此为止，改region正式open</p>
<p>4、如果regionserver无法打开region，它会notify master，master会把region迁移到closed的状态，然后在其它的regionserver上重试打开</p>
<p>5、如果region无法打开，master会它迁移到failed_open的状态，直到hbase shell发出重新assign的命令，或者在该服务重启之前，对这个region将不再会有任何操作</p>
<p>6、master会把region从open到close的状态.regionserver会一直保持region直到收到close的状态</p>
<p>7、如果regionserver停止服务了，或者有NotServingRegionException异常的时候，master会负责把region迁移到offline的状态，然后把该region迁移到其它的reginserver</p>
<p>8、如果regionserver正常服务，但是在master重试多次后仍然不可达，master会把该region迁移到filed_close的状态，直到hbase shell发出重新assign的命令，或者在该服务重启之前，对这个region将不再会有任何操作</p>
<p>9、当regionserver得到关闭region的请求时候，它会关闭region，并且通知master，后者会把该region标记为closed的状态，并且会移动到其它的regionserver</p>
<p>10、在移动到其它的regionserver之前，master会先把一个closed的region标记为offline的状态</p>
<p>11、当一个regionserver准备split一个region，它会通知master。master会把将要切分的reigon从open状态移动到splitting状态，然后把切分出来的两个region初始化成spliting_new的状态</p>
<p>12、通知了master以后，regionserver开始切分region.如果返回超时，regionserver会再次通知master来更新hbase:meta表。在master确定split结束之前，master不会更新region的状态。如果split切分成功后，正在被切分的region会从splitting状态切换到split状态，然后被切分的两个新region会从splitting_new切换到open状态</p>
<p>13、如果切分失败，被切分的region从splitting状态切换回滚到open状态，然后两个新切出来的region被从splitting_new切换到offline的状态</p>
<p>14、如果regionserver要合并两个region，会先通知master。master会把要合并的两个region从open切换到merging的状态，然后增加一个新的region到regionserver，这个新的region是merging_new的状态</p>
<p>15、通知完master之后，regionserver开始合并region，一旦超时，regionserver会再次通知master来更新meta表。如果合并成功，两个正在合并的region会从merging迁移到merged状态，而新的region会从merging_new到open的状态</p>
<p>16、如果合并失败，两个需要合并的region会从merging迁移到open的状态，然后准备要合并到的目的region会从merging_new到offline的状态</p>
<p>17、对于一个region处于failed_open或者failed_close状态，master会尝试关闭这些节点.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2015/10/19/如何自定义一个HBase命令行的命令/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2015/10/19/如何自定义一个HBase命令行的命令/" itemprop="url">如何自定义一个HBase命令行的命令</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2015-10-18T21:06:06-08:00">
                2015-10-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在一些场景下，我们会大量频繁的执行一些HBase的代码，如果每次都通过jar命令的方式，会很麻烦，需要指定很多的系统库的路径，比如在前文中我们开发了一个单标balance的工具，想要把这个命令方便执行，那么最好的方法是把这个工具加入到命令行中，让这件事情变得傻瓜化。</p>
<h1 id="开发目的"><a href="#开发目的" class="headerlink" title="开发目的"></a>开发目的</h1><p>完成对单表的迅速balance小工具的命令行开发</p>
<h1 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h1><h2 id="1、自定义相应的命令行接口"><a href="#1、自定义相应的命令行接口" class="headerlink" title="1、自定义相应的命令行接口"></a>1、自定义相应的命令行接口</h2><figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">module</span> <span class="title">Shell</span></span></div><div class="line">  <span class="class"><span class="keyword">module</span> <span class="title">Commands</span></span></div><div class="line">    <span class="class"><span class="keyword">class</span> <span class="title">STableBalancer</span> &lt; Command</span></div><div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">help</span></span></div><div class="line">        <span class="keyword">return</span> <span class="string">&lt;&lt;-EOF</span></div><div class="line">BaiFenDian table balancer. Returns true if balancer ran and was able to</div><div class="line">tell the region servers to unassign all the regions to balance  (the re-assignment itself is async). </div><div class="line">Otherwise false (Will not run if regions in transition). </div><div class="line">Author: aiping.liang</div><div class="line">Describe the named table. For example:</div><div class="line">    hbase&gt; bfd_table_balance 't1'</div><div class="line">EOF</div><div class="line">      <span class="keyword">end</span> </div><div class="line">      <span class="function"><span class="keyword">def</span> <span class="title">command</span><span class="params">(table_name)</span></span></div><div class="line">        format_simple_command <span class="keyword">do</span></div><div class="line">          formatter.row([admin.tableBalancer(table_name.to_java_bytes)? <span class="string">"true"</span>: <span class="string">"false"</span>]) </div><div class="line">        <span class="keyword">end</span> </div><div class="line">      <span class="keyword">end</span> </div><div class="line">    <span class="keyword">end</span> </div><div class="line">  <span class="keyword">end</span> </div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<p>这是一个简单的ruby脚本，稍微解释下，formatter.header、formatter.row、formatter.footer都是自定义的一些输出到命令行的一些命令，这里Class的方法一定要注意格式，因为Class的名字代表着命令行的名称</p>
<h2 id="2、定义自己操作对应的方法"><a href="#2、定义自己操作对应的方法" class="headerlink" title="2、定义自己操作对应的方法"></a>2、定义自己操作对应的方法</h2><p>增加改命令对应的实际的操作到HBase的admin命令行中</p>
<p>修改hbase-shell/src/main/ruby/hbase/admin.rb，如下</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#----------------------------------------------------------------------------------------------</span></div><div class="line"><span class="comment"># Requests a table balance</span></div><div class="line"><span class="comment"># Returns true if balancer ran</span></div><div class="line"><span class="comment"># Add by aiping.liang                                                                                                             </span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tableBalancer</span><span class="params">(table_or_region_name)</span></span></div><div class="line">   @admin.tableBalancer(table_or_region_name)</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
<p>​    注意，这里的@admin就是HBaseAdmin的对象</p>
<h2 id="3、实现相应的接口"><a href="#3、实现相应的接口" class="headerlink" title="3、实现相应的接口"></a>3、实现相应的接口</h2><p>   修改hbase-client/src/main/java/org/apache/hadoop/hbase/client/Admin.java接口，增加相应的接口方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">tableBalancer</span><span class="params">(<span class="keyword">final</span> <span class="keyword">byte</span>[] tableName)</span> <span class="keyword">throws</span> IOException</span>;</div><div class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">tableBalancer</span><span class="params">(<span class="keyword">final</span> TableName tableName)</span> <span class="keyword">throws</span>  IOException</span>;</div></pre></td></tr></table></figure>
<h2 id="4、实现该接口"><a href="#4、实现该接口" class="headerlink" title="4、实现该接口"></a>4、实现该接口</h2><p>   客户端默认的接口只有一个HBaseAdmin.java，实现如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">tableBalancer</span><span class="params">(<span class="keyword">final</span> <span class="keyword">byte</span>[] tableName)</span> <span class="keyword">throws</span> IOException</span>&#123; </div><div class="line">    <span class="keyword">return</span> tableBalancer(TableName.valueOf(tableName));</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">tableBalancer</span><span class="params">(<span class="keyword">final</span> TableName tableName)</span><span class="keyword">throws</span> TableNotFoundException,IOException</span>&#123;</div><div class="line">          <span class="keyword">if</span> (!tableExists(tableName))&#123;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> TableNotFoundException(tableName);</div><div class="line">    &#125;   </div><div class="line">       <span class="keyword">return</span> <span class="keyword">true</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>   这一步需要实现自己的具体方法，具体的table balance的算法见上一篇文章</p>
<h2 id="5、-在命令行增加相应的命令"><a href="#5、-在命令行增加相应的命令" class="headerlink" title="5、 在命令行增加相应的命令"></a>5、 在命令行增加相应的命令</h2><p>  修改hbase-shell/src/main/ruby/shell.rb文件</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Shell.load_command_group(  <span class="string">'ddl'</span>,  <span class="symbol">:full_name</span> =&gt; <span class="string">'TABLES MANAGEMENT COMMANDS'</span>,  <span class="symbol">:commands</span> =&gt; <span class="string">%w[     alter    bfd_table_balancer</span></div></pre></td></tr></table></figure>
<p>​    这一步是在命令行能自动tap，且使用自定义命令 必要的</p>
<h2 id="6、部署"><a href="#6、部署" class="headerlink" title="6、部署"></a>6、部署</h2><p>​    部署分两部分：</p>
<p>​     a、把更改后的包编译</p>
<p>​     b、 把hbase-client下相应的jar包，以及hbase-shell下修改的rb文件放到HBASE_HOME相应的路径下</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2015/10/16/实现一个HBase单表balance的工具/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2015/10/16/实现一个HBase单表balance的工具/" itemprop="url">实现一个HBase单表balance的工具</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2015-10-15T22:41:06-08:00">
                2015-10-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="需求-问题"><a href="#需求-问题" class="headerlink" title="需求+问题"></a>需求+问题</h1><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>所有人都知道HBase中表的region的分布对于读写性能影响很大，如果所有的region集中到一台机器上，势必会导致读写都很慢，所以我们希望HBase的Region能够尽量均匀的分布在集群的所有机器上。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>HBase自己有Balance的机智，主要是靠HMaster通过定期的检查所有机器的region分布来调整单个节点上的region个数，尽量保证整个集群的reigon在每台机器上的数量是接近的。</p>
<p>但是在一些特殊的场景下，会导致所有的reigon，或者很大部分的region分不到一台机器上</p>
<p>1、通过自定的rowkey分割方法对表进行预分配</p>
<p>2、HBase加载过程中，如果某一个region出问题，导致加载异常</p>
<p>3、表的个数小于集群机器数量，会导致一些机器个数多余其它机器</p>
<h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><p>主要的思路是，通过HMaster的管理机制，通过读取表的region个数，以及按照机器轮训的方式来对特定表负载更多region的机器迁移出一些reigon到负载低的机器上。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//初始化系统配置，读取要balance的表</span></div><div class="line">Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line">String tableName = args[<span class="number">0</span>];</div><div class="line"></div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">//创建HBase的连接</span></div><div class="line">    <span class="comment">//98以后最新的连接池的方法</span></div><div class="line">    Connection conn = ConnectionFactory.createConnection(conf);</div><div class="line"></div><div class="line">    <span class="comment">//存放所有的均衡表所有的信息，结构为region所在的机器HostName, 需要迁移的serverName对象，可以迁移的region的列别哦</span></div><div class="line">    Map regionCountCalculator = <span class="keyword">new</span> HashMap&lt;&lt;Integer&gt;&gt;();</div><div class="line"></div><div class="line">    <span class="comment">//创建HMaster的Admin对象,然后获取服务信息</span></div><div class="line">    Admin admin = conn.getAdmin();</div><div class="line">    ClusterStatus clusterStatus = admin.getClusterStatus();</div><div class="line">    Collection servers = clusterStatus.getServers();</div><div class="line"></div><div class="line">    <span class="keyword">int</span> serverTotal = servers.size();</div><div class="line">    <span class="keyword">for</span> (ServerName server:servers)&#123;</div><div class="line">        LOG.info(<span class="string">"Get Servername:"</span>+ server.getHostname());</div><div class="line">        <span class="comment">//初始化所有的RS存储信息</span></div><div class="line">        regionCountCalculator.put(server.getHostname(),  <span class="keyword">new</span> Pair&lt;&gt;(server, <span class="keyword">new</span> HashSet()));</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// calculator the regionserver -- servername -- regionInfo</span></div><div class="line"></div><div class="line">    RegionLocator regionLocator = conn.getRegionLocator(TableName.valueOf(tableName));</div><div class="line">    Pair regionPari = regionLocator.getStartEndKeys();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; regionPari.getFirst().length;i++)&#123;</div><div class="line">        LOG.info(<span class="string">"Get Start Key"</span> + <span class="keyword">new</span> String(CellUtil.cloneRow(CellUtil.createCell(regionPari.getFirst()[i]))));</div><div class="line">        HRegionLocation regionLocat = regionLocator.getRegionLocation(regionPari.getFirst()[i], <span class="keyword">false</span>);</div><div class="line">        String regionHostName = regionLocat.getHostname();</div><div class="line">        <span class="comment">//初始化regionCountCalculator对象中的servername对象，并且把所有的该节点上的region初始化</span></div><div class="line">        regionCountCalculator.get(regionHostName).getSecond().add(regionLocat.getRegionInfo());</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//按照机器数，获取平均每台机器上应该存储的region的数量</span></div><div class="line">    <span class="keyword">int</span> avgRegionNum = regionPari.getFirst().length/serverTotal;</div><div class="line">    <span class="comment">//初始化两个小map，类似，分别存放可以移入和移出的region信息</span></div><div class="line">    Map moveOutMap = <span class="keyword">new</span> HashMap();</div><div class="line">    Map moveInMap = <span class="keyword">new</span> HashMap();</div><div class="line"></div><div class="line">    <span class="comment">//第一遍轮训获取所有的可以移入和移除的region信息</span></div><div class="line">    <span class="keyword">for</span> (String hostKey: regionCountCalculator.keySet())&#123;</div><div class="line">        <span class="comment">//获取当前机器上已经分配的region的个数</span></div><div class="line">        <span class="keyword">int</span> regionCount = regionCountCalculator.get(hostKey).getSecond().size();</div><div class="line">        LOG.info(<span class="string">"Get region count:"</span>+regionCount+<span class="string">" and avgRegionNum:"</span>+avgRegionNum);</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (regionCount &lt; avgRegionNum)&#123;</div><div class="line">            <span class="comment">//如果小于平均值，则表明该server可以写入region</span></div><div class="line">            moveInMap.put(regionCountCalculator.get(hostKey).getFirst(), avgRegionNum-regionCount+<span class="number">1</span>);</div><div class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (regionCount &gt; avgRegionNum+<span class="number">1</span>)&#123;</div><div class="line">            <span class="comment">//相反，如果大于平均值，则表明该server可移出region</span></div><div class="line">            moveOutMap.put(regionCountCalculator.get(hostKey).getFirst(), regionCount-avgRegionNum);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    LOG.info(<span class="string">"Get in length:"</span>+moveInMap.size()+<span class="string">" and out length:"</span>+moveOutMap.size());</div><div class="line">    <span class="comment">//第二遍轮训开始迁移所有的可以移除和移入的region</span></div><div class="line">    <span class="comment">//轮寻所有需要移除的队列</span></div><div class="line">    <span class="keyword">for</span> (ServerName outHost: moveOutMap.keySet())&#123;</div><div class="line">        <span class="keyword">boolean</span> needMoveOut = <span class="keyword">true</span>;</div><div class="line">        <span class="comment">//记录已经移出的个数，如果超过可移除的最大值，则停止移除</span></div><div class="line">        <span class="keyword">int</span> moveCount = moveOutMap.get(outHost);</div><div class="line"></div><div class="line">        <span class="keyword">for</span> (HRegionInfo moveOutRegionInfo:regionCountCalculator.get(outHost.getHostname()).getSecond())&#123;</div><div class="line">            <span class="keyword">if</span> (needMoveOut)&#123;</div><div class="line">                ServerName inHost = <span class="keyword">null</span>;</div><div class="line">                <span class="keyword">int</span> inCount = <span class="number">0</span>;</div><div class="line">                <span class="comment">//轮寻所有的可以移入的队列，获取可以移入的机器</span></div><div class="line">                <span class="keyword">for</span> (ServerName moveInHost: moveInMap.keySet())&#123;</div><div class="line">                    inCount = moveInMap.get(moveInHost);</div><div class="line">                    LOG.info(<span class="string">"Host in :"</span>+moveInHost+<span class="string">" can be in :"</span>+inCount);</div><div class="line">                    <span class="keyword">if</span> (inCount &gt; <span class="number">0</span> )&#123;</div><div class="line">                        inHost = moveInHost;</div><div class="line">                    &#125;</div><div class="line">                &#125;</div><div class="line">                LOG.info(<span class="string">"Move Region "</span>+moveOutRegionInfo.getRegionNameAsString()+<span class="string">" from "</span>+outHost.getHostname() + <span class="string">" to "</span>+inHost.getHostname());</div><div class="line">                <span class="keyword">int</span> outCount = <span class="number">0</span>;</div><div class="line">                <span class="keyword">if</span> (moveOutRegionInfo != <span class="keyword">null</span>)&#123;</div><div class="line">                    <span class="comment">//调用服务管理接口，把region移动到相应的机器</span></div><div class="line">                    admin.move(moveOutRegionInfo.getEncodedNameAsBytes(), Bytes.toBytes(inHost.toString()));</div><div class="line">                    LOG.info(<span class="string">"Move from: "</span>+moveOutRegionInfo.getRegionNameAsString()+<span class="string">" to:"</span>+inHost.toString());</div><div class="line"></div><div class="line">                &#125;</div><div class="line">                moveInMap.remove(inHost);</div><div class="line">                <span class="keyword">if</span> (--inCount &gt; <span class="number">0</span>)&#123;</div><div class="line">                    <span class="comment">//如果可移入的机器已经移入最大值，则从map中移除</span></div><div class="line">                    moveInMap.put(inHost,inCount);</div><div class="line">                &#125;</div><div class="line"></div><div class="line">                <span class="keyword">if</span> (--moveCount == <span class="number">0</span>)&#123;</div><div class="line">                    needMoveOut = <span class="keyword">false</span>;</div><div class="line">                    <span class="comment">//如果需要移出的机器已经把所有的region都移出，则从map中删除</span></div><div class="line">                    outCount++;</div><div class="line">                &#125;</div><div class="line">            &#125; <span class="keyword">else</span> &#123;</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        LOG.info(<span class="string">"Get out host length:"</span>+moveOutMap.size());</div><div class="line">    &#125;</div><div class="line">&#125;<span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">    <span class="comment">//handle exception</span></div><div class="line">    e.printStackTrace();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>通过以上一个小小的脚本，解决了我们的问题。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2015/04/07/记录一次HBase的问题排查/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2015/04/07/记录一次HBase的问题排查/" itemprop="url">记录一次HBase的问题排查</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2015-04-06T19:22:14-08:00">
                2015-04-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h1><p>所有的region被卡死，无法打开，访问缓慢</p>
<h1 id="问题跟踪"><a href="#问题跟踪" class="headerlink" title="问题跟踪"></a>问题跟踪</h1><p>老实说，这个现象很常见，有时候是因为压力太大，有时候因为别的原因，所以我这里的跟进方法是一般的一个方法，但是不能解决所有问题，具体问题需要具体分析。</p>
<h1 id="1、找到有问题的region"><a href="#1、找到有问题的region" class="headerlink" title="1、找到有问题的region"></a>1、找到有问题的region</h1><p>这个方法比较多，一般来说可以根据请求慢的key，以及通过管理页面，定位到确定的出问题的Region，一个形如0313e59d82326803c21470f5c25fef79的串</p>
<h2 id="2、查找出问题region所在的物理机"><a href="#2、查找出问题region所在的物理机" class="headerlink" title="2、查找出问题region所在的物理机"></a>2、查找出问题region所在的物理机</h2><p>从管理页面上看到的region分布不一定是正确的分布，因为中间可能发生过迁移，所以最稳妥的方法是去Master上搜索所有和这个region相关的日志，查找出问题时间段这个region应该所处的节点，例如发现如下日志</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="number">2015</span>-<span class="number">04</span>-<span class="number">05</span> <span class="number">14</span>:<span class="number">08</span>:<span class="number">49</span>,<span class="number">257</span> INFO org.apache.hadoop.hbase.master.AssignmentManager: Server bjlg-<span class="number">48</span>p28-hadoop18.bfdabc.com,<span class="number">60020</span>,<span class="number">1420985184893</span> returned java.net.SocketTimeoutException: Call to [bjlg-<span class="number">48</span>p28-hadoop18.bfdabc.com/<span class="number">192.168</span>.48.28:<span class="number">60020</span>](http:<span class="comment">//bjlg-48p28-hadoop18.bfdabc.com/192.168.48.28:60020) failed on socket timeout exception: java.net.SocketTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=[bjlg-48p28-hadoop18.bfdabc.com/192.168.48.28:60020](http://bjlg-48p28-hadoop18.bfdabc.com/192.168.48.28:60020)] for 0313e59d82326803c21470f5c25fef79</span></div></pre></td></tr></table></figure>
<h2 id="3、到相关的节点上进一步排查"><a href="#3、到相关的节点上进一步排查" class="headerlink" title="3、到相关的节点上进一步排查"></a>3、到相关的节点上进一步排查</h2><p>一般来说，先从机器的load，或者磁盘使用率、cpu使用率、内存状况等开始入手排查，这些基本的我都跳过了，直接说一个我们的问题：查看regionserver的进程，发现了一个死锁</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"># Found one Java-level deadlock:</div><div class="line"></div><div class="line">"IPC Server handler 39 on 60020":</div><div class="line">waiting to lock monitor 0x00002aaac9c5b838 (object 0x000000058cacc9b8, a java.lang.Object),</div><div class="line">which is held by "regionserver60020.logRoller"</div><div class="line">"regionserver60020.logRoller":</div><div class="line">waiting to lock monitor 0x00002aaac981f4d0 (object 0x000000058cacc9d0, a java.lang.Object),</div><div class="line">which is held by "IPC Server handler 37 on 60020"</div><div class="line">"IPC Server handler 37 on 60020":</div><div class="line">waiting to lock monitor 0x00002aaac9c5b838 (object 0x000000058cacc9b8, a java.lang.Object),</div><div class="line">which is held by "regionserver60020.logRoller"</div></pre></td></tr></table></figure>
<h2 id="4、跟踪死锁的进程："><a href="#4、跟踪死锁的进程：" class="headerlink" title="4、跟踪死锁的进程："></a>4、跟踪死锁的进程：</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="string">"IPC Server handler 39 on 60020"</span>:</div><div class="line">at org.apache.hadoop.hbase.regionserver.wal.HLog.append(HLog.[java:<span class="number">1122</span>](http:<span class="comment">//java:1122/))</span></div><div class="line">\- waiting to lock &lt;<span class="number">0x000000058cacc9b8</span>&gt; (a java.lang.Object)</div><div class="line">at org.apache.hadoop.hbase.regionserver.wal.HLog.appendNoSync(HLog.[java:<span class="number">1168</span>](http:<span class="comment">//java:1168/))</span></div><div class="line">at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchMutation(HRegion.[java:<span class="number">2225</span>](http:<span class="comment">//java:2225/))</span></div><div class="line">at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.[java:<span class="number">1973</span>](http:<span class="comment">//java:1973/))</span></div><div class="line">at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.[java:<span class="number">3492</span>](http:<span class="comment">//java:3492/))</span></div><div class="line">at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)</div><div class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class="number">25</span>)</div><div class="line">at java.lang.reflect.Method.invoke(Method.[java:<span class="number">597</span>](http:<span class="comment">//java:597/))</span></div><div class="line">at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.[java:<span class="number">364</span>](http:<span class="comment">//java:364/))</span></div><div class="line">at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.[java:<span class="number">1426</span>](http:<span class="comment">//java:1426/))</span></div><div class="line"><span class="string">"regionserver60020.logRoller"</span>:</div><div class="line">at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.[java:<span class="number">1302</span>](http:<span class="comment">//java:1302/))</span></div><div class="line">\- waiting to lock &lt;<span class="number">0x000000058cacc9d0</span>&gt; (a java.lang.Object)</div><div class="line">at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.[java:<span class="number">1280</span>](http:<span class="comment">//java:1280/))</span></div><div class="line">at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.[java:<span class="number">1433</span>](http:<span class="comment">//java:1433/))</span></div><div class="line">at org.apache.hadoop.hbase.regionserver.wal.HLog.cleanupCurrentWriter(HLog.[java:<span class="number">866</span>](http:<span class="comment">//java:866/))</span></div><div class="line">at org.apache.hadoop.hbase.regionserver.wal.HLog.rollWriter(HLog.[java:<span class="number">640</span>](http:<span class="comment">//java:640/))</span></div><div class="line">\- locked &lt;<span class="number">0x000000058cacc9b8</span>&gt; (a java.lang.Object)</div><div class="line">at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:<span class="number">94</span>)</div><div class="line">at java.lang.Thread.run(Thread.[java:<span class="number">662</span>](http:<span class="comment">//java:662/))</span></div><div class="line"><span class="string">"IPC Server handler 37 on 60020"</span>:</div><div class="line">at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.[java:<span class="number">1311</span>](http:<span class="comment">//java:1311/))</span></div><div class="line">\- waiting to lock &lt;<span class="number">0x000000058cacc9b8</span>&gt; (a java.lang.Object)</div><div class="line">\- locked &lt;<span class="number">0x000000058cacc9d0</span>&gt; (a java.lang.Object)</div><div class="line">at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.[java:<span class="number">1437</span>](http:<span class="comment">//java:1437/))</span></div><div class="line"></div><div class="line">at org.apache.hadoop.hbase.regionserver.HRegion.syncOrDefer(HRegion.[java:<span class="number">5205</span>](http:<span class="comment">//java:5205/))</span></div><div class="line"></div><div class="line">at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchMutation(HRegion.[java:<span class="number">2245</span>](http:<span class="comment">//java:2245/))</span></div><div class="line">at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.[java:<span class="number">1973</span>](http:<span class="comment">//java:1973/))</span></div><div class="line">at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.[java:<span class="number">3492</span>](http:<span class="comment">//java:3492/))</span></div><div class="line">at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)</div><div class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class="number">25</span>)</div><div class="line">at java.lang.reflect.Method.invoke(Method.[java:<span class="number">597</span>](http:<span class="comment">//java:597/))</span></div><div class="line">at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.[java:<span class="number">364</span>](http:<span class="comment">//java:364/))</span></div><div class="line">at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.[java:<span class="number">1426</span>](http:<span class="comment">//java:1426/))</span></div></pre></td></tr></table></figure>
<h2 id="5、分析"><a href="#5、分析" class="headerlink" title="5、分析"></a>5、分析</h2><p>最后确定这是logRoller和HLog.syncer两个线程发生了死锁。</p>
<p>在HBase，采用WAL(Write Ahead Log)的方式来保证数据不丢失。一个Region Server对应一个HLog，任何数据都是先写HLog，然后才真正的写MemStore。HLog文件有个轮转的过程，当达到默认大小（一般是block size * 0.95）就会写一个新的HLog文件，logRoller线程就是做这个事情的。而buffer中的数据每隔一秒钟会写一次HLog文件，这个是HLog.syncer线程负责的。logRoller和HLog.syncer进程不能同时进行，不然会导致数据问题，于是就需要通过锁来进行互斥了。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">long</span> <span class="title">append</span><span class="params">(HRegionInfo info, <span class="keyword">byte</span> [] tableName, WALEdit edits, UUID clusterId,</span></span></div><div class="line"></div><div class="line"><span class="keyword">final</span> <span class="keyword">long</span> now, HTableDescriptor htd, <span class="keyword">boolean</span> doSync)</div><div class="line"></div><div class="line"><span class="keyword">throws</span> IOException &#123;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (edits.isEmpty())</div><div class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.unflushedEntries.get();</div><div class="line">    ;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.closed) &#123;</div><div class="line"></div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Cannot append; log is closed"</span>);</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">long</span> txid = <span class="number">0</span>;</div><div class="line"></div><div class="line">    <span class="keyword">synchronized</span> (<span class="keyword">this</span>.updateLock) &#123; <span class="comment">//在追加Hlog文件的时候，需要获取到文件的锁</span></div><div class="line"></div><div class="line">        <span class="keyword">long</span> seqNum = obtainSeqNum();</div><div class="line"></div><div class="line">        <span class="comment">// The 'lastSeqWritten' map holds the sequence number of the oldest</span></div><div class="line"></div><div class="line">        <span class="comment">// write for each region (i.e. the first edit added to the particular</span></div><div class="line"></div><div class="line">        <span class="comment">// memstore). . When the cache is flushed, the entry for the</span></div><div class="line"></div><div class="line">        <span class="comment">// region being flushed is removed if the sequence number of the flush</span></div><div class="line"></div><div class="line">        <span class="comment">// is greater than or equal to the value in lastSeqWritten.</span></div><div class="line"></div><div class="line">        <span class="comment">// Use encoded name.  Its shorter, guaranteed unique and a subset of</span></div><div class="line"></div><div class="line">        <span class="comment">// actual  name.</span></div><div class="line"></div><div class="line">        <span class="keyword">byte</span>[] encodedRegionName = info.getEncodedNameAsBytes();</div><div class="line"></div><div class="line">        <span class="keyword">this</span>.lastSeqWritten.putIfAbsent(encodedRegionName, seqNum);</div><div class="line"></div><div class="line">        HLogKey logKey = makeKey(encodedRegionName, tableName, seqNum, now, clusterId);</div><div class="line"></div><div class="line">        doWrite(info, logKey, edits, htd);</div><div class="line"></div><div class="line">        <span class="keyword">this</span>.numEntries.incrementAndGet();</div><div class="line"></div><div class="line">        txid = <span class="keyword">this</span>.unflushedEntries.incrementAndGet();</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (htd.isDeferredLogFlush()) &#123;</div><div class="line"></div><div class="line">            lastDeferredTxid = txid;</div><div class="line"></div><div class="line">        &#125;</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line"></div><div class="line">        <span class="keyword">long</span> doneUpto;</div><div class="line"></div><div class="line">        <span class="keyword">long</span> now = System.currentTimeMillis();</div><div class="line"></div><div class="line">        <span class="comment">// First flush all the pending writes to HDFS. Then</span></div><div class="line"></div><div class="line">        <span class="comment">// issue the sync to HDFS. If sync is successful, then update</span></div><div class="line"></div><div class="line">        <span class="comment">// syncedTillHere to indicate that transactions till this</span></div><div class="line"></div><div class="line">        <span class="comment">// number has been successfully synced.</span></div><div class="line"></div><div class="line">        <span class="keyword">synchronized</span> (flushLock) &#123;</div><div class="line"></div><div class="line">            <span class="keyword">if</span> (txid &lt;= <span class="keyword">this</span>.syncedTillHere) &#123;</div><div class="line"></div><div class="line">                <span class="keyword">return</span>;</div><div class="line"></div><div class="line">            &#125;</div><div class="line"></div><div class="line">            doneUpto = <span class="keyword">this</span>.unflushedEntries.get();</div><div class="line"></div><div class="line">            List pending = logSyncerThread.getPendingWrites();</div><div class="line"></div><div class="line">            <span class="keyword">try</span> &#123;</div><div class="line"></div><div class="line">                logSyncerThread.hlogFlush(tempWriter, pending);</div><div class="line"></div><div class="line">            &#125; <span class="keyword">catch</span> (IOException io) &#123;</div><div class="line"></div><div class="line">                <span class="keyword">synchronized</span> (<span class="keyword">this</span>.updateLock) &#123; <span class="comment">//当hlogFlush发生exception的时候发生了死锁</span></div><div class="line"></div><div class="line">                    <span class="comment">// HBASE-4387, HBASE-5623, retry with updateLock held</span></div><div class="line"></div><div class="line">                    tempWriter = <span class="keyword">this</span>.writer;</div><div class="line"></div><div class="line">                    logSyncerThread.hlogFlush(tempWriter, pending);</div><div class="line"></div><div class="line">                &#125;</div><div class="line"></div><div class="line">            &#125;</div><div class="line"></div><div class="line">        &#125;</div><div class="line"></div><div class="line">    &#125;</div></pre></td></tr></table></figure>
<p>查看了下代码，是在logSyncerThread.hlogFlush往HDFS写文件发生IO Exception时会触发死锁。<a href="http://www.rigongyizu.com/tag/hbase/" target="_blank" rel="external">hbase</a> 0.94.X版本的一个bug，在社区已经修复了：<a href="https://issues.apache.org/jira/browse/HBASE-7728" target="_blank" rel="external">https://issues.apache.org/jira/browse/HBASE-7728</a>。</p>
<h2 id="6、修复"><a href="#6、修复" class="headerlink" title="6、修复"></a>6、修复</h2><p>​    临时的修复方案是sync所有的memstore后、重启regionserver</p>
<p>​    长久的修复方案就是老老实实打patch修复</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/11/24/HBase从0-94升级到0-98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/11/24/HBase从0-94升级到0-98/" itemprop="url">HBase从0.94升级到0.98</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-11-23T11:56:29-08:00">
                2014-11-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一、HBase0-98和0-96两个版本的一些变化"><a href="#一、HBase0-98和0-96两个版本的一些变化" class="headerlink" title="一、HBase0.98和0.96两个版本的一些变化"></a>一、HBase0.98和0.96两个版本的一些变化</h1><p>HBase0.98:</p>
<p>1、支持了reverse的scan模式(HBASE-4811)。</p>
<p>​      用户可以正向的或者反向的扫描表，这个和正向扫描仅仅有一点点的性能差别，使用的方法如下：</p>
<p>​      Scan scan = new Scan();</p>
<p>​      scan.setReversed(true);</p>
<p>2、Compaction升级成了Stripe Compaction(HBASE-7667)</p>
<p>​    按照以往的经验,首先，过多Region会增大RS维护的开销，降低RS的读写性能。随着数据量的增大，在一定程度上增加Region个数，会提高系统的吞吐率。然而，RS上服务的Region个数增多，增加了RS下内存维护的开销，尤其每个Store下都配置有一个MemStore，从而会造成频率更高的Flush操作，影响系统的读写性能。因此，如果能够提出更轻量级的mini-Region，不仅能够降低服务多个Region的开销，而且能够提升读写数据的效率;其次，Region Compaction容易”放大”。例如，Region区间[1FFF，2FFF）,在该区间内仅有[1FFF，21FF)区间有大量的写操作(put、delete)，但是,在触及MajorCompaction条件时，却需要对所有的文件执行Major Compaction，从而引起大量的IO。最后，Region Split操作代价较大。</p>
<p>​     为了减少region的数量，同时能降低compaction带来的影响，在新版本中加入了Stripe Compaction。其核心思想如下：</p>
<p>​     1）对于Region下的rowkey区间进行二次切分，例如[1FFF,2FFF)，切分成[1FFF,24FF),[24FF,2FFF)两个区间，每个区间成为Stripe。</p>
<p>​     2）Region下的数据文件分为Level-0和Level-1两层。其中Level-0主要用来存储临时的数据文件(例如使用bulkload或者执行mem flush操作之后的数据)， Level-1层的数据是按照Stripe的分区来区分。</p>
<p>​     3）支持两种方式的配置：Mini-regions的个数设置、或者以Size-based为大小触发因子的自动切分机制。</p>
<p>​     4）容错机制。如果在Stripes之间存在空洞。那么可以根据在Store当中的设置，将所有的处于Level-1层的文件回归到Level-0重新进行compaction。</p>
<p>​     5）Get操作时，一个Row所涉及到文件有：MemStore、Level-0下所有文件、以及Level-1下对应Stripe区下的文件。根据Stack的意见，最终Level-0下的文件只是一个暂时的状态，大部分文件都位于Level-1 Stripe下，因此，这样随机读时，需要涉及到的文件更聚集。</p>
<p>​     6）Scan操作时，需要定位startrow即可。在扫描过程中，会按照Stripe的row区间的排序，依次进行。</p>
<p>​     7）Compaction，是Level-0上升到Level-1的过程，同时，在Level-1层次的数据，也会进行相关的合并。</p>
<p>​     8）在Split操作时，定位Rowkey区间的中心点，可以根据Stripe记录的位置，进一步查找，因此，使用预置的Stripe会有利于Split操作的进行，可以实现多数HFile文件直接拷贝到子Region目录，从而加快了Split操作的效率。</p>
<p>下面对于Cassandra以及LevelDB中使用的多层次Compaction算法做一个介绍。</p>
<p>​    1)分层式压缩方式将数据分成条个层，最底层的叫L0，其上分别是L1，L2….，每一层的数据大小是其上的那一层数据最大大小的10倍，其中最底层L0的大小为5M (可以配置)</p>
<p>​    2) 当level层次大于0时，同一层的各个文件之间的Rowkey区间不会重叠。所以在level n与level n+1的数据块进行合并时，可以明确的知道某个key值处在哪个数据块中，可以一个数据块一个数据块的合并，合并后生成新块就丢掉老块。不用一直到所有合并完成后才能删除老的块。</p>
<p>​     3）整体执行流程是从L0-&gt;L1-&gt;L2，依次合并的过程，如下图所示。</p>
<p>​     <a href="http://www.binospace.com/wp-content/uploads/2013/06/compaction1.png" target="_blank" rel="external">http://www.binospace.com/wp-content/uploads/2013/06/compaction1.png</a></p>
<p>由上图，我们可以得知，越是level较低的块，它的数据就越新，在满足向下归约合并的过程中，就会按照文件的Rowkey的区间，进行合并，去除多余的版本，或者执行相关删除操作。因此，在读请求最极端的情况下，从Level0开始读数据，一直读到最下层Level n。这种Compaction的优势在于：</p>
<p>1）大部分的读操作如果有LRU特性，都会落入较低的Level上。因此，数据越”热”，Level就越低。从而有利于未来HFile多种存储介质的定位问题。</p>
<p>2）在合并的过程中，仅需在由上到下的部分文件参与，而不是要对所有文件执行Compaction操作。这样会加快Compaction执行的效率。</p>
<p>劣势在于，如果层次太多，在递归合并的过程中，容易造成某个区间的Compaction风暴，影响该区间数据操作的吞吐。</p>
<p>因此，HBase-Stripe Compaction的方案中，只有两层，Level 0和Level1，这种方法在保留分层压缩的优势的同时，降低了总文件个数，有利于RS执行Split、Merge等操作。</p>
<p>3、支持对快照进行MR(HBASE-8369)</p>
<p>​    现在快照作为hbase的元数据的一个分支文件，一个快照文件包含了表描述符、包含的region列表、reion的存储文件、region的相关WAL。利用HBase的快照进行MR，就是实现对RS的直接操作，类似与HDFS的直读(<a href="http://www.importnew.com/6151.html)。通过TableSnapshotInputFormat和TableSnapshotScanner" target="_blank" rel="external">http://www.importnew.com/6151.html)。通过TableSnapshotInputFormat和TableSnapshotScanner</a> 把表快照发送到客户端，然后客户端的操作就可以绕过HBase的Server层，直接通过java程序或者MR来流式的获取scan的结果(大概有5倍的性能提升)。可以参考<a href="http://www.slideshare.net/enissoz/mapreduce-over-snapshots" target="_blank" rel="external">http://www.slideshare.net/enissoz/mapreduce-over-snapshots</a></p>
<p>4、基于Cell的ACL以及可视化(HBASE-6222))</p>
<p>   类似与apache accumulo，其ACL的功能主要是为每个独立的cell提供强制访问控制。可视化的功能主要是利用了HBase本身的协处理器，然后结合ACL实现的。然后授予用户可以查看这个标签的权限，这些ACL的数据存储在HBase的一个新的系统表中。</p>
<p>对于如果使用这个特性，可以参考：<a href="https://blogs.apache.org/hbase/entry/hbase_cell_security和" target="_blank" rel="external">https://blogs.apache.org/hbase/entry/hbase_cell_security和</a></p>
<p>对于特性的详细描述，请参考：<a href="https://hbase.apache.org/book/security.html" target="_blank" rel="external">https://hbase.apache.org/book/security.html</a></p>
<p>5、透明的服务器端的加密(HBASE-7544)</p>
<p>​      这个特性主要提供了对wal和hfile的加密，主要是为REST接口提供的。</p>
<p>6、其它的一些小的功能点：</p>
<p>​       wal线程模式(HBASE-8755)、基于REST接口的流扫描(HBASE-9343)、解决了客户读访问Server的时候RPC数异常增长的BUG(HBASE-3787)</p>
<p>HBase0.96</p>
<p>   HBase0.96中改变大部分是向后兼容的，但是一部分是不向后兼容的，接下来分别指出：</p>
<p>   不兼容的改变：</p>
<p>​       删除了-ROOT-表(HBASE-3171) </p>
<p>​       </p>
<p>不再支持hadoop0.20(HBASE-6706) </p>
<p>更改了zk上的节点的名称(HBASE-4451 )</p>
<p>不支持HFileV1(HBASE-7660)</p>
<p>删除了Avro(HBASE-6553)</p>
<p>删除了客户端的行锁(HBASE-7315 HBASE-7263)</p>
<p>其它的改变：</p>
<p>   1、Bucket Cache-HBase的二级读cache( HBASE-7404)</p>
<p>​     HBase上Regionserver的内存分为两个部分，一部分作为Memstore，主要用来写；另外一部分作为BlockCache，主要用于读。Block的cache命中率对HBase的读性能影响十分大。在之前的版本中默认的是LruBlockCache，直接使用JVM的HashMap来管理BlockCache，会有Heap碎片和Full GC的问题。</p>
<p>在新版本中引入了Bucket Cache的概念。Bucket Cache可以放在内存中，也可以放在像SSD这样的适合高速随机读的外存储设备上，这样使得缓存的空间可以非常大，可以显著提高HBase读性能。Bucket Cache的本质是让HBase自己来管理内存资源而不是让Java的GC来管理，这个特点也是HBase自从诞生以来一直在激烈讨论的问题。</p>
<p>​     可以参考<a href="http://zjushch.iteye.com/blog/1751387" target="_blank" rel="external">http://zjushch.iteye.com/blog/1751387</a></p>
<p>   2、MSLAB(HBASE-8163)</p>
<p>​     MSLAB提升支持 MemStore-Local Allocation Buffers，通过预先分配内存块的方式解决了因为内存碎片造成的Full GC问题，但是对于频繁更新操作的时候，MemStore被flush到文件系统时没有reference的chunk还是会触发很多的Young GC。所以提出了MemStoreChunkPool的概念，也就是由HBase来管理一个ChunkPool用来存放chunk，不再依赖JVM的GC。这个ticket的本质也是由HBase进程来管理内存分配和重分配，不再依赖于Java GC。</p>
<p>​      可以参考：<a href="http://www.taobaotest.com/blogs/2310" target="_blank" rel="external">http://www.taobaotest.com/blogs/2310</a></p>
<p>   3、支持在线的region merge(HBASE-8219):</p>
<p>​      在线的region merge是移动快照，而离线的是真正的合并数据</p>
<p>​     4、变化比较大的 Compaction</p>
<p>​      HBase的Compaction是长期以来广受诟病的一个feature，很多人吐槽HBase也是因为这个特征。不过我们不能因为HBase有这样一个缺点就把它一棒子打死，更多的还是希望能够驯服它，能够使得它适应自己的应用场景。根据业务负载类型调整compaction的类型和参数，一般在业务高峰时候禁掉Major Compaction。在0.96中HBase社区为了提供更多的compaction的策略适用于不同的应用场景，采用了插件式的架构。同时改进了HBase在RegionServer端的存储管理，原来是直Region-&gt;Store-&gt;StoreFile，现在为了支持更加灵活多样的管理StoreFile和compact的策略，RS端采用了StoreEngine的结构。一个StoreEngine涉及到StoreFlusher, CompactionPolicy, Compactor, StoreFileManager。不指定的话默认是DefaultStoreEngine，四个组件分别是DefaultStoreFlusher, ExploringCompactionPolicy,DefaultCompactor, DefaultStoreFileManager。默认的Compaction算法从RatioBasedCompactionPolicy改为了ExploringCompactionPolicy。为什么要这么改，首先从compaction的优化目标来看：compaction is about trading some disk IO now for fewer seeks later，也就是compaction的优化目标是执行Compaction操作能合并越多的文件越好，如果合并同样多的文件产生的IO越小越好，这样select出来的列表才是最优的。</p>
<p>主要不同在于：RatioBasedCompactionPolicy是简单的从头到尾遍历StoreFile列表，遇到一个符合Ratio条件的序列就选定执行compaction。对于典型的不断flush memstore形成 StoreFile的场景是合适的，但是对于bulk-loaded是不合适的，会陷入局部最优。而ExploringCompactionPolicy则是从头到尾遍历的同时记录下当前最优，然后从中选择一个全局最优列表。</p>
<p>​          关于这两个算法的逻辑可以在代码中参考对应的applyCompactionPolicy()函数。其他Compaction Policy的研究和开发也非常活跃，例如Tier-based compaction (HBASE-6371,来自Facebook) 和stripe compaction(HBASE-7667)</p>
<p>​        此外compaction还有很多修改，参考HBASE-7678 、HBASE-766 、HBASE-7110、HBASE-7603、</p>
<p>HBASE-7842</p>
<p>​          吐槽：HBase compaction为什么会问题这么多，我感觉缺少了一个整体的IO负载的反馈和调度机制。因为compaction是从HDFS读数据，然后再写到HDFS中，和其他HDFS上的负载一样在抢占IO资源。如果能有个IO资源管理和调度的机制，在HDFS负载轻的时候执行compaction，在负载重的时候不要执行。而且这个问题在Hadoop/HDFS里同样存在，Hadoop的资源管理目前只针对CPU/Memory的资源管理，没有对IO的资源管理，会导致有些Job受自己程序bug的影响可能会写大量的数据到HDFS，会严重影响其他正常Job的读写性能。</p>
<p>​     5、Mean Time To Recovery/MTTR优化</p>
<p>​       目前HBase对外提供服务，Region Server是单点。如果某台RS挂掉，那么直到该RS上的所有Region被重新分配到其他RS上之前，这些Region是的数据是无法访问的。</p>
<p>​        对这个过程的改进主要包括：</p>
<p>HBASE-5844 和 HBASE-5926：删除zookeeper上Region Server/Master对应的znode，这样就省的等到znode 30s超时才发现对应的RS/Master挂了。</p>
<p>HBASE-7006： Distributed Log Replay，就是直接从HDFS上读取宕机的WAL日志，直接向新分配的RS进行Log Replay，而不是创建临时文件recovered.edits然后再进行Log Replay</p>
<p>HBASE-7213/8631： HBase的META表中所有的Region所在的Region Server将会有两个WAL，一个是普通的，一个专门给META表对应的Region用。这样在进行recovery的时候可以先恢复META表。  </p>
<p>​    6、PrefixTreeCompression(HBASE-4676 )</p>
<p>​     由于HBase的KeyValue存储是按照Row/Family/Qualifier/TimeStamp/Value的形式存储的，Row/Family/Qualifier这些相当于前缀，如果每一行都按照原始数据进行存储会导致占据存储空间比较大。HBase 0.94版本就已经引入了DataBlock Encode的概念(HBASE-4218)，将重复的Row/Family/Qualifier按照顺序进行压缩存储，提高内存利用率，支持四种压缩方式FAST_DIFF\PREFIX\PREFIX_TRIE\DIFF。但是这个feature也仅仅是通过delta encoding/compression降低了内存占用率，对数据查询效率没有提升，甚至会带来压缩/解压缩对CPU资源占用的情况。</p>
<p>​      PrefixTreeCompression是把重复的Row/Family/Qualifier按照Prefix Tree的形式进行压缩存储的，可以在解析时生成前缀树，并且树节点的儿子是排序的，所以从DataBlock中查询数据的效率可以超过二分查找。</p>
<p>7、使用了Protocol Buffer(HBASE-5305):</p>
<p>​    为了更好的跨版本的兼容性，引进了Protocol Buffer作为序列化/反序列化引擎使用到RPC中(此前Hadoop的RPC也全部用PB重写了)。因为随着HBase Server的不断升级，有些Client的版本可能还比较旧，所以需要RPC在新旧版本之间兼容。</p>
<p>​       8、 HBase Table Snapshot(HBASE-6055 HBASE-7290):</p>
<p>创建snaphost对HBase集群没有性能影响，只是生成了snaphost对应的metadata而不会去拷贝数据。用户可以通过创建snaphost实现backup和disaster recovery，例如用户在创建一个snaphost之后可能会误操作导致一些表出现了问题，这样我们可以选择回滚到创建snaphost的那个阶段而不会导致数据全都不可用。也可以定期创建snapshot然后拷贝到其他集群用于定时的offline processing</p>
<p>​    9、HBASE-8015：在0.96中，ROOT表已经改名为hbase:namespace，META则是hbase:meta。而且hbase:namespace是存在zookeeper上的。这个namespace类似于RDBMS里的database的概念，可以更好的做权限管理和安全控制。HBase中table的META信息也是作为一种Region存放在Region Server上的，那么META表的Region和其他普通Region就会产生明显的资源竞争。为了改善META Region的性能，360的HBase中提出了专属MetaServer，在这个Region Server上只存放META Region</p>
<p>10、HBASE-5229：同一个Region内的跨行事务。一次操作中涉及到同一个Region中的所有写操作在获取到的相关Row的所有行锁（按照RowKey的顺序依次取行锁，防止死锁）之后事务执行。</p>
<p>11、HBASE-4811：Reverse Scan。过去被问到说如何反向查找HBase中的数据，常常被答道再建一张反向存储的表，而且LevelDB和Cassandra都支持反向扫描。HBase反向扫描比正向扫描性能下降30%，这个和LevelDB是差不多的。</p>
<p>12、在不用安装cygwin的情况下支持在windows下安装</p>
<p>13、Hoya — HBase on YARN。可以在一个YARN集群上部署多个不同版本、不同配置的HBase instance，可以参考 <a href="https://github.com/hortonworks/hoya" target="_blank" rel="external">https://github.com/hortonworks/hoya</a></p>
<p>14、集成测试环境，从Netflix刚开源的Chaos Monkey中集成了一个测试环境，支持大规模的测试。</p>
<p>15、灵活的数据类型支持，详细请看<a href="https://blogs.apache.org/hbase/entry/data_types_schema" target="_blank" rel="external">https://blogs.apache.org/hbase/entry/data_types_schema</a></p>
<p>16、模块化，HBase已经拆分成了server、client等很多独立的模块。</p>
<p>17、提升了HBase的监控功能hbase tracing，HBASE-9121                     </p>
<p>​            <a href="https://blogs.apache.org/hbase/entry/migration_to_the_new_metrics" target="_blank" rel="external">https://blogs.apache.org/hbase/entry/migration_to_the_new_metrics</a></p>
<p>18、新的balancer的算法，类似与Simulated annealing 或者 Greedy Hillclimbing，不仅仅是更具数量来进行负载均衡，还增加了对读写的判断。 </p>
<p>​       19、增加了一些新的API的支持(HBase-8693、HBase8089、HBase8201)</p>
<p>20、通过调用mlockall增加了服务的稳定性(HBase4391)</p>
<h1 id="二、常用的一些API相关地址："><a href="#二、常用的一些API相关地址：" class="headerlink" title="二、常用的一些API相关地址："></a>二、常用的一些API相关地址：</h1><p>   1、详细HBase的基本API查询</p>
<p>   2、HBase的社区的所有BUG已经新特性的集合</p>
<p>   3、HBase Book</p>
<h1 id="三、HBASE发展方向"><a href="#三、HBASE发展方向" class="headerlink" title="三、HBASE发展方向"></a>三、HBASE发展方向</h1><p>   总的来说，HBase正在往更加好的方向发展，但是不同的需求造就了HBase本身也有各种不同的需求：</p>
<p>1、 Cloudera/Hortonworks/Yahoo/Facebook的人从系统和性能等多方面关注</p>
<p>2、Salesfore/huawei的人貌似更关注企业级特性，毕竟他们面对的客户都是电信、金融、证券等高帅富行业</p>
<p>3、来自国内的阿里巴巴/小米/360等公司更加关注系统性能、稳定性和运维相关的话题。国内互联网行业用HBase更加关注的是如何解决业务问题。</p>
<p>4、越来越多的公司把它们的HBase集群构建在云上，例如Pinterest所有的HBase集群都是在AWS上，国外的start up环境太好了，有了AWS自己根本不用花费太多的资源在基础设施上。</p>
<p>5、传统的HBase应用是online storage，实时数据读取服务。例如支付宝用HBase存放用户的历史交易信息服务用户查询，中国联通也使用HBase存储用户的上网历史记录信息用于用户的实时查询需求。现在HBase也向实时数据挖掘的应用场景中发展，例如wibidata公司开源的kiji (<a href="http://www.kiji.org/)能够在HBase上轻松构建实时推荐引擎、实时用户分层和实时欺诈监控。" target="_blank" rel="external">http://www.kiji.org/)能够在HBase上轻松构建实时推荐引擎、实时用户分层和实时欺诈监控。</a></p>
<p>参考资料：</p>
<p><a href="https://blogs.apache.org/hbase/entry/hbase_0_96_0_released" target="_blank" rel="external">https://blogs.apache.org/hbase/entry/hbase_0_96_0_released</a></p>
<p><a href="http://zh.hortonworks.com/blog/announcing-apache-hbase-0-96/" target="_blank" rel="external">http://zh.hortonworks.com/blog/announcing-apache-hbase-0-96/</a></p>
<p><a href="http://www.binospace.com/index.php/hbase-new-features-stripe-compaction/" target="_blank" rel="external">http://www.binospace.com/index.php/hbase-new-features-stripe-compaction/</a></p>
<p><a href="http://zh.hortonworks.com/blog/hbase-0-98-0-released/" target="_blank" rel="external">http://zh.hortonworks.com/blog/hbase-0-98-0-released/</a></p>
<p><a href="http://yanbohappy.sinaapp.com/?p=434" target="_blank" rel="external">http://yanbohappy.sinaapp.com/?p=434</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/10/30/HBase客户端开发建议/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/10/30/HBase客户端开发建议/" itemprop="url">HBase客户端开发建议</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-10-29T11:07:52-08:00">
                2014-10-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>​     1、HTable不是线程安全的。建议使用同一个HBaseConfiguration实例来创建HTable实例。这样可以共享ZooKeeper和socket实例。对于需要多线程大量写入HBase的，提供了HTablePool来支持多线程写入hbase，多线程同时从HTablePool中取出HTable并写入是安全的。</p>
<p>​       2、如果为了加快写的速度，设置了客户端的写缓存池，并且关闭了HTable对象的AutoFlush，那么在完成所有的操作后，最好调用HTable对象的close和flushCommits的操作，这样才能保证写到缓存池中的数据全部传送到服务器端。</p>
<p>​       3、当flush的时候，现有的memstore会生成快照，然后清空。在执行快照的时候，Hbase会继续接收修改操作，保存在memstore外面，直到快照完成，所以在flush的时候，应该避免大量的写入，可能会导致数据不同步。</p>
<p>未完待续…</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">aiping.lap</p>
              <p class="site-description motion-element" itemprop="description">aiping.liang s home</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">41</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">aiping.lap</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.3</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  








  





  

  

  

  

  

  

</body>
</html>
