<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="aiping.liang s home">
<meta property="og:type" content="website">
<meta property="og:title" content="Aiping.LAP">
<meta property="og:url" content="http://yoursite.com/page/3/index.html">
<meta property="og:site_name" content="Aiping.LAP">
<meta property="og:description" content="aiping.liang s home">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Aiping.LAP">
<meta name="twitter:description" content="aiping.liang s home">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/3/"/>





  <title>Aiping.LAP</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Aiping.LAP</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">认真工作，快乐生活</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/07/22/HBase的日常维护/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/07/22/HBase的日常维护/" itemprop="url">HBase的日常维护</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-07-21T22:17:24-08:00">
                2014-07-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="常用维护命令"><a href="#常用维护命令" class="headerlink" title="常用维护命令"></a>常用维护命令</h1><p>1、major_compact ‘testtable’，通常生产环境会关闭自动major_compact(配置文件中hbase.hregion.majorcompaction设 为0)，选择一个晚上用户少的时间窗口手工major_compact，如果hbase更新不是太频繁，可以一个星期对所有表做一次 major_compact，这个可以在做完一次major_compact后，观看所有的storefile数量，如果storefile数量增加到 major_compact后的storefile的近二倍时，可以对所有表做一次major_compact，时间比较长，操作尽量避免高锋期。</p>
<p>2、flush ‘testtable’，将所有memstore刷新到hdfs，通常如果发现regionserver的内存使用过大，造成该机的regionserver很多线程block，可以执行一下flush操作，这个操作会造成hbase的storefile数量剧增，应尽量避免这个操 作，还有一种情况，在hbase进行迁移的时候，如果选择拷贝文件方式，可以先停写入，然后flush所有表，拷贝文件。</p>
<p>3、balance_switch true或者balance_switch flase，配置master是否执行平衡各个regionserver的region数量，当我们需要维护或者重启一个regionserver时，会 关闭balancer，这样就使得region在regionserver上的分布不均，这个时候需要手工的开启balance。</p>
<p>4、move一个region到另一个RS。</p>
<p>Move 要移动的regiong的id 目的机器，端口，startkey</p>
<h1 id="服务维护命令"><a href="#服务维护命令" class="headerlink" title="服务维护命令"></a>服务维护命令</h1><p>1、bin/graceful_stop.sh –restart –reload –debugnodename。这个操作是平滑的重启regionserver进程，对服务不会有影响，他会先将需要重启的regionserver上面的所有 region迁移到其它的服务器，然后重启，最后又会将之前的region迁移回来，但我们修改一个配置时，可以用这种方式重启每一台机子，这个命令会关 闭balancer，所以最后我们要在hbase shell里面执行一下balance_switch true，对于hbase regionserver重启，不要直接kill进程，这样会造成在zookeeper.session.timeout这个时间长的中断，也不要通过 bin/hbase-daemon.sh stop regionserver去重启，如果运气不太好，-ROOT-或者.META.表在上面的话，所有的请求会全部失败。</p>
<p>2、关闭下线一台regionserver bin/graceful_stop.sh –stop<strong>nodename</strong> <strong>。</strong>和上面一样，系统会在关闭之前迁移所有region，然后stop进程，同样最后我们要手工balance_switch true，开启master的region均衡。</p>
<p>3、检查region是否正常以及修复：bin/hbase hbck (检查) &amp;&amp; bin/hbase hbck -fix （修复）。执行修复的命令后会返回所有的region是否正常挂载，如没有正常挂载可以使用下一条命令修复，如果还是不能修复，那需要看日志为什么失败，手工处理，这个命令时间稍微有点时间长，慎用。</p>
<p>4、有时候，用户有必要绕过HBase并直接访问一个HFile，例如，检查它的健康程度，或者转存它的内容。hbase org.apache.hadoop.hbase.io.hfile.HFile -f /hbase/t1/e634b203a20933f8e13ad83ecb1511dc/f1/4f24802e1e8a47ba82c30d9de35589cc -v -m -p</p>
<p>5、检查复制的数据是否完整。hbase org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication –starttime=1265875194289 –stoptime=1265878794289 1 TestTable</p>
<p>6、更新所有meta的信息（慎用） hbase hbck -fixMeta -fixAssignments</p>
<h1 id="HBase的迁移"><a href="#HBase的迁移" class="headerlink" title="HBase的迁移"></a>HBase的迁移</h1><p>1，copytable方式</p>
<p>bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable –peer.adr=zookeeper1,zookeeper2,zookeeper3:/hbase ‘testtable’</p>
<p>目前0.92之前的版本的不支持多版本的复制，0.94已经支持多个版本的复制。当然这个操作需要添加hbase目录里的conf/mapred-site.xml，可以复制hadoop的过来。</p>
<p>2，Export/Import</p>
<p>bin/hbase org.apache.hadoop.hbase.mapreduce.Exporttesttable /user/testtable [versions] [starttime] [stoptime]</p>
<p>bin/hbase org.apache.hadoop.hbase.mapreduce.Importtesttable /user/testtable</p>
<p>跨版本的迁移，我觉得是一个不错的选择，而且copytable不支持多版本，而export支持多版本，比copytable更实用一些。</p>
<p>3，直接拷贝hdfs对应的文件</p>
<p>首先拷贝hdfs文件，如bin/hadoop distcp hdfs://srcnamenode:9000/hbase/testtable/ hdfs://distnamenode:9000/hbase/testtable/</p>
<p>然后在目的hbase上执行bin/hbase org.jruby.Main bin/add_table.rb /hbase/testtable</p>
<p>生成meta信息后，重启hbase</p>
<p>这个操作是简单的方式，操作之前可以关闭hbase的写入，执行flush所有表（上面有介绍）,再distcp拷贝，如果hadoop版本不一致，可以用hftp接口的方式，我推荐使用这种方式，成本低</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/07/16/Hadoop和HBase开启对LZO压缩的支持/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/07/16/Hadoop和HBase开启对LZO压缩的支持/" itemprop="url">Hadoop和HBase开启对LZO压缩的支持</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-07-15T21:19:12-08:00">
                2014-07-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这本来是想写一个小的系列，包括后续的压测，所以先从环境搭建开始.</p>
<p>PS:看了很多的文章后，总结出来的结论：网络上搭建方式各种坑，实践才是检验真理的唯一方式</p>
<h1 id="基础环境"><a href="#基础环境" class="headerlink" title="基础环境"></a>基础环境</h1><p>hadoop版本1.0.4,Hbase0.94.3，系统环境：Centos5X, 64位</p>
<h1 id="搭建过程"><a href="#搭建过程" class="headerlink" title="搭建过程"></a>搭建过程</h1><p> 一共需要准备4部分东西，这4个部分缺一不可。</p>
<h2 id="1、lzo的系统库文件"><a href="#1、lzo的系统库文件" class="headerlink" title="1、lzo的系统库文件"></a>1、lzo的系统库文件</h2><p>不用相信网上的方法，亲测了，和很多库一样，这个不需要编译环境和运行环境是同一套，不过最好本地和运行的基本库一样，否则会有一些系统库不是向下兼容的问题。</p>
<p>下载lzo-2.06.tar.gz</p>
<p>然后依次执行以下的命令编译系统库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">tar -zxvf lzo-2.06.tar.gz</div><div class="line"></div><div class="line">./configure --enable-shared --prefix /usr/local/lzo-2.06</div><div class="line"></div><div class="line">make</div><div class="line"></div><div class="line">make install</div></pre></td></tr></table></figure>
<p>生成的库文件如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">├── lib</div><div class="line">│   ├── liblzo2.a</div><div class="line">│   ├── liblzo2.la</div><div class="line">│   ├── liblzo2.so -&gt; liblzo2.so.2.0.0</div><div class="line">│   ├── liblzo2.so.2 -&gt; liblzo2.so.2.0.0</div><div class="line">│   └── liblzo2.so.2.0.0</div></pre></td></tr></table></figure>
<p>由于这些都是最基本的lzo压缩库，所以需要放到集群上的公共库中(我的系统是64位的，放在了/usr/lib64下面)，要求每个节点都有该库，因为所有的Map和Reduce都只依赖节点本身的库。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop131 ~]$ ls /usr/lib64/liblzo2.*</div><div class="line"></div><div class="line">/usr/lib64/liblzo2.a   /usr/lib64/liblzo2.so    /usr/lib64/liblzo2.so.2.0.0</div><div class="line"></div><div class="line">/usr/lib64/liblzo2.la  /usr/lib64/liblzo2.so.2</div></pre></td></tr></table></figure>
<h2 id="2、导入hadoop依赖lzo的jar包"><a href="#2、导入hadoop依赖lzo的jar包" class="headerlink" title="2、导入hadoop依赖lzo的jar包"></a>2、导入hadoop依赖lzo的jar包</h2><p>下载hadoop-gpl-packaging-0.6.1-1.x86_64.rpm</p>
<p>在系统上安装:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rpm -ivh hadoop-gpl-packaging-0.6.1-1.x86_64.rpm</div></pre></td></tr></table></figure>
<p>将会在本地安装以下的目录文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">├── lib</div><div class="line">│   ├── cdh4.0.1</div><div class="line">│   │   └── elephant-bird-gerritjvv</div><div class="line">│   │       └── elephant-bird-2.0.5.jar</div><div class="line">│   ├── guava-12.0.jar</div><div class="line">│   ├── hadoop-lzo-0.4.17.jar</div><div class="line">│   ├── hadoop-lzo.jar -&gt; /opt/hadoopgpl/lib/hadoop-lzo-0.4.17.jar</div><div class="line">│   ├── pig-0.10.0</div><div class="line">│   │   ├── elephant-bird-gerritjvv</div><div class="line">│   │   │   └── elephant-bird-2.0.5.jar</div><div class="line">│   │   ├── elephant-bird.jar -&gt; /opt/hadoopgpl/lib/pig-0.10.0/elephant-bird-gerritjvv/elephant-bird-2.0.5.jar</div><div class="line">│   │   ├── udfs-0.3.1.jar</div><div class="line">│   │   └── udfs.jar -&gt; /opt/hadoopgpl/lib/pig-0.10.0/udfs-0.3.1.jar</div><div class="line">│   ├── pig-0.6.0</div><div class="line">│   │   └── elephant-bird-1.0.jar</div><div class="line">│   ├── pig-0.7.0</div><div class="line">│   │   └── elephant-bird-1.0.jar</div><div class="line">│   ├── pig-0.8.0</div><div class="line">│   │   ├── elephant-bird-dvryaboy</div><div class="line">│   │   │   └── elephant-bird-2.0.jar</div><div class="line">│   │   └── elephant-bird-gerritjvv</div><div class="line">│   │       └── elephant-bird-2.0.jar</div><div class="line">│   ├── protobuf-java-2.4.1.jar</div><div class="line">│   ├── slf4j-api-1.5.8.jar</div><div class="line">│   ├── slf4j-log4j12-1.5.10.jar</div><div class="line">│   └── yamlbeans-0.9.3.jar</div><div class="line">└── native</div><div class="line">    └── Linux-amd64-64</div><div class="line">        ├── libgplcompression.a</div><div class="line">        ├── libgplcompression.la</div><div class="line">        ├── libgplcompression.so</div><div class="line">        ├── libgplcompression.so.0</div><div class="line">        ├── libgplcompression.so.0.0.0</div><div class="line">        ├── LzoCompressor.lo</div><div class="line">        ├── LzoCompressor.o</div><div class="line">        ├── LzoDecompressor.lo</div><div class="line">        └── LzoDecompressor.o</div></pre></td></tr></table></figure>
<p>现在只需要 hadoop-lzo.jar，需要将这个jar包拷贝到$HADOOP_HOME/lib和$HBAS_HOME/lib下，需要集群中所有的节点都有这个包。</p>
<h2 id="3、hadoop调用系统库的jni库文件"><a href="#3、hadoop调用系统库的jni库文件" class="headerlink" title="3、hadoop调用系统库的jni库文件"></a>3、hadoop调用系统库的jni库文件</h2><p>在上面的Linux-amd64-64目录下，存放了需要的所有的JNI库文件，需要把这些库放到          </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$HADOOP_HOME/lib/native/Linux-amd64-64</div></pre></td></tr></table></figure>
<p>拷贝其中所有的libgplcompression.* 到</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$HBASE_HOME/lib/native/Linux-amd64-64</div></pre></td></tr></table></figure>
<p>注意，一般Hbase中不存在该目录，直接创建就可以了。</p>
<h2 id="4、需要配置中，引用相关的库文件"><a href="#4、需要配置中，引用相关的库文件" class="headerlink" title="4、需要配置中，引用相关的库文件"></a>4、需要配置中，引用相关的库文件</h2><p>需要修改3个hadoop的配置：</p>
<h3 id="core-site-xml-中增加："><a href="#core-site-xml-中增加：" class="headerlink" title="core-site.xml 中增加："></a>core-site.xml 中增加：</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codec.lzo.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<h3 id="mapred-site-xml-中增加："><a href="#mapred-site-xml-中增加：" class="headerlink" title="mapred-site.xml 中增加："></a>mapred-site.xml 中增加：</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.output.compress<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.child.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>-Djava.library.path=$&#123;HADOOP_HOME&#125;/lib/native/Linux-amd64-64<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.map.output.compression.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<h3 id="hadoop-env-sh-中增加："><a href="#hadoop-env-sh-中增加：" class="headerlink" title="hadoop-env.sh 中增加："></a>hadoop-env.sh 中增加：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/lib:$HBASE_HOME/lib:</div><div class="line"></div><div class="line">export JAVA_LIBRARY_PATH=$JAVA_LIBRARY_PATH:$HADOOP_HOME/lib/native/Linux-amd64-64:$HADOOP_HOME/lib/native:$HADOOP_HOME/lib</div></pre></td></tr></table></figure>
<h1 id="结果测试"><a href="#结果测试" class="headerlink" title="结果测试"></a>结果测试</h1><p>完成上述的4步配置后，需要重启集群。使用下面的方式对结果进行测试</p>
<h2 id="hadoop测试使用lzo的压缩："><a href="#hadoop测试使用lzo的压缩：" class="headerlink" title="hadoop测试使用lzo的压缩："></a>hadoop测试使用lzo的压缩：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop jar $&#123;HADOOP_HOME&#125;/contrib/streaming/hadoop-streaming-1.0.4.jar -input /in/part-00009 -output /testalzo -mapper cat -reducer cat -jobconf mapred.output.compress=true -jobconf mapred.output.compression.codec=org.apache.hadoop.io.compress.LzoCodec</div></pre></td></tr></table></figure>
<p>测试成功的话会生成/in/part-00009的压缩文件/testalzo/part-00000.lzo_deflate</p>
<h2 id="hbase测试使用lzo压缩："><a href="#hbase测试使用lzo压缩：" class="headerlink" title="hbase测试使用lzo压缩："></a>hbase测试使用lzo压缩：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hbase org.apache.hadoop.hbase.util.CompressionTest hdfs://hadoop131/terasort/output/part-00005 lzo</div></pre></td></tr></table></figure>
<p>测试成功的话会输出SUCCESS   </p>
<p>修改表压缩方式为lzo，接下来以usertable为例子进行</p>
<p>1、describe ‘usertable’ 查看原来的压缩方式，默认情况下的压缩方式 COMPRESSION =&gt; ‘NONE’</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">hbase(main):003:0&gt; describe &apos;usertable&apos; </div><div class="line">DESCRIPTION                                                       ENABLED                           </div><div class="line">&#123;NAME =&gt; &apos;usertable&apos;, FAMILIES =&gt; [&#123;NAME =&gt; &apos;cf&apos;, DATA_BLOCK_ENC true                              </div><div class="line">ODING =&gt; &apos;NONE&apos;, BLOOMFILTER =&gt; &apos;NONE&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;                                   </div><div class="line">, VERSIONS =&gt; &apos;3&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, T                                   </div><div class="line">TL =&gt; &apos;2147483647&apos;, KEEP_DELETED_CELLS =&gt; &apos;false&apos;, BLOCKSIZE =&gt;                                    </div><div class="line">&apos;65536&apos;, IN_MEMORY =&gt; &apos;false&apos;, ENCODE_ON_DISK =&gt; &apos;true&apos;, BLOCKCA                                   </div><div class="line">CHE =&gt; &apos;true&apos;&#125;]&#125;  </div><div class="line">&gt;disable &apos;usertable&apos;</div><div class="line">&gt;alter &apos;usertable&apos;,&#123;NAME=&gt;&apos;info&apos;,COMPRESSION=&gt;&apos;LZO&apos;&#125;</div><div class="line">&gt;enable &apos;TestTable&apos;</div><div class="line">&gt;compact &apos;TestTable&apos;</div></pre></td></tr></table></figure>
<p>测试结果：</p>
<p>先用自带的工具生成1G的测试数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hbase org.apache.hadoop.hbase.PerformanceEvaluation  sequentialWrite 1</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">1059592589  hdfs://hadoop131/hbase/TestTable</div><div class="line">409232235   hdfs://hadoop131/hbase/TestTable</div></pre></td></tr></table></figure>
<p>原来1G的数据被压缩到了400M，这个压缩效果和数据有关系，但是从网上的经验看，至少20%的压缩比还是能达到的。</p>
<h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><p>2014-10-24日增加:</p>
<p>升级为hadoop2.4.1版本，为新版本增加LZO的压缩：</p>
<p>1、编译本地的LZO库同上</p>
<p>2、编译Hadoop-LZO依赖：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">git init </div><div class="line"></div><div class="line">git clone https://github.com/twitter/hadoop-lzo</div><div class="line"></div><div class="line">export C_INCLUDE_PATH=/usr/local/lzo/include</div><div class="line"></div><div class="line">export LIBRARY_PATH=/usr/local/lzo/lib</div><div class="line"></div><div class="line">mvn clean package -Dmaven.test.skip=true</div></pre></td></tr></table></figure>
<p>3、拷贝相应的库和包到集群上：</p>
<p>需要拷贝的包括(如果需要HBase支持ZLO，需要把所有的包拷贝到hbase下的lib下，没有的话创建</p>
<p>所有的lzo依赖库，把所有/usr/local/lzo/lib下的库文件拷贝到集群上，且通过ldd的工具设置为系统库。</p>
<p>拷贝maven编译完成后的target/hadoop-lzo-0.4.20-SNAPSHOT.jar拷贝到hadoop的share/hadoop/common下</p>
<p>拷贝target/native/Linux-amd64-64/lib下的所有库文件到hadoop的lib/native/下</p>
<p>4、配置集群文件同上<br>5、重启集群<br>6、测试压缩是否可以用<br>在本地安装lzop的工具，然后随便找一个文件，执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">lzop -U -9 testFile</div></pre></td></tr></table></figure></p>
<p> 然后上传到HDFS上，如果直接用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop fs -cat</div></pre></td></tr></table></figure>
<p> 可以看到文件是乱码，但是用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hdfs dfs -text</div></pre></td></tr></table></figure>
<p> 看到的是非乱码就证明lzo正确安装了。</p>
<h1 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h1><p><a href="http://www.quora.com/How-do-I-install-LZO-compression-with-Hbase-and-Hadoop" target="_blank" rel="external">http://www.quora.com/How-do-I-install-LZO-compression-with-Hbase-and-Hadoop</a><br><a href="http://shitouer.cn/2013/01/hadoop-hbase-snappy-setup-final-tutorial/" target="_blank" rel="external">http://shitouer.cn/2013/01/hadoop-hbase-snappy-setup-final-tutorial/</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/06/30/HBase查找数据的流程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/06/30/HBase查找数据的流程/" itemprop="url">HBase查找数据的流程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-06-29T20:35:24-08:00">
                2014-06-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>﻿﻿﻿﻿StoreFile是只读的，一旦创建后就不可以再修改，一个Stroe中包含了多个的Storefile和一个memStore。StoreFile是以HDFS文件的方式直接存储在HDFS上的。</p>
<p>当查询到某个HFile的时候</p>
<p>先读取Trailer字段，在其中保存了每个段的其实位置，然后将Data Index会被读取到内存中，这样，当检索某个key时，不需要扫描整个HFile，而只需从内存中找到key所在的block，通过一次磁盘io将整个 block读取到内存中，再找到需要的key。</p>
<p>在HFile 中根据一个key 搜索一个data 的过程：</p>
<p>1、先内存中对HFile的root index进行二分查找。如果支持多级索引的话，则定位到的是leaf/intermediate index，如果是单级索引，则定位到的是data block</p>
<p>2、如果支持多级索引，则会从缓存/hdfs（分布式文件系统）中读取leaf/intermediate index chunk，在leaf/intermediate chunk根据key值进行二分查找（leaf/intermediate index chunk支持二分查找），找到对应的data block。</p>
<p>3、从缓存/hdfs中读取data block</p>
<p>4、在data block中遍历查找key。<br>在data block 中，所有的Key value是一个固定格式的字节数组，存储的格式如下：<br>   <img src="http://www.searchtb.com/wp-content/uploads/2011/01/image0090.jpg" alt="img"><br>具体的流程可以参考：<a href="http://www.webrube.com/hbase-hfile-web_rube/2412" target="_blank" rel="external">http://www.webrube.com/hbase-hfile-web_rube/2412</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/06/28/HBase获取region元数据的流程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/06/28/HBase获取region元数据的流程/" itemprop="url">HBase获取region元数据的流程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-06-27T19:16:41-08:00">
                2014-06-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>﻿﻿﻿﻿客户从Hbase Client端访问Hbase的时候，如果知道自己应该到哪个Regionserver上获取相应的Region来操作数据呢，本文将详细介绍这个流程。</p>
<p>介于nosql和RDBMS之间，仅能通过主键(row key)和主键的range来检索数据，仅支持单行事务(可通过hive支持来实现多表join等复杂操作)。主要用来存储非结构化和半结构化的松散数据。</p>
<p><img src="http://pic002.cnblogs.com/images/2012/79891/2012060400513336.jpg" alt="img"></p>
<p>那么如果通过row key来查找到相应的数据呢，本文将先进行基本的ReginServer的定位。</p>
<p>1、首先需要介绍两张特殊的表：<br>  -ROOT-   记录了.META.表的元数据信息。<br>  .META.    记录了用户表的元数据信息。</p>
<p>先看-ROOT-表，其RowKey依次 包含了表名(.META.)，StartKey，TimeStamp，而其列簇中共有4个字段，regioninfo、server、serverstartcode、v<br>其次看.META.表，RowKey中包含了包含的表名(这里只有一个test表),startKey，timeStamp，列簇中有3个字段， regioninfo、server、serverstartcode。</p>
<p>2、接下来我们从后往前的分析如何具体的查找到一个表<br>​    所有的数据都是存储在不同的ReginServer上的，这些分布的元数据就存储在了表.META.中，假设我们需要在test表中查找RK为x的数据，则首先是通过.META.的Rowkey中查找表名为test，<br>   通过.META.中ROWKEY可以找到包含有test表的所有纪录，然后根据偏移可以找到某行数据包含了该条记录，最后在Server列中，就可以查找到在哪个Regionserver上。</p>
<p>​    当表的个数和大小都不停的增长的时候，.META.表也会分块，这样就不能很快定位要查找的.META.在哪个Regionserver上，这就需要用-ROOT-定位，和上面的原理类似。</p>
<p>   最后把-ROOT-的信息存放到ZK上，这样就可以保证快速的定位到某个具体的RegionServer上。</p>
<p>​      </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/06/25/HBase写入流程分析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/06/25/HBase写入流程分析/" itemprop="url">HBase写入流程分析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-06-24T19:23:23-08:00">
                2014-06-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这一节将分析一下，HBase如何存储数据。<br>1、首先分享几个基本的流程：<br>   Table –&gt; split –&gt;regiongs –&gt; region<a href="不同的region会分配到不同的regionserver上">startkey, endkey</a><br>​                              1                           n                  n<br>  HRregionServer —-&gt;  HRregion   —–&gt;   Store —-&gt;  storefile[当memStroe到达一定量的时候，会生成stroefile]<br>​                                                                                            |<br>​                                                                                    底层实现是HFile<br><img src="http://static.open-open.com/lib/uploadImg/20111201/20111201143315_949.png" alt="分布式数据库 HBase"><br>2、通过图解，分析整个存储过程。<br>​    当用户在表test中增加一跳记录的时候。<br>  <img src="http://pic002.cnblogs.com/images/2012/79891/2012060400513336.jpg" alt="img"><br>   表插入新的记录后 ，相当与写入了一个KeyValue的数据，这时候会导致Hfile的增加。</p>
<p>HFile里面的每个KeyValue对就是一个简单的byte数组。但是这个byte数组里面包含了很多项，并且有固定的结构。我们来看看里面的具体结构：</p>
<p><img src="http://www.searchtb.com/wp-content/uploads/2011/01/image0090.jpg" alt="img"></p>
<p>开始是两个固定长度的数值，分别表示Key的长度和Value的长度。紧接着是Key，开始是固定长度的数值，表示RowKey的长度，紧接着是RowKey，然后是固定长度的数值，表示Family的长度，然后是Family，接着是Qualifier，然后是两个固定长度的数值，表示Time Stamp和Key Type（Put/Delete）。Value部分没有这么复杂的结构，就是纯粹的二进制数据了。<br>​      每个KV的数据又是存储在HFile的Data段中，Data Block是HBase I/O的基本单元，为了提高效率，HRegionServer中有基于LRU的Block Cache机制。每个Data块的大小可以在创建一个Table的时候通过参数指定，大号的Block有利于顺序Scan，小号Block利于随机查询。每个Data块除了开头的Magic以外就是一个个KeyValue对拼接而成, Magic内容就是一些随机数字，目的是防止数据损坏。后面会详细介绍每个KeyValue对的内部构造。<br>​    很多的Data又可以组成一个Hfile。HFile文件是不定长的，长度固定的只有其中的两块：Trailer和FileInfo。正如图中所示的，Trailer中有指针指向其他数据块的起始点。File Info中记录了文件的一些Meta信息，例如：AVG_KEY_LEN, AVG_VALUE_LEN, LAST_KEY, COMPARATOR, MAX_SEQ_ID_KEY等。Data Index和Meta Index块记录了每个Data块和Meta块的起始点。<br>​      <img src="http://www.searchtb.com/wp-content/uploads/2011/01/image0080.jpg" alt="img"></p>
<p>​      很多的Hfile构成了一个Store，Store存储是HBase存储的核心了，其中由两部分组成，一部分是MemStore，一部分是StoreFiles。MemStore是Sorted Memory Buffer，用户写入的数据首先会放入MemStore，当MemStore满了以后会Flush成一个StoreFile（底层实现是HFile），当StoreFile文件数量增长到一定阈值，会触发Compact合并操作，将多个StoreFiles合并成一个StoreFile，合并过程中会进行版本合并和数据删除，因此可以看出HBase其实只有增加数据，所有的更新和删除操作都是在后续的compact过程中进行的，这使得用户的写操作只要进入内存中就可以立即返回，保证了HBase I/O的高性能。当StoreFiles Compact后，会逐步形成越来越大的StoreFile，当单个StoreFile大小超过一定阈值后，会触发Split操作，同时把当前Region Split成2个Region，父Region会下线，新Split出的2个孩子Region会被HMaster分配到相应的HRegionServer上，使得原先1个Region的压力得以分流到2个Region上。下图描述了Compaction和Split的过程：<br>​      <img src="http://www.searchtb.com/wp-content/uploads/2011/01/image0070.gif" alt="img"><br>​      HRegionServer内部管理了一系列HRegion对象，每个HRegion对应了Table中的一个Region，HRegion中由多个HStore组成。每个HStore对应了Table中的一个Column Family的存储，可以看出每个Column Family其实就是一个集中的存储单元，因此最好将具备共同IO特性的column放在一个Column Family中，这样最高效。<br> 参考文章：<br><a href="http://www.searchtb.com/2011/01/understanding-hbase.html" target="_blank" rel="external">http://www.searchtb.com/2011/01/understanding-hbase.html</a><a href="http://www.open-open.com/lib/view/open1322721298671.html" target="_blank" rel="external">http://www.open-open.com/lib/view/open1322721298671.html</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/06/20/java多线程-Executor/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/06/20/java多线程-Executor/" itemprop="url">java多线程(Executor)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-06-19T19:42:58-08:00">
                2014-06-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Executor框架是指java 5中引入的一系列并发库中与executor相关的一些功能类，其中包括线程池，Executor，Executors，ExecutorService，CompletionService，Future，Callable等。</p>
<p>下面这个例子是为了证明一个问题：当一个ExecutorService在执行了shutdown后，并不会影响当前线程的执行，当前的线程会继续执行，只不过该对象不能再增加新的线程。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.sql.DatabaseMetaData;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.Date;</div><div class="line"><span class="keyword">import</span> java.util.concurrent.Callable;</div><div class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</div><div class="line"><span class="keyword">import</span> java.util.concurrent.ExecutorService;</div><div class="line"><span class="keyword">import</span> java.util.concurrent.Executors;</div><div class="line"><span class="keyword">import</span> java.util.concurrent.Future;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCallable</span> <span class="keyword">implements</span> <span class="title">Callable</span> </span>&#123;    </div><div class="line">    <span class="keyword">private</span> <span class="keyword">int</span> myNum = <span class="number">0</span>;</div><div class="line">    </div><div class="line">    MyCallable(<span class="keyword">int</span> threadNum)&#123;</div><div class="line">        <span class="keyword">this</span>.myNum = threadNum;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">        System.out.println(<span class="string">"&gt;&gt;&gt;"</span>+myNum+<span class="string">" thread start"</span>);</div><div class="line">        Date dateTmp1 = <span class="keyword">new</span> Date();</div><div class="line">        Thread.sleep(<span class="number">1000</span>);</div><div class="line">        Date dataTmp2 = <span class="keyword">new</span> Date();</div><div class="line">        <span class="keyword">long</span> time = dataTmp2.getTime()-dateTmp1.getTime();</div><div class="line">        System.out.println(<span class="string">"&gt;&gt;&gt;"</span>+myNum+<span class="string">" thread start, it has run time for "</span>+time);</div><div class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="meta">@SuppressWarnings</span>(<span class="string">"unchecked"</span>)</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadTest</span> </span>&#123;</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String []args)</span> <span class="keyword">throws</span> ExecutionException,InterruptedException</span>&#123;</div><div class="line">        System.out.println(<span class="string">"----Thread test stat-----"</span>);</div><div class="line">        Date dateNow = <span class="keyword">new</span> Date();</div><div class="line">        <span class="keyword">int</span> taskSize = <span class="number">5</span>;</div><div class="line">        </div><div class="line">       ExecutorService threadPool = Executors.newFixedThreadPool(taskSize);</div><div class="line">        List list = <span class="keyword">new</span> ArrayList();</div><div class="line">        <span class="keyword">for</span> ( <span class="keyword">int</span> i=<span class="number">0</span>;i</div><div class="line">            Callable callable = <span class="keyword">new</span> MyCallable(i);</div><div class="line">            Future future = threadPool.submit(callable);</div><div class="line">            list.add(future);</div><div class="line">        &#125;</div><div class="line">        threadPool.shutdown();</div><div class="line">        </div><div class="line">        <span class="keyword">for</span> (Future future : list)&#123;</div><div class="line">            <span class="keyword">if</span> (future.get() != <span class="keyword">null</span>)&#123;</div><div class="line">                System.out.println(<span class="string">"&gt;&gt;&gt;"</span> + future.get().toString());</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        </div><div class="line">        Date dataEnd = <span class="keyword">new</span> Date();</div><div class="line">        <span class="keyword">long</span> timeEnd = dateNow.getTime()-dataEnd.getTime();</div><div class="line">        System.out.println(<span class="string">"----Thread test "</span>+timeEnd+<span class="string">" end-----"</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在上面的例子中，用的是<strong>newFixedThreadPool</strong>来创建指定数量的线程。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutorService <span class="title">newFixedThreadPool</span><span class="params">(<span class="keyword">int</span> nThreads)</span></span></div></pre></td></tr></table></figure>
<p>创建固定数目线程的线程池。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutorService <span class="title">newCachedThreadPool</span><span class="params">()</span></span></div></pre></td></tr></table></figure>
<p>一个可缓存的线程池，调用<code>execute</code> 将重用以前构造的线程（如果线程可用）。如果现有线程没有可用的，则创建一个新线程并添加到池中。终止并从缓存中移除那些已有 60 秒钟未被使用的线程。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutorService <span class="title">newSingleThreadExecutor</span><span class="params">()</span></span></div></pre></td></tr></table></figure>
<p>一个单线程化的Executor。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ScheduledExecutorService <span class="title">newScheduledThreadPool</span><span class="params">(<span class="keyword">int</span> corePoolSize)</span></span></div></pre></td></tr></table></figure>
<p>一个支持定时及周期性的任务执行的线程池，多数情况下可用来替代Timer类。<br>上面的几个方法中，应该优先使用<strong>newCachedThreadPool</strong>，因为它创建与需求数量一致的线程数，只有当这个方法出问题的时候再考虑用<strong>newFixedThreadPool</strong>newSingleThreadExecutor对于希望在程序中连续运行的任何事物都是有用的。<br>在上面的例子中，使用的是callable，也可以使用runnable，这两个的区别就是如果想让线程执行有返回值，应该用callable。调用callable不是使用run，而是call，而且在executorserver中添加的时候，不是使用execute，而是使用submit，这个调用会返回一个futurer的对象，然后该对象可以调用get来获取结果<br>线程的yield方法是建议让具有相同优先级的其它线程可以运行了。</p>
<p>ExecutorService扩展了Executor并添加了一些生命周期管理的方法。一个Executor的生命周期有三种状态，<strong>运行</strong>，<strong>关闭</strong>，<strong>终止</strong>。Executor创建时处于运行状态。当调用ExecutorService.shutdown()后，处于关闭状态，isShutdown()方法返 回true。这时，不应该再想Executor中添加任务，所有已添加的任务执行完毕后，Executor处于终止状态，isTerminated()返 回true。</p>
<p>如果Executor处于关闭状态，往Executor提交任务会抛出unchecked exception RejectedExecutionException<br>线程的后台执行: 设置thread的对象setDaemon为true就可以设置为后台进程<br>使用join方法可以有一个特效，使得某个线程A在另一个线程B上调用A.join，则当前的B将会挂起，直到A执行完成后，B才会继续执行，也可以加一个超时时间，当然也可以中断挂起，用B的interrupt的方法.<br>join的例子：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.concurrent.Callable;</div><div class="line"><span class="keyword">import</span> java.util.concurrent.ExecutorService;</div><div class="line"><span class="keyword">import</span> java.util.concurrent.Executors;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sleeper</span> <span class="keyword">extends</span> <span class="title">Thread</span></span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> <span class="keyword">int</span> duriation;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Sleeper</span><span class="params">(String name, <span class="keyword">int</span> sleepTime)</span></span>&#123;</div><div class="line"></div><div class="line">        <span class="keyword">super</span>(name);</div><div class="line"></div><div class="line">        duriation = sleepTime;</div><div class="line"></div><div class="line">        start();</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</div><div class="line"></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line"></div><div class="line">            sleep(duriation);</div><div class="line"></div><div class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line"></div><div class="line">            <span class="comment">// <span class="doctag">TODO:</span> handle exception</span></div><div class="line"></div><div class="line">            System.out.println(getName()+<span class="string">"  was interrupted. "</span>+<span class="string">" is interrupete "</span>+isInterrupted());</div><div class="line"></div><div class="line">            <span class="keyword">return</span>;</div><div class="line"></div><div class="line">        &#125;</div><div class="line"></div><div class="line">        System.out.println(getName()+<span class="string">" has awakened"</span>);</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Joiner</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> Sleeper sleeper;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Joiner</span><span class="params">(String name, Sleeper sleeper)</span></span>&#123;</div><div class="line"></div><div class="line">        <span class="keyword">super</span>(name);</div><div class="line"></div><div class="line">        <span class="keyword">this</span>.sleeper = sleeper;</div><div class="line"></div><div class="line">        start();</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</div><div class="line"></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line"></div><div class="line">            sleeper.join();</div><div class="line"></div><div class="line">            <span class="comment">//            sleep(1);</span></div><div class="line"></div><div class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line"></div><div class="line">            <span class="comment">// <span class="doctag">TODO:</span> handle exception</span></div><div class="line"></div><div class="line">            e.printStackTrace();</div><div class="line"></div><div class="line">        &#125;</div><div class="line"></div><div class="line">        System.out.println(getName() + <span class="string">" join completed"</span>);</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">threadTest</span></span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line"></div><div class="line">        <span class="comment">// TODO Auto-generated method stub</span></div><div class="line"></div><div class="line">        Sleeper sleeper = <span class="keyword">new</span> Sleeper(<span class="string">"a"</span>, <span class="number">15000</span>);</div><div class="line"></div><div class="line">        <span class="comment">//        Sleeper another = new Sleeper("b", 15000);</span></div><div class="line"></div><div class="line">        Joiner doc = <span class="keyword">new</span> Joiner(<span class="string">"doc"</span>, sleeper);</div><div class="line"></div><div class="line">        <span class="comment">//        sleeper.interrupt();</span></div><div class="line"></div><div class="line">        <span class="comment">//        another.interrupt();</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>对于Executor管理的线程，如果发生了异常，需要捕获，尝试通过try catch的方法是无法正确捕获的，这时候可以通过Thread.UncaugthExceptionHandler.uncaughtException()来捕获临近死亡时候的状态。<br>为什么使用executor<br>构造器中启动线程是很危险的，因为另一个任务可能会在构造器结束之前开始执行，这意味着该任务能够访问处于不稳定状态的对象。<br>可以替代线程组更好的管理线程</p>
<p>参考博客：</p>
<p><a href="http://www.iteye.com/topic/366591" target="_blank" rel="external">http://www.iteye.com/topic/366591</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/06/18/Map到Reduce的主流程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/06/18/Map到Reduce的主流程/" itemprop="url">Map到Reduce的主流程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-06-17T15:09:40-08:00">
                2014-06-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一、Map"><a href="#一、Map" class="headerlink" title="﻿﻿一、Map"></a>﻿﻿一、Map</h1><p>1、spilit 和map是多对一或者一对一，split主要就是通过inutFormat程序对数据切片后的分块。<br>2、spill  这个过程就是在map过程中，把map的循环队列中的数据从内存写道硬盘的过程，写的时候会排序<br>3、merge Map端的Merge主要是，在完成了所有的map计算后，把所有的spill的块和当前内存中的块合并，然后作为reduce的输入，注意，如果设置了combined的函数，在这个阶段也会进行combine<br>4、combine  也叫pre-reduce，就是把所有的spill和内存中的map进行排序。</p>
<h1 id="二、Map结果怎么传送给Reduce"><a href="#二、Map结果怎么传送给Reduce" class="headerlink" title="二、Map结果怎么传送给Reduce"></a>二、Map结果怎么传送给Reduce</h1><p>1、shuffle整个从map端传送数据到reduce端的过程<br>2、partitioner 通过map输出的key/value个数和reduce的数量来决定哪个数据由哪个reduce来处理</p>
<h1 id="三、reduce"><a href="#三、reduce" class="headerlink" title="三、reduce"></a>三、reduce</h1><p>1、mergeReduce阶段的Merge就是通过shuffle从不同的map端下载的数据进行合并，注意，这部分数据不会直接写入磁盘，而是先在内存中，到一定数量再写入。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/06/15/Python装饰器基本理解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/06/15/Python装饰器基本理解/" itemprop="url">Python装饰器基本理解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-06-14T15:41:01-08:00">
                2014-06-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>﻿﻿﻿﻿装饰器的亮点作用：</p>
<p>插入日志、性能测试、事务处理</p>
<p>装饰器的作用:为已经存在的对象添加额外的功能</p>
<p>类似与c++的回调函数</p>
<p>修饰符等价于包装调用</p>
<p>修饰函数必须返回一个“可调用对象”</p>
<p>内置的装饰器有三个，分别是staticmethod、classmethod和property，作用分别是把类中定义的实例方法变成静态方法、类方法和类属性</p>
<p>当有多个装饰器的时候，会从下往上调用，也就是调用离函数最近的一个装饰器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"> <span class="number">3</span> <span class="keyword">import</span> time</div><div class="line"> <span class="number">4</span> </div><div class="line"> <span class="number">5</span> <span class="function"><span class="keyword">def</span> <span class="title">timesLong</span><span class="params">(func)</span>:</span></div><div class="line"> <span class="number">6</span>     <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">()</span>:</span></div><div class="line"> <span class="number">7</span>         start = time.asctime()</div><div class="line"> <span class="number">8</span>         <span class="keyword">print</span> <span class="string">"At "</span> + repr(start) + <span class="string">":It is start"</span></div><div class="line"> <span class="number">9</span>         func()</div><div class="line"><span class="number">10</span>         end = time.asctime()</div><div class="line"><span class="number">11</span>         <span class="keyword">print</span> <span class="string">"At"</span> + repr(end) + <span class="string">"it is end"</span></div><div class="line"><span class="number">12</span>         <span class="keyword">return</span> end</div><div class="line"><span class="number">13</span>     <span class="keyword">return</span> call</div><div class="line"><span class="number">14</span>     </div><div class="line"><span class="number">15</span> @timesLong</div><div class="line"><span class="number">16</span> <span class="function"><span class="keyword">def</span> <span class="title">function</span><span class="params">()</span>:</span></div><div class="line"><span class="number">17</span>     y = <span class="number">0</span></div><div class="line"><span class="number">18</span>     <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line"><span class="number">19</span>         y = y + i + <span class="number">1</span></div><div class="line"><span class="number">20</span>         <span class="keyword">print</span> y </div><div class="line"><span class="number">21</span>     <span class="keyword">return</span> y</div><div class="line"><span class="number">22</span>     </div><div class="line"><span class="number">23</span> <span class="keyword">print</span> function()</div></pre></td></tr></table></figure>
<p>执行流程是：<br>​    1、调用print function<br>​    2、调用timeslong<br>​    3、调用timeslong.call<br>​    4、调用function<br>​    5、调用call的剩余部分<br>​    6、最后timeslong返回了call的返回结果，然后print了出来</p>
<p>所以输出是：</p>
<p>​    At ‘Tue May  6 19:32:55 2014’:It is start</p>
<p>1</p>
<p>3</p>
<p>6</p>
<p>10</p>
<p>15</p>
<p>21</p>
<p>28</p>
<p>36</p>
<p>45</p>
<p>55</p>
<p>At’Tue May  6 19:32:55 2014’it is end</p>
<p>Tue May  6 19:32:55 2014</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/06/14/通过MR读取Kafka的数据/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/06/14/通过MR读取Kafka的数据/" itemprop="url">通过MR读取Kafka的数据</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-06-13T15:13:27-08:00">
                2014-06-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="为什么要用MR读取Kafka"><a href="#为什么要用MR读取Kafka" class="headerlink" title="为什么要用MR读取Kafka"></a>为什么要用MR读取Kafka</h1><p>通过Mapreduce读取Kafka，有几个原因：</p>
<p>1、防止数据量大的读取，导致单台机器承载能力不够</p>
<p>2、利用MR的优势，更方便的处理数据</p>
<p>3、可以根据数据的类型，把数据直接存储到HDFS的不同路径上</p>
<h1 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h1><p>1、数据来源是kafka的consumer，所以不能用类似于TextInputFormat等自带的传统的数据切分的方法。<br>2、数据的Map过程需要对数据进行处理，为了减少对数据处理的次数，在Map的时候就需要把后面拼接路径的有效信息提取到<br>3、不需要有Reduce，只需要有直接的OutputStream就可以直接把不同路径的数据分发到不同的HDFS上<br>4、需要基本的ZK管理类，来直接读写偏移的信息</p>
<h1 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h1><h2 id="提交任务主类"><a href="#提交任务主类" class="headerlink" title="提交任务主类"></a>提交任务主类</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div></pre></td><td class="code"><pre><div class="line">writeNewOffsetFromConfig(conf,TOPIC,GROUP_ID,OFFSETSTART,<span class="keyword">true</span>);</div><div class="line">Job job = <span class="keyword">new</span> Job(conf, <span class="string">"Kafka.Consumer"</span>);</div><div class="line">job.setJarByClass(getClass());</div><div class="line">job.setMapperClass(KafkaMapper.class);</div><div class="line"></div><div class="line"><span class="comment">// input</span></div><div class="line">job.setInputFormatClass(KafkaInputFormat.class);</div><div class="line"></div><div class="line"><span class="comment">// output</span></div><div class="line">job.setOutputKeyClass(Text.class);</div><div class="line">job.setOutputValueClass(Text.class);</div><div class="line">job.setOutputFormatClass(KafkaOutputFormat.class);</div><div class="line">job.setNumReduceTasks(<span class="number">0</span>);</div><div class="line"></div><div class="line">KafkaOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(HDFS_PATH + <span class="string">"/"</span> + TOPIC));</div><div class="line"><span class="comment">//上面这部分的代码，展示了整个的流程：writeNewOffsetFromConfig是实现了一个从配置读取偏移，不使用上次默认偏移的方法。</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">writeNewOffsetFromConfig</span><span class="params">(Configuration conf,String topic,String groupId,String offsetStr,<span class="keyword">boolean</span> needBackUp)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">    ZkUtils zk = <span class="keyword">new</span> ZkUtils(conf);</div><div class="line">    String errorStr = <span class="string">""</span>;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (needBackUp)&#123;</div><div class="line">        backupLastOffset(zk, conf, topic, groupId); <span class="comment">//这个是备份之前的配置，主要通过topic来读取相应的brocker、partiotion、lastOffset的信息，然后存储到ZK。</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        <span class="keyword">if</span> (offsetStr == <span class="keyword">null</span> || offsetStr == <span class="string">""</span>)&#123; <span class="comment">//如果配置中未指定默认的起始地址，则接着上次的偏移开始读</span></div><div class="line">            <span class="keyword">return</span>;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        String [] splitStrs = offsetStr.split(<span class="string">","</span>);</div><div class="line">        logger.info(<span class="string">"Get Start message:"</span> + offsetStr);</div><div class="line">        <span class="keyword">if</span> (splitStrs.length &lt; <span class="number">1</span>)&#123;</div><div class="line">            logger.info(<span class="string">"INFO, when start message get splitStr: "</span>+offsetStr+<span class="string">" length :"</span> + splitStrs.length);</div><div class="line">            <span class="keyword">return</span>;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">for</span> ( String splitStr : splitStrs)&#123;</div><div class="line">            String [] offsetDetailInfos = splitStr.split(<span class="string">":"</span>);</div><div class="line">            <span class="keyword">if</span> (ffsetDetailInfos.length != <span class="number">2</span>)&#123;</div><div class="line">                errorStr = <span class="string">"Error: when change offset from config, get error system config (OFFSETSTART):"</span> + offsetStr;</div><div class="line">                logger.error(errorStr);</div><div class="line">            &#125;  <span class="keyword">else</span> &#123;</div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                    String partition = offsetDetailInfos[<span class="number">0</span>];</div><div class="line">                    <span class="keyword">long</span> offset = Long.valueOf(offsetDetailInfos[<span class="number">1</span>]);</div><div class="line">                    logger.info(<span class="string">"Info: update group:"</span> + groupId + <span class="string">" topic:"</span></div><div class="line">                        + topic + <span class="string">" parti:"</span> + partition + <span class="string">" offset:"</span>+ offset);</div><div class="line">                    zk.setLastCommit(groupId, topic, partition, offset,<span class="keyword">false</span>); <span class="comment">//把当前的偏移设置为从配置中读取到的默认偏移</span></div><div class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">                    errorStr = <span class="string">"When set offset which is read from config OFFSETSTART:"</span> + offsetStr + <span class="string">". Get Error:"</span> + e.toString();</div><div class="line">                    logger.error(errorStr);</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125; <span class="keyword">catch</span> (Exception e)&#123;</div><div class="line">        String lastOffset = zk.getLastOffset(groupId, topic);</div><div class="line">        <span class="keyword">if</span> (lastOffset != <span class="keyword">null</span>)&#123;</div><div class="line">            writeNewOffsetFromConfig(conf, topic, groupId, lastOffset,<span class="keyword">false</span>);</div><div class="line">        &#125;</div><div class="line">        errorStr = <span class="string">"Error: when change offset from config, get exception:"</span> + e.toString();</div><div class="line">        logger.error(errorStr);</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">        zk.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其它的MR的方法，接下来会依次介绍，可以看到，这里主要实现了InputFormat、 Mapper、OutputFormat的方法来实现从Kafka消费数据，然后根据数据的类型写到不同的HDFS上。</p>
<h2 id="关于InputSplit"><a href="#关于InputSplit" class="headerlink" title="关于InputSplit"></a>关于InputSplit</h2><p>﻿﻿在 MapReduce过程中，Map之前要进行的是Inputsplit，通过数据的切分，然后才能把数据分发到不同的节点上进行计算。</p>
<p>InputSplit整个过程包含了个重要的方法getSplits和getRecorder方法。</p>
<p>getSplits主要返回了一个InputSplit的对象，这个对象在逻辑上对整个数据进行了切片(注意，没有进行实际的物理切分)</p>
<p>切分的过程中，会涉及到分片大小的问题：</p>
<p>在默认的切分中，会通过下面的方式计算分片的大小：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">goalSize = totalSize / mapred.map.tasks <span class="comment">// mapred.map.tasks默认的配置为2</span></div><div class="line">minSize = max &#123;mapred.min.split.size, minSplitSize&#125;  <span class="comment">//mapred.min.split.size的默认配置为1</span></div><div class="line">splitSize = max (minSize, min(goalSize, dfs.block.size))  <span class="comment">//dfs.block.size默认为64M</span></div></pre></td></tr></table></figure>
<p>通过最后一个计算公式，可以看到，一般正常配置的情况下分片最大不会超过dfs.block.size(minSize一般小于分块的大小)</p>
<p>接下来分析我使用的分片方式</p>
<h3 id="1、定义kafka-consumer的数据进行了分片结构："><a href="#1、定义kafka-consumer的数据进行了分片结构：" class="headerlink" title="1、定义kafka consumer的数据进行了分片结构："></a>1、定义kafka consumer的数据进行了分片结构：</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> String brokerId;  <span class="comment">//数据所在的broker的Id</span></div><div class="line"><span class="keyword">private</span> String broker;  <span class="comment">// 数据所在的broker</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">int</span> partition; <span class="comment">//具体的每个brocker的某个partition</span></div><div class="line"><span class="keyword">private</span> String topic; <span class="comment">//读取的topic</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">long</span> lastCommit; <span class="comment">//记录上一次的偏移量</span></div></pre></td></tr></table></figure>
<h3 id="2、构建逻辑分片"><a href="#2、构建逻辑分片" class="headerlink" title="2、构建逻辑分片"></a>2、构建逻辑分片</h3><p>​     要想使用上面的数据结构来构建splits，需要从Kafka的consumer组中，查找指定的topic的各个数据。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">List splits = <span class="keyword">new</span> ArrayList();</div><div class="line">List partitions = zk.getPartitions(topic);  <span class="comment">//获取指定的topic的分区情况</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> (String partition : partitions) &#123;</div><div class="line">    String[] sp = partition.split(<span class="string">"-"</span>); <span class="comment">//在消费队列中，存储的格式为brocker-partition的，通过切分获取相应的brocker和partition</span></div><div class="line">    <span class="keyword">long</span> last = zk.getLastCommit(group, topic, partition); <span class="comment">//获取对应的brocker-partition的最后便宜量</span></div><div class="line">    InputSplit split = <span class="keyword">new</span> KafkaSplit(sp[<span class="number">0</span>], zk.getBroker(sp[<span class="number">0</span>]),topic, Integer.valueOf(sp[<span class="number">1</span>]), last); <span class="comment">//构建split分片对象</span></div><div class="line">    splits.add(split); <span class="comment">//在总的分片列表中加入刚创建的分片对象</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>最后返回splits，这样就获取到了所有的逻辑分片,这样的分片，可以保证，每个Map读取对应的brocker的数据(因为Map的数量是由split的个数决定的)。</p>
<h3 id="3、完成Reader"><a href="#3、完成Reader" class="headerlink" title="3、完成Reader"></a>3、完成Reader</h3><p>﻿﻿﻿﻿完成数据的逻辑分片后，需要对具体的某个分片，获取相应的数据，接下来就涉及到了InputSplit的另一个重要的方法。</p>
<p>RecordReader方法的使命是通过逻辑分片，读取相应的数据，然后传递给Map。</p>
<p>为什么用了使命呢，因为这部分是比较棘手的一部分。</p>
<p>这部分是返回一个RecordReader的对象，需要重写以下几个方法：</p>
<h4 id="1、initialize"><a href="#1、initialize" class="headerlink" title="1、initialize"></a>1、initialize</h4><p>初始化RecordReader对象，这个过程中，我会新开一个线程去从Kafka读取数据：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">kafka.api.FetchRequest request = <span class="keyword">new</span> FetchRequest(topic, partition, offset, fetchSize);</div><div class="line">messages = consumer.fetch(request);这里需要判断一下返回结构是否正确</div><div class="line"><span class="keyword">int</span> code = messages.getErrorCode();</div><div class="line"><span class="keyword">if</span> (code == <span class="number">0</span>) &#123;    </div><div class="line">  hasData = <span class="keyword">true</span>;  <span class="comment">//设置是否还有数据    </span></div><div class="line">  offset += messages.validBytes(); <span class="comment">// 把offset设置为当前的offset加上读出的数据的长度，为下次读取的偏移</span></div><div class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (hasData &amp;&amp; code == ErrorMapping.OffsetOutOfRangeCode()) &#123;    </div><div class="line">  <span class="comment">//表示已经没有数据了        </span></div><div class="line">  stop = <span class="keyword">true</span>; <span class="comment">//停止读取数据</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>但是这里访问kafka的时候，可能会有并发的问题，所以我加入了ArrayBlockingQueue队列来存储所有的message。在初始化阶段，还需要把偏移的start和end获取到：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">startOffset = SimpleConsumer kafka.consumer.KafkaContext.consumer.getOffsetsBefore(topic, partition, -<span class="number">2L</span>, <span class="number">1</span>)[<span class="number">0</span>];</div><div class="line">lastOffset = SimpleConsumer kafka.consumer.KafkaContext.consumer.getOffsetsBefore(topic, partition, -<span class="number">1L</span>, <span class="number">1</span>)[<span class="number">0</span>];</div></pre></td></tr></table></figure>
<h4 id="2、nextKeyValue"><a href="#2、nextKeyValue" class="headerlink" title="2、nextKeyValue"></a>2、nextKeyValue</h4><p>这个方法是最为重要的一个方法，主要是把获取到的逻辑分片，返回一个key value的数据对，传递给Map。</p>
<p>作为最重要的处理方法，在我们这里其实很简单，就是读取当前的message，把offset作为key，获取到的message作为value返回。</p>
<p>通过 Iterator iterator.hasNext判断是否还有新的message，如果有的话，调用Iterator iterator.message获取到当前的消息，然后设置key和value返回。</p>
<h4 id="3、getProgress"><a href="#3、getProgress" class="headerlink" title="3、getProgress"></a>3、getProgress</h4><p>这个函数主要是通过初始化的时候获取到的起始偏移来返回当前的进度，</p>
<h4 id="4、close"><a href="#4、close" class="headerlink" title="4、close"></a>4、close</h4><p>在关闭当前的读取之后，需要把读取完之后的offset写回，</p>
<p>和split的时候获取的方法想对应，这里调用的是：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">zk.setLastCommit(group, ksplit.getTopic(), partition, pos, <span class="keyword">true</span>);</div></pre></td></tr></table></figure>
<p>写回的偏移保证下次的split的时候能从新的偏移开始读数据。</p>
<p>通过createRecordReader(旧版的API里面为RecorderReader)返回上面的一个recorder对象，就可以提供在Map的时候处理的KV。</p>
<h2 id="实现Map类"><a href="#实现Map类" class="headerlink" title="实现Map类"></a>实现Map类</h2><p>﻿其实Map类很简单，</p>
<p>分到的key为上一次的offset</p>
<p>分到的 value 为上一次读取到的message</p>
<p>为什么是上一次呢，因为在上次读取完之后，已经修改了offset，对于每个map，下一次要处理的数据，不再是同一个。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"> Text out = <span class="keyword">new</span> Text();</div><div class="line">out.set(value.getBytes(), <span class="number">0</span>, value.getLength());</div></pre></td></tr></table></figure>
<p>上面的两次可以获取到具体的message，然后可以进行处理，这里的处理不再细说。</p>
<h2 id="实现output"><a href="#实现output" class="headerlink" title="实现output"></a>实现output</h2><p>﻿﻿首先我们的OutPut类继承了FileOutputFormat方法，这样就可以实现不同的数据存储到不同路径的需求。    </p>
<p>然后需要重写方法getRecordWriter，这个方法需要返回一个RecordWriter的数据，这个数据中指定了不同的数据存储到不同的节点上。</p>
<p>这里需要补充一个问题，因为我们的HDFS上需要存储数据的基本路径可能是固定的，比如/user/XXX的数据存放在相应的目录下。我们可以在主类中执行最基本的路径：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">OutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(HDFS_PATH + <span class="string">"/"</span> + TOPIC));</div></pre></td></tr></table></figure>
<p>然后可以通过一下方法在output的时候获取到相应的路径：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Path outputPath = <span class="keyword">super</span>.getOutputPath(conf);</div></pre></td></tr></table></figure>
<p>基本路径取定后，我们需要实现一个RecordWriter类，这个类中可以定义构造函数，指定要写入的job名称和基础路径。<br>而RecordWriter需要重写以下的方法：</p>
<p>在write中是将指定的数据写入到指定的HDFS上，基本路径在之前已经获取到，然后需要指定写入的文件名，我们可以简单的使用key.toString()作为文件名称。</p>
<p>然后通过Path file = new Path(workPath,key.toString() );拼接出来新的路径。</p>
<p>接下来创建一个HDFS的输出流：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">FSDataOutputStream fileOut = file.getFileSystem(conf).create(file, <span class="keyword">true</span>);</div></pre></td></tr></table></figure>
<p>调用out.write方法，把刚才获取到的value值写到流中。</p>
<p> 这里的write实现了数据的写入。</p>
<p>在close的时候需要做的是，把所有打开的RecordWriter都关闭。</p>
<p>我在实现的时候使用了private HashMap&gt; recordWriters 类存储所有的对象，然后轮寻所有hashmap,</p>
<p>依次调用close方法关闭相应的输出流，最后应该清空hashMap,这样可以最快的释放内存。</p>
<h2 id="ZK偏移量的管理"><a href="#ZK偏移量的管理" class="headerlink" title="ZK偏移量的管理"></a>ZK偏移量的管理</h2><p>﻿zk的使用，可以使用ZkClient或者使用org.apache.zookeeper.ZooKeeper，实现，本文使用的是前者。</p>
<p>zk的创建：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ZkClient client = <span class="keyword">new</span> ZkClient(zk, stimeout, ctimeout, <span class="keyword">new</span> StringSerializer() );</div></pre></td></tr></table></figure>
<p>zk获取目录的列表：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">client.getChildren(path);</div></pre></td></tr></table></figure>
<p>zk获取节点的值：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">client.readData(znode, <span class="keyword">true</span>);</div></pre></td></tr></table></figure>
<p>zk删除节点：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">client.delete(path);</div></pre></td></tr></table></figure>
<p>zk创建路径：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">client.createPersistent(path, <span class="keyword">true</span>);</div></pre></td></tr></table></figure>
<p>zk写值</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">client.writeData(path, value);</div></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/03/23/c-实现算法插入排序/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/03/23/c-实现算法插入排序/" itemprop="url">c++实现算法插入排序</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-03-22T13:58:45-08:00">
                2014-03-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">showResult</span><span class="params">(<span class="keyword">int</span> *sortList, <span class="keyword">int</span> number)</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> i = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span> (; i &lt;= number; i++)</div><div class="line">    &#123;</div><div class="line">        <span class="built_in">cout</span>&lt;&lt;sortList[i]&lt;&lt;<span class="string">" "</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="built_in">endl</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">insertSort</span><span class="params">( <span class="keyword">int</span> *sortList, <span class="keyword">int</span> number)</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> j = <span class="number">1</span>;</div><div class="line">    <span class="keyword">for</span> (; j &lt;= number; j++)</div><div class="line">    &#123;</div><div class="line">        <span class="keyword">int</span> current = j<span class="number">-1</span>;</div><div class="line">        <span class="keyword">int</span> pre = j;</div><div class="line">        <span class="keyword">while</span> (current &gt;= <span class="number">0</span> &amp;&amp; sortList[current] &gt; sortList[pre] )</div><div class="line">        &#123;</div><div class="line">            <span class="keyword">int</span> tmp = sortList[current];</div><div class="line">            sortList[current] = sortList[pre];</div><div class="line">            sortList[pre] = tmp;</div><div class="line">            current--;</div><div class="line">            pre--;</div><div class="line">        &#125;</div><div class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">"the "</span>&lt;&lt;j&lt;&lt;<span class="string">" time: "</span>;</div><div class="line">        showResult(sortList,number<span class="number">-1</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span> <span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> sortList[<span class="number">10</span>] = &#123;<span class="number">5</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">3</span>&#125;;</div><div class="line">    <span class="keyword">int</span> number = <span class="number">10</span>;</div><div class="line">    insertSort(sortList,number<span class="number">-1</span>);</div><div class="line">    showResult(sortList,number<span class="number">-1</span>);</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">aiping.lap</p>
              <p class="site-description motion-element" itemprop="description">aiping.liang s home</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">41</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">aiping.lap</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.3</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  








  





  

  

  

  

  

  

</body>
</html>
