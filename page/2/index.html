<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="aiping.liang s home">
<meta property="og:type" content="website">
<meta property="og:title" content="Aiping.LAP">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Aiping.LAP">
<meta property="og:description" content="aiping.liang s home">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Aiping.LAP">
<meta name="twitter:description" content="aiping.liang s home">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/2/"/>





  <title>Aiping.LAP</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Aiping.LAP</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">认真工作，快乐生活</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/09/04/HBase的一些元数据操作/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/09/04/HBase的一些元数据操作/" itemprop="url">HBase的一些元数据操作</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-09-03T20:26:52-08:00">
                2014-09-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="NameSpace"><a href="#NameSpace" class="headerlink" title="NameSpace"></a>NameSpace</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>NameSpace 命名空间在很多地方都会用，主要其实是在指定一个规则，或者一个环境变量。在Hbase0.96后，增加了命名空间的概念。这个NameSpace是存储在ZK上的，类似与RDBMS里面的database的概念，主要似乎用来做权限管理和安全控制的。 </p>
<h2 id="收益"><a href="#收益" class="headerlink" title="收益"></a>收益</h2><p> 命名空间，是一个逻辑分组的概念，可见这个概念是为了多用户准备的，为的就是资源隔离。资源的隔离直接带来了几个特性： </p>
<p>1、资源在Hbase中，逻辑上可以用的资源是table，在物理上可以用的资源是region(当然还有更细力度的storeFile) </p>
<p>2、资源的分配主要是用来控制不同的namespace可以使用的资源</p>
<p>3、资源的存储 Hbase对于一个逻辑的资源table，如何能根据其所在的空间，被固定的分配到一个或者个多个regionserver上 </p>
<p>4、为不同的用户提供不同的资源 </p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><p>在系统中，有两个默认的命名空间，一个是hbase，是系统表.meta.使用的；另一个是default，主要是提供所有没有指定命名空间表用的。 </p>
<p>接下来分析一个自定义的命名空间的创建和使用</p>
<p>创建一个命名空间 最简单的办法是在hbase shell中创建，例如：create_namespace ‘lap’， </p>
<p>在创建完后，最直观的是在/hbase/data/下多了一个文件夹lap，同时，在zk的/hbase/namespace下也会多一个lap的节点。</p>
<p>在命名空间下创建一个表 create ‘lap:lapTable’,’info’。</p>
<p>删除一个命名空间 drop_namespace ‘lap’ </p>
<h3 id="使用namespace实现资源隔离"><a href="#使用namespace实现资源隔离" class="headerlink" title="使用namespace实现资源隔离"></a>使用namespace实现资源隔离</h3><p>每一个Table都隶属于一个NameSpace之下，每个NameSpace都有自己的Quota信息。例如Region的个数、hfile存储空间、甚至HBase之上的RPC的流量控制等。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">NamespaceDescriptor namespaceDescriptor = admin.getNamespaceDescriptor(tableName);</div><div class="line">namespaceDescriptor.setConfiguration(<span class="string">"hbase.namespace.quota.maxregion"</span>, <span class="string">"10"</span>);</div><div class="line">namespaceDescriptor.setConfiguration(<span class="string">"hbase.namespace.quota.maxtables"</span>, <span class="string">"10"</span>);</div></pre></td></tr></table></figure>
<p>这一部分在0.10中还未实现，相信在新的版本中，一定能给出好的解决方案。</p>
<h1 id="Table元数据操作"><a href="#Table元数据操作" class="headerlink" title="Table元数据操作"></a>Table元数据操作</h1><h2 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h2><p>table的操作 对表的操作主要包括增、删、改、查，接下来用一段代码来展示</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">voidcreateTable</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> Exception</span>&#123;</div><div class="line"> 	Configuration conf = HBaseConfiguration.create();</div><div class="line">	HBaseAdmin admin = <span class="keyword">new</span> HBaseAdmin(conf);</div><div class="line">	<span class="comment">//创建一个新的命名空间</span></div><div class="line">	admin.createNamespace(NamespaceDescriptor.create(<span class="string">"lap"</span>).build());</div><div class="line">	<span class="comment">//申请新的表到该命名空间</span></div><div class="line">	HTableDescriptor tableDescriptor = <span class="keyword">new</span> HTableDescriptor(TableName.valueOf(<span class="string">"lap:"</span>+tableName));</div><div class="line">	<span class="comment">//设置表的属性，比如setDurability是设置acl的属性，而后一个直接设置所有的属性，当然这部分还包括列簇的属性</span></div><div class="line">	tableDescriptor.setDurability(Durability.SYNC_WAL);</div><div class="line">	tableDescriptor.setConfiguration(<span class="string">"BLOOMFILTER"</span>, <span class="string">"ROW"</span>);</div><div class="line"></div><div class="line">  	<span class="comment">//创建一个列簇并且加入到表中</span></div><div class="line">	HColumnDescriptor hColumnDescriptor = <span class="keyword">new</span> HcolumnDescriptor(<span class="string">"cf"</span>);</div><div class="line">	<span class="comment">//设置列簇的属性</span></div><div class="line">	hColumnDescriptor.setBloomFilterType(BloomType.ROW);</div><div class="line">	tableDescriptor.addFamily(hColumnDescriptor);</div><div class="line">	<span class="comment">//创建表</span></div><div class="line">	admin.createTable(tableDescriptor);</div><div class="line">	<span class="comment">//关闭链接</span></div><div class="line">	admin.close();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面可以看出来，表有两个Descriptor需要设置，一个是namespace的，一个是本身的。</p>
<p>表的命名空间的描述短时间内应该不会有很大的应用，因为不是很完善。而本身的属性会有很多需要设置的，比如：DURABILIT（设置写日志的方式）、MEMSTORE_FLUSHSIZE（memstore刷到硬盘上的大小）、BLOOMFILTER（roekey的头压缩）、DATA_BLOCK_ENCODING（数据部分的压缩）、TTL（过期时间）、IN_MEMORY（是否在内存中）等一系列的属性。这些属性会直接影响到表的查询和写入速度等特性，所以需要根据实际的数据设置。</p>
<h2 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deleteTable</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> Exception</span>&#123;</div><div class="line">	Configuration conf = HBaseConfiguration.create();</div><div class="line">	HBaseAdmin admin = <span class="keyword">new</span> HBaseAdmin(conf);</div><div class="line">	<span class="keyword">if</span>(admin.tableExists(tableName))&#123;</div><div class="line">		<span class="keyword">try</span>&#123;</div><div class="line">			<span class="comment">//必须要禁用表</span></div><div class="line">			admin.disableTable(tableName);</div><div class="line">			<span class="comment">//删除表</span></div><div class="line">			admin.deleteTable(tableName);</div><div class="line">		&#125;<span class="keyword">catch</span>(Exception e)&#123;</div><div class="line">			e.printStackTrace();</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	admin.close();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>可见，删除表的时候就比较简单了，直接用admin的对象，禁用表（这里必须禁用，防止别人正在操作该表，出现异常的问题），然后直接删除就可以了，在命令行是同样的道理。</p>
<h2 id="更新表"><a href="#更新表" class="headerlink" title="更新表"></a>更新表</h2><p>更新表分为两个部分，一部分是更新表的属性，一部分是更新表中的数据，所以这里要分两个部分讲： 对于更新表的属性，这里依旧要用表的描述符来修改，主要的操作包括，增加删除一个列簇，修改一个列簇的属性，具体如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">alterTable</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> Exception</span>&#123;</div><div class="line">	Configuration conf = HBaseConfiguration.create();</div><div class="line">	HBaseAdmin admin= <span class="keyword">new</span> HBaseAdmin(conf);</div><div class="line">	<span class="keyword">if</span>(admin.tableExists(tableName))&#123;</div><div class="line">		<span class="comment">//和删除同样的道理，在更改表的描述符之前，必须先disable掉表</span></div><div class="line">		admin.disableTable(tableName);</div><div class="line">		<span class="comment">//获取当前表的描述符</span></div><div class="line">      	HTableDescriptor descriptor = admin.getTableDescriptor(Bytes.toBytes(tableName));</div><div class="line">		<span class="comment">//判断如果存在info的列簇，删除</span></div><div class="line">		<span class="keyword">if</span>(descriptor.hasFamily(Bytes.toBytes(<span class="string">"info"</span>)))&#123;</div><div class="line">			descriptor.removeFamily(Bytes.toBytes(<span class="string">"info"</span>));</div><div class="line">         &#125;</div><div class="line">		<span class="comment">//创建一个新的列簇</span></div><div class="line">		HColumnDescriptor hColumnDescriptor = <span class="keyword">new</span> HColumnDescriptor(Bytes.toBytes(<span class="string">"lapNew"</span>));</div><div class="line">		<span class="comment">//设置列簇的属性</span></div><div class="line">		hColumnDescriptor.setCompressionType(Algorithm.GZ);</div><div class="line">		<span class="comment">//为表增加一个新的列簇</span></div><div class="line">      	descriptor.addFamily(hColumnDescriptor);</div><div class="line">		<span class="comment">//获取到原先表中的列簇对象</span></div><div class="line">		HColumnDescriptor changeInfo = descriptor.getFamily(Bytes.toBytes(<span class="string">"clomun"</span>));</div><div class="line">		<span class="comment">//修改该对象的属性</span></div><div class="line">		changeInfo.setBlockCacheEnabled(<span class="keyword">true</span>);</div><div class="line">		<span class="comment">//让新的表描述符生效</span></div><div class="line">		admin.modifyTable(tableName, descriptor);</div><div class="line">		<span class="comment">//启用表</span></div><div class="line">		admin.enableTable(tableName);</div><div class="line">      	admin.close();</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>对于表内容的修改，其实就是增加一行，因为Hbase的存储直接依赖与HDFS，而HDFS是只支持创建和追加的，不支持某一行的单独修改，所以Hbase对表的增删改的操作，事实上都是在对表增加，不同的是，如果是删除，会在表中删除字段标记为true，在表进行major compaction的时候，会删除该行。对表的修改实际是，增加一个新的时间戳或者新版本的行，在查询的时候，会按照时间戳排序，保证能查到最新的修改。鉴于上面这些，这里不会单独讲表内容的修改，会在接下来文章中详细将对与表内容的所有动作。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/08/17/HBase读写性能相关-请求不均匀的问题优化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/08/17/HBase读写性能相关-请求不均匀的问题优化/" itemprop="url">HBase读写性能相关-请求不均匀的问题优化</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-08-16T21:39:13-08:00">
                2014-08-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="影响到随机读的速度的主要因素"><a href="#影响到随机读的速度的主要因素" class="headerlink" title="影响到随机读的速度的主要因素"></a>影响到随机读的速度的主要因素</h1><p>1、请求次数的分布均衡、</p>
<p>2、StoreFile数量、</p>
<p>3、表属性的设置</p>
<p>4、Cache大小以及命中率</p>
<p>今天主要讨论的是请求不均匀的问题</p>
<h1 id="Hbase的负载均衡的判断"><a href="#Hbase的负载均衡的判断" class="headerlink" title="Hbase的负载均衡的判断"></a>Hbase的负载均衡的判断</h1><p>各region server的region数是否均衡</p>
<p>读写请求是否均衡分布。</p>
<h1 id="保证Hbase集群LB的方法"><a href="#保证Hbase集群LB的方法" class="headerlink" title="保证Hbase集群LB的方法"></a>保证Hbase集群LB的方法</h1><p>对于region的个数，Hbase的balance策略会保证</p>
<p>对于table的region分布均衡，读请求仍然不均衡分布的情况，说明应用的请求有热点的状况，如这种状况造成了读速度的不OK，可以手工将region进行拆分，并分配到不同的region server上，这是hbase很简单的一种应对热点的解决方法。</p>
<h1 id="统计方法"><a href="#统计方法" class="headerlink" title="统计方法"></a>统计方法</h1><p>先用execel把相应表的RS列和TPS列单独取出来放到一个文件中，用如下统计脚本进行统计</p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/perl -w </span></div><div class="line"><span class="keyword">use</span> Data::Dumper; </div><div class="line"><span class="keyword">open</span>(FILE,<span class="string">"&lt;"</span>,$ARGV[<span class="number">0</span>]) <span class="keyword">or</span> <span class="keyword">die</span> <span class="string">"can not open file"</span>; </div><div class="line"><span class="keyword">my</span> %allStatus = (); </div><div class="line"><span class="keyword">while</span> ()&#123; </div><div class="line">    <span class="keyword">if</span> ($_ =~ <span class="regexp">m/(\S+)\s+(\d+)/</span>)&#123; </div><div class="line">	     $allStatus&#123;$1&#125; += $2; </div><div class="line">	&#125;   </div><div class="line">&#125; </div><div class="line"><span class="keyword">foreach</span> <span class="keyword">my</span> $key (<span class="keyword">keys</span>%allStatus)&#123; </div><div class="line">    <span class="keyword">print</span> <span class="string">"$key "</span>.$allStatus&#123;$key&#125;.<span class="string">"\n"</span>; </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>选择了TPS比较高，又经常访问的表CleanOfflineUserProfileTempV1，进行了查询。结果如下(这里省略)</p>
<p>可以明显看到分成了4个量级别的请求数目，不同的之间TPS相差了50W的请求量，可见不同的regionserver之间的请求差别很大。</p>
<h1 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h1><p> 通过实验对比因为访问不均衡导致的时间差别。</p>
<p> 1、先随机的写入100w条的数据           </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">time ./hbase org.apache.hadoop.hbase.PerformanceEvaluation  randomWrite 1</div></pre></td></tr></table></figure>
<p>2、查看regionserver的分布状态</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">TestTable,,1405596957945.af6be5a5763589d054d77ab533af721f. hadoop141:60030</div><div class="line">TestTable,0000262285,1405597137509.f376305528ef3570901120a457d80a2f. hadoop141:60030</div><div class="line">TestTable,0000393515,1405597137509.4a9738e4f29128709a0b2cd63ed8d39d. hadoop141:60030</div><div class="line">TestTable,0000524451,1405597162495.47c7234c3d48a1b75a521eaaedae72b4. hadoop141:60030</div><div class="line">TestTable,0000654788,1405597162495.53c2647b5f707032dc3839cdea26965e. hadoop141:60030</div><div class="line">TestTable,0000785234,1405596978669.810195f07f006d900913898abf4385e1. hadoop141:60030</div></pre></td></tr></table></figure>
<p>3、接下来进行实验的重点部分</p>
<p>需要用到的命令</p>
<p>a、测试100w条的随机读</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">time ./hbase org.apache.hadoop.hbase.PerformanceEvaluation  randomRead 1</div></pre></td></tr></table></figure>
<p>b、关闭balancer</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hbase shell &gt; balance_switch flase</div></pre></td></tr></table></figure>
<p>c、手动move region</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&quot;move &apos;96b1f2b1569aec38030d2c0c95eb9dc5&apos;,&apos;dw-perf-11,60020,1328178878605&apos;</div></pre></td></tr></table></figure>
<p>其中第一个参数为要复制的region的startKey，第二个参数分别是要迁移到的机器，端口号，要插入的起始地址</p>
<p>调整后的测试结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">TestTable,,1405596957945.af6be5a5763589d054d77ab533af721f. hadoop131:60030</div><div class="line">TestTable,0000262285,1405597137509.f376305528ef3570901120a457d80a2f. hadoop132:60030</div><div class="line">TestTable,0000393515,1405597137509.4a9738e4f29128709a0b2cd63ed8d39d. hadoop142:60030</div><div class="line">TestTable,0000524451,1405597162495.47c7234c3d48a1b75a521eaaedae72b4. hadoop141:60030</div><div class="line">TestTable,0000654788,1405597162495.53c2647b5f707032dc3839cdea26965e. hadoop141:60030</div><div class="line">TestTable,0000785234,1405596978669.810195f07f006d900913898abf4385e1. hadoop132:60030</div></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">调整后</th>
<th style="text-align:center">调整前</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">内部执行时间</td>
<td style="text-align:center">1156830ms</td>
<td style="text-align:center">1741539ms</td>
</tr>
<tr>
<td style="text-align:center">在用户态的耗时</td>
<td style="text-align:center">3m38.686s</td>
<td style="text-align:center">3m34.142s</td>
</tr>
<tr>
<td style="text-align:center">程序本身的耗时</td>
<td style="text-align:center">19m20.268s</td>
<td style="text-align:center">29m5.021s</td>
</tr>
</tbody>
</table>
<p>可见，在这个实验中，对于不同的分布，对请求的结果差会有30%到40%的性能差。</p>
<h1 id="修复方法"><a href="#修复方法" class="headerlink" title="修复方法"></a>修复方法</h1><p>​      手动的把表进行split，表被拆分到小表后，会使访问更加均匀，也可以用其它方法。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/08/15/HBase读写性能相关-客户端的程序优化建议/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/08/15/HBase读写性能相关-客户端的程序优化建议/" itemprop="url">HBase读写性能相关-客户端的程序优化建议</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-08-14T21:35:16-08:00">
                2014-08-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="对于rowkey的设计"><a href="#对于rowkey的设计" class="headerlink" title="对于rowkey的设计"></a>对于rowkey的设计</h1><p>a、表的设计，尽量要把一起访问的放到一个cf里面</p>
<p>b、借助HBase单行操作的原子性。一个用户的所有信息存储成一行,这包括消息本身、Message Index、Search Index、以及相关Meta Data(Actions:addMessage、markAsRead,etc)</p>
<p>c、存储时，数据按照Row key的字典 序(byte order)排序存储。设计key时，要充分排序存储这个特性，将经常一起读取的行存储放到一起。(位置相关性)</p>
<p>d、推介RowKey：md5sum(userid)+userid。</p>
<h2 id="关于表的scan操作"><a href="#关于表的scan操作" class="headerlink" title="关于表的scan操作"></a>关于表的scan操作</h2><p>a、每次遍历完Scan的返回结果集后，都不要忘了关闭ResultScanner对象，否则会引发问题。</p>
<p>b、如果生产环境有mapred任务去scan hbase的时候，如果读取的是热点数据，可以在mapred scan类中加一个</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scan.setCacheBlocks(<span class="keyword">true</span>)</div></pre></td></tr></table></figure>
<p>c、几个重要的方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setBatch</span><span class="params">(<span class="keyword">int</span> batch)</span> ：<span class="comment">//为设置获取记录的列个数，默认无限制，也就是返回所有的列</span></span></div><div class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setCaching</span><span class="params">(<span class="keyword">int</span> caching)</span>：<span class="comment">//每次从服务器端读取的行数，默认为配置文件中设置的值,可以减少RPC的次数：</span></div><div class="line">HTable.<span class="title">setScannerCaching</span><span class="params">(<span class="keyword">int</span> scannerCaching)</span></div></pre></td></tr></table></figure>
<p>注意：有三个地方可以进行配置(上面三个参数)：</p>
<p>1）在HBase的conf配置文件中进行配置；</p>
<p>2）通过调用HTable.setScannerCaching(int scannerCaching)进行配置；</p>
<p>3）通过调用Scan.setCaching(int caching)进行配置。三者的优先级越来越高。</p>
<p> RPCs=(Rows* Cols per Row) / Min(Cols per Row, Batch size) / Scanner caching</p>
<p>d、默认情况下，HBase的自动刷新功能是处于激活状态的。则样每一个单个的Put都会自动调用一次RPC向server发送一次请求，为性能计，可以关闭自动刷新功能(setAutoFlush(false))，这样会客户端的write buffer已“满”或客户端强制flushComits()刷新再或者调用HTable的close()时才调用RPC一次把多个Put操作发送给服务器。</p>
<h2 id="关于put操作"><a href="#关于put操作" class="headerlink" title="关于put操作"></a>关于put操作</h2><p>a、在Put操作保存数据时，可以考虑关闭WAL的功能——调用writeToWAL(false)，这意味着在写数据时不会讲数据先写到WAL日志文件，直接写入memstore，这可以提高一些性能。但是者会带来一些副作用，那就是当Region Server出问题时，没有写log的数据会丢失，这在保存重要数据时会带来很大的危险，只有在保存不南无重要的数据时才考虑这样做。</p>
<p>或者设置setDeferredLogFlush为true，服务端将采用异步写hlog的方式，客户端写响应时间会降到1ms以下。如果regionserver挂掉会丢失1s的数据。同样有数据丢失风险，这种方式会小很多很多（rs被kill -9或者物理机器直接宕机），而且写入的数据马上可以被读取到，客户端也不需要采取特别的策略。</p>
<p>b、setWriteBufferSize可以设置table的客户端缓存。</p>
<p>c、通过batch 方式来put：HTable.put(List)</p>
<h1 id="其它的一些技术"><a href="#其它的一些技术" class="headerlink" title="其它的一些技术"></a>其它的一些技术</h1><p> 使用filter <a href="http://mysun.iteye.com/blog/1584635" target="_blank" rel="external">http://mysun.iteye.com/blog/1584635</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/07/30/HBase读写性能相关-关于缓存/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/07/30/HBase读写性能相关-关于缓存/" itemprop="url">HBase读写性能相关-关于缓存</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-07-29T19:24:06-08:00">
                2014-07-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="命中缓存的重要性"><a href="#命中缓存的重要性" class="headerlink" title="命中缓存的重要性"></a>命中缓存的重要性</h1><p>做一个简单的小实验，读取同样5条数据，命中catch和不命中catch的对比：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[hadoop@bjlg-49p21-hadoop-client03 hbase]$ ./start.sh </div><div class="line">83bd782bcb74fca2000032170000503e4fe29093 57848 </div><div class="line">a0225452001ccb15000055ce0004c15d538f05d9 895 </div><div class="line">a0225452001ccb150000793e00001258538efb1c 606 </div><div class="line">b12ed4ae526364b50007d61e0024a45652ee6a4e 189248 </div><div class="line">92fbd4ae526c85c80008fb96000997fa514811d6 3387</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[hadoop@bjlg-49p21-hadoop-client03 hbase]$ ./start.sh </div><div class="line">83bd782bcb74fca2000032170000503e4fe29093 4928 </div><div class="line">a0225452001ccb15000055ce0004c15d538f05d9 660 </div><div class="line">a0225452001ccb150000793e00001258538efb1c 762 </div><div class="line">b12ed4ae526364b50007d61e0024a45652ee6a4e 1227 </div><div class="line">92fbd4ae526c85c80008fb96000997fa514811d6 1218</div></pre></td></tr></table></figure>
<h1 id="提高缓存命中率的理论"><a href="#提高缓存命中率的理论" class="headerlink" title="提高缓存命中率的理论"></a>提高缓存命中率的理论</h1><p>1、总的catch的大小，这个调整主要通过提高Regionserver的堆内存和提高hfile.block.cache.size的百分比来提高</p>
<p>2、数据本身的合理设计，把可能会一起访问的数据设计到一起，把需要整行读取的数据设计到同一个CF里面。如果数据本身不存在热点，就会导致访问完全随机，就只能靠加机器来提高命中率了。</p>
<p>3、block块的大小，由于每次存放到catch中的数据是一整个block，而不是某个key-value。所以一般来说，不同的数据，有着不同的效果。对于非重复有热点的数据来说，如果block块设置大的话，catch命中率会随着块的增大而增加；对于重复的无热点数据来说，刚好相反。(这里要插一句，block本身的大小很相影响随机读的效果，如果块大的话，索引就相对较小，随机读效果就差)</p>
<h1 id="关于Hbase的catch本身的实现"><a href="#关于Hbase的catch本身的实现" class="headerlink" title="关于Hbase的catch本身的实现"></a>关于Hbase的catch本身的实现</h1><p>在Hbase的每个Regionserver中，一共维护了3个catch队列：</p>
<p>Single：如果一个Block第一次被访问，则放在这一优先级队列中；</p>
<p>Multi：如果一个Block被多次访问，则从Single队列移到Multi队列中；</p>
<p>InMemory：如果一个Block是inMemory的，则放到这个队列中，例如-ROOT-表和.META.表。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">hbase(main):002:0&gt; describe &apos;.META.&apos; </div><div class="line">DESCRIPTION                                                       ENABLED                           </div><div class="line"> &#123;NAME =&gt; &apos;.META.&apos;, IS_META =&gt; &apos;true&apos;, FAMILIES =&gt; [&#123;NAME =&gt; &apos;inf true                              </div><div class="line"> o&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, BLOOMFILTER =&gt; &apos;NONE&apos;, REPLIC                                   </div><div class="line"> ATION_SCOPE =&gt; &apos;0&apos;, COMPRESSION =&gt; &apos;NONE&apos;, VERSIONS =&gt; &apos;10&apos;, TTL                                   </div><div class="line">  =&gt; &apos;2147483647&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, KEEP_DELETED_CELLS =&gt; &apos;fa                                   </div><div class="line"> lse&apos;, BLOCKSIZE =&gt; &apos;8192&apos;, ENCODE_ON_DISK =&gt; &apos;true&apos;, IN_MEMORY =                                   </div><div class="line"> &gt; &apos;true&apos;, BLOCKCACHE =&gt; &apos;true&apos;&#125;]&#125;                                                                  </div><div class="line">1 row(s) in 0.0550 seconds </div><div class="line">hbase(main):003:0&gt; describe &apos;t3&apos; </div><div class="line">DESCRIPTION                                                       ENABLED                           </div><div class="line"> &#123;NAME =&gt; &apos;t3&apos;, FAMILIES =&gt; [&#123;NAME =&gt; &apos;f3&apos;, DATA_BLOCK_ENCODING = true                              </div><div class="line"> &gt; &apos;NONE&apos;, BLOOMFILTER =&gt; &apos;NONE&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;, VERSI                                   </div><div class="line"> ONS =&gt; &apos;3&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, TTL =&gt; &apos;                                   </div><div class="line"> 2147483647&apos;, KEEP_DELETED_CELLS =&gt; &apos;false&apos;, BLOCKSIZE =&gt; &apos;65536&apos;                                   </div><div class="line"> , IN_MEMORY =&gt; &apos;false&apos;, ENCODE_ON_DISK =&gt; &apos;true&apos;, BLOCKCACHE =&gt;                                    </div><div class="line"> &apos;true&apos;&#125;, &#123;NAME =&gt; &apos;f4&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, BLOOMFILT                                   </div><div class="line"> ER =&gt; &apos;NONE&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;, VERSIONS =&gt; &apos;3&apos;, COMPRES                                   </div><div class="line"> SION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, TTL =&gt; &apos;2147483647&apos;, KEEP_D                                   </div><div class="line"> ELETED_CELLS =&gt; &apos;false&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, IN_MEMORY =&gt; &apos;fal                                   </div><div class="line"> se&apos;, ENCODE_ON_DISK =&gt; &apos;true&apos;, BLOCKCACHE =&gt; &apos;true&apos;&#125;]&#125;                                             </div><div class="line">1 row(s) in 0.0470 seconds</div></pre></td></tr></table></figure>
<p>这三个队列Single、Multi、InMemory分别占用了总大小的0.25、0.50和0.25。队列如何生成的呢？对于第一次访问到集群的数据，如果不是被标记为IN_MEMORY，会将它写入single队列，否则写入memory队列。当再次访问该数据并且在single中读到了该数据时，single会升级为multi。通过将缓存分级的好处是避免Cache之间相互影响，尤其是对HBase来说像Meta表这样的Cache应该保证高优先级。</p>
<h1 id="统计命中率的方法"><a href="#统计命中率的方法" class="headerlink" title="统计命中率的方法"></a>统计命中率的方法</h1><p>目前在ganglia中没有提供特别有效的命中率的详细统计，主要通过一下方式获取,接下来以获取访问命中率的例子来进行分析。提供一个脚本通过日志来获取7天内的历史命中率</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">#!/bin/bash </div><div class="line">baseDir=$(cd $(dirname $0); pwd) </div><div class="line">hbaseLogDir=&quot;/opt/hbase_logs/&quot; </div><div class="line">checkServerList=$(cat /opt/hbase/conf/regionservers) </div><div class="line">checkUser=&quot;hadoop&quot; </div><div class="line">checkStr=&quot;org.apache.hadoop.hbase.io.hfile.LruBlockCache&quot; </div><div class="line">logPrefix=&quot;hbase-hadoop-regionserver-&quot; </div><div class="line">resultDir=$&#123;baseDir&#125;/result </div><div class="line">max=7 </div><div class="line">if [ ! -d $resultDir ];then </div><div class="line">    mkdir $resultDir </div><div class="line">fi </div><div class="line">for checkServer in $&#123;checkServerList&#125; </div><div class="line">    do </div><div class="line">        for ((i=0 ; i &lt; $max ; i++)) </div><div class="line">           do </div><div class="line">               logFile= </div><div class="line">                if [ $i -ne 0 ];then </div><div class="line">                    day=.$(date +%Y-%m-%d --date &quot;$i days ago&quot;) </div><div class="line">                fi </div><div class="line">                hostname=$(ssh $&#123;checkUser&#125;@$&#123;checkServer&#125; &quot;hostname&quot;) </div><div class="line">                logFile=$&#123;hbaseLogDir&#125;$&#123;logPrefix&#125;$&#123;hostname&#125;.log$&#123;day&#125; </div><div class="line">                echo $logFile </div><div class="line">                ssh $&#123;checkUser&#125;@$&#123;checkServer&#125; &quot;grep $&#123;checkStr&#125; -r $&#123;logFile&#125;&quot; &gt;&gt; $&#123;resultDir&#125;/$&#123;checkServer&#125; </div><div class="line">            done </div><div class="line">        #echo $result &gt;&gt;$&#123;resultDir&#125;/$&#123;checkServer&#125; </div><div class="line">    done</div></pre></td></tr></table></figure>
<p>通过awk呈现历史的命中率：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">2014-07-15 00:18:36,290 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: LRU Stats: total1.86 GB, free=367.23 MB, max=2.22 GB, blocks=29427, accesses=109550765, hits=84075136, hitRatio=76.74%, cachingAccesses=88885825, cachingHits=83592639, cachingHitsRatio=94.04%, evictions=1248, evicted=4889752, evictedPerRun=3918.070556640625</div></pre></td></tr></table></figure>
<p>上面展示了一个LRU日志的内容，接下来简单解析一下LRU日志的各个参数的含义：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">total 当前catch正在使用的内存大小</div><div class="line">free 系统可用的catch的大小</div><div class="line">max  总的可以用于catch的内存大小</div><div class="line">blocks   当前系统的catch中存储的block的个数</div><div class="line">accesses  catch被访问的总的次数</div><div class="line">hits    catch被访问，而且命中的次数</div><div class="line">hitRatio   当前的catch的总的命中率</div><div class="line">catchAccesses 总共请求查找块的次数</div><div class="line">catchingHits 请求查找块的命中次数</div><div class="line">catchHistsRatio 查找块的位置的命中率</div><div class="line">evictions  清除catch发成的次数</div><div class="line">evicted    被清除的block的总数</div><div class="line">evictedPerRun  平均每次清除的个数</div></pre></td></tr></table></figure>
<p>我们需要的是hitRatio的数据，这部分数据显示的是总的请求的命中率</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat $file| grep &quot;cachingHitsRatio&quot; |awk -F &quot;,&quot; &apos;&#123;print $1 $8&#125;&apos;</div></pre></td></tr></table></figure>
<p>通过这个命令，可以看到系统每隔5分钟会进行，平均的命中率是75%</p>
<p>通过perl脚本获取到单个节点的平均命中率：</p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/perl </span></div><div class="line"><span class="keyword">my</span> $file = $ARGV[<span class="number">0</span>]; </div><div class="line"><span class="keyword">my</span> $ratioBefore; </div><div class="line"><span class="keyword">my</span> $ratioLast; </div><div class="line"><span class="keyword">my</span> $count = <span class="number">0</span>; </div><div class="line"><span class="keyword">if</span> ( -e $file )&#123; </div><div class="line">    <span class="keyword">open</span>(FILE,<span class="string">"&lt;"</span>,$file)||<span class="keyword">die</span> <span class="string">"can not open $file\n"</span>; </div><div class="line">    <span class="keyword">while</span>()&#123; </div><div class="line">        <span class="keyword">if</span> ( $_ =~ <span class="regexp">m/hitRatio=(\d+.\d+)%/</span>)&#123; </div><div class="line">            $ratioBefore += $1; </div><div class="line">            $count += <span class="number">1</span>; </div><div class="line">        &#125;   </div><div class="line">    &#125;   </div><div class="line">&#125;</div><div class="line"><span class="keyword">print</span> $ratioBefore/$count;</div></pre></td></tr></table></figure>
<h1 id="提高catch的命中率"><a href="#提高catch的命中率" class="headerlink" title="提高catch的命中率"></a>提高catch的命中率</h1><p>1、在系统堆内存总量不边的情况下，修改了以下的参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hbase.regionserver.global.memstore.upperLimit  0.35</div></pre></td></tr></table></figure>
<p>在Region服务器中所有的memstore所占Java虚拟机比例的最大值  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hbase.regionserver.global.memstore.lowerLimit  0.30</div></pre></td></tr></table></figure>
<p>在Region服务器中所有的memstore所占Java虚拟机比例的最小值 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hfile.block.cache.size  0.40</div></pre></td></tr></table></figure>
<p>HFile/StoreFile缓存所占java虚拟机堆的大小的百分比 </p>
<p> 修改了这部分参数意义上是减少了写缓存的数量，提高了读缓存的数量。</p>
<p>2、使用使用LRU和BucketCatche结合的方式时候</p>
<p>3、修改表的属性，包括DATA_BLOCK_ENCODING、BLOCKSIZE对命中率有一定的影响</p>
<p>4、合理的设置rowkey，建议用md5等方式，保证分布足够均匀</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/07/28/HBase读写性能相关-基本优化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/07/28/HBase读写性能相关-基本优化/" itemprop="url">HBase读写性能相关-基本优化</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-07-27T21:52:42-08:00">
                2014-07-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="split触发时机"><a href="#split触发时机" class="headerlink" title="split触发时机"></a>split触发时机</h1><p>a、compact之后会调用CompactSplitThread.requestSplit(HRegion)</p>
<p>b、flush之前会检测区域中HStoreFile数目是否超hbase.hstore.blockingStoreFiles，如果超过且没有等待超时会调用CompactSplitThread.requestSplit(HRegion)</p>
<p>c、flush之后会调用HRegion.checkSplit()检测是否需要split，如果需要则调用  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">CompactSplitThread.requestSplit(HRegion)</div></pre></td></tr></table></figure>
<p>d、手动的split</p>
<h1 id="compact发生的时间"><a href="#compact发生的时间" class="headerlink" title="compact发生的时间"></a>compact发生的时间</h1><p>a、Memstoreflush时；</p>
<p>b、HRegionServer定期做Compaction Checker时；</p>
<p>c、HBaseAdmin客户端发起的请求；</p>
<p>d、CompactTool发起。</p>
<h1 id="影响写速度的因素"><a href="#影响写速度的因素" class="headerlink" title="影响写速度的因素"></a>影响写速度的因素</h1><p>1、rowKey是否足够合理</p>
<p>因为hbase是按rowKey连续存储的，因此如应用写入数据时rowKey是连续的，那么就会造成写的压力会集中在单台region server上，这样造成了负载不均衡。因此应用在设计rowKey时，要尽可能的保证写入是分散的，当然，这可能会对有连续读需求的场景产生一定的冲击。</p>
<p>2、写阻塞等待</p>
<p>造成这个现象的原因是各个region的memstore使用的大小加起来超过了总的阈值，于是阻塞并开始进行flush，这个过程会需要消耗掉一些时间，通常来说造成这个的原因是单台RegionServer上region数太多了，因此其实单台RegionServer上最好不要放置过多的region，一种调节方法是调大split的fileSize，这样可以适当的减少region数，但需要关注调整后读性能的变化。一般会报错：Flush thread woke up with memory above low water。或者在memstore的大小超过其阈值的时候，也会进行flush，错误日志：delaying flush up to。</p>
<p>3、compact带来的影响</p>
<p>通常是store file太多compact造成的，错误日志： has too many store files; delaying flush up to</p>
<p> 4、 split造成的</p>
<p>split会造成读写短暂的失败，如写的数据比较大的话，可能会有频繁的split出现，sptlit期间会让region下线，然后block住该region的写请求。</p>
<h1 id="随机写的测试工具YCSB"><a href="#随机写的测试工具YCSB" class="headerlink" title="随机写的测试工具YCSB"></a>随机写的测试工具YCSB</h1><p> ycsb是一个非常方便的针对分布式文件系统的测试工具,它的特点是：1 可以任意设置读写比例、线程数量，打印结果比较详细；2 它是hbase等nosql官方jira上面的测试标准，与人交流时ycsb的测试数据最能说明问题。</p>
<p>下载0.1.3版本，编译hbase的测试类，把hbase的配置文件拷贝到db/hbase/conf下，依次执行ant &amp;&amp; ant dbcompile-hbase</p>
<p>执行随机写的测试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">java -cp build/ycsb.jar:db/hbase/lib/* com.yahoo.ycsb.Client -load -db com.yahoo.ycsb.db.HBaseClient -P workloads/workloada -p columnfamily=family -p table=hello -p recordcount=100000000 -p threadcount=4  -s 2&gt;&amp;1 &gt;result</div></pre></td></tr></table></figure>
<p> 执行随机读的测试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">java -cp build/ycsb.jar:db/hbase/lib/* com.yahoo.ycsb.Client -t -db com.yahoo.ycsb.db.HBaseClient -P workloads/workloada -p columnfamily=info -p table=hee -p operationcount=100000 -s -threads 10 -target 100 &gt; transactions.dat</div></pre></td></tr></table></figure>
<p> workload[a-f]依次代表的百分比为：</p>
<p>结果分析(以插入1000000条数据的结果做分析)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">3 [OVERALL], RunTime(ms), 2459212.0  //总的运行时间</div><div class="line"></div><div class="line">4 [OVERALL], Throughput(ops/sec), 406.63432026193755  //每秒内插入的数据 </div><div class="line"></div><div class="line">5 [INSERT], Operations, 1000000 //插入的总数量</div><div class="line"></div><div class="line">6 [INSERT], AverageLatency(ms), 9.27774  //每条数据的插入时间</div><div class="line"></div><div class="line">7 [INSERT], MinLatency(ms), 0 //最短插入间隔</div><div class="line"></div><div class="line">8 [INSERT], MaxLatency(ms), 265720 //最长的插入间隔</div><div class="line"></div><div class="line">9 [INSERT], 95thPercentileLatency(ms), 0 </div><div class="line"></div><div class="line">10 [INSERT], 99thPercentileLatency(ms), 0 </div><div class="line"></div><div class="line">11 [INSERT], Return=0, 1000000 //插入1000000万条的返回值</div><div class="line"></div><div class="line">12 [INSERT], 0, 994510 //有994510条数据在0-1s内插入</div></pre></td></tr></table></figure>
<h1 id="表的信息影响随机读写性能的测试"><a href="#表的信息影响随机读写性能的测试" class="headerlink" title="表的信息影响随机读写性能的测试"></a>表的信息影响随机读写性能的测试</h1><p>1、DEFERRED_LOG_FLUSH的设置：</p>
<p>设置数据表的DEFERRED_LOG_FLUSH属性为true，服务端将采用异步写hlog的方式，客户端写响应时间会降到1ms以下。如果regionserver挂掉会丢失1s的数据。同样有数据丢失风险，这种方式会小很多很多（rs被kill -9或者物理机器直接宕机），而且写入的数据马上可以被读取到，客户端也不需要采取特别的策略。</p>
<p>2、BLOOMFILTER 的设置：</p>
<p>对于某个region的随机读，HBase会遍历读memstore及storefile（按照一定的顺序），将结果合并返回给客户端。如果你设置了bloomfilter，那么在遍历读storefile时，就可以利用bloomfilter，忽略某些storefile</p>
<p>有两种类型的bloomfilter类型：</p>
<p> a)ROW, 根据KeyValue中的row来过滤storefile </p>
<p>举例：假设有2个storefile文件sf1和sf2， </p>
<p>sf1包含kv1（r1 cf:q1 v）、kv2（r2 cf:q1 v） </p>
<p>sf2包含kv3（r3 cf:q1 v）、kv4（r4 cf:q1 v） </p>
<p>如果设置了CF属性中的bloomfilter为ROW，那么get(r1)时就会过滤sf2，get(r3)就会过滤sf1 </p>
<p>b)ROWCOL,根据KeyValue中的row+qualifier来过滤storefile </p>
<p>举例：假设有2个storefile文件sf1和sf2， </p>
<p>sf1包含kv1（r1 cf:q1 v）、kv2（r2 cf:q1 v） </p>
<p>sf2包含kv3（r1 cf:q2 v）、kv4（r2 cf:q2 v） </p>
<p>如果设置了CF属性中的bloomfilter为ROW，无论get(r1,q1)还是get(r1,q2)，都会读取sf1+sf2； 而如果设置了CF属性中的bloomfilter为ROWCOL，那么get(r1,q1)就会过滤sf2，get(r1,q2)就会过滤sf1 </p>
<p>3、DATA_BLOCK_ENCODING的设置：</p>
<p> Data Block Encoding是0.94的重要特性，可以很大程度上改善lrucache的使用率，从而提高get scan性能。从测试结果来看设置DATA_BLOCK_ENCODING =&gt; ‘DIFF’可以获得最有性价比的性能提升。可以选择的类型包括：</p>
<p>PREFIX(这是一个比较公用的压缩算法，其压缩过程也比较简单快速，通用于存在相同的Row前缀的情景);</p>
<p>DIFF(相比于PREFIX，同样是需要应用在存在相同的Row前缀的情景，但其只写一份family内容，写入timestamp的差值，比较长度是否一样等特性，使其压缩幅度会更大，当然压缩的CPU开销也会稍大);    </p>
<p>FAST_DIFF(和DIFF类似，但是对于存在相同Value内容的场景,那肯定是使用它了)</p>
<p>更加详细的内容请参考   <a href="http://blog.cloudera.com/blog/2012/06/hbase-io-hfile-input-output/" target="_blank" rel="external">http://blog.cloudera.com/blog/2012/06/hbase-io-hfile-input-output/</a></p>
<p>4、BLOCKSIZE的设置(HBase-3864)：</p>
<p>在配置中有一个hbase.mapreduce.hfileoutputformat.blocksize，这个参数和表描述中的一样，主要是用来初始化MR的。配置中的采纳数主要是从HFileOutputFormat写hfile的时候，会强制改成这个值。但是表中的配置会覆盖这个配置，所以这个配置一般没有用。</p>
<p>5、COMPRESSION的设置：</p>
<p>设置压缩格式，可以间接的提高查询效率，但是会影响写入的效率。</p>
<p>接下来测试开启该配置和不开启的区别：</p>
<p>我们用4线程，写入100w条记录做实验，查看平均的插入时间</p>
<table>
<thead>
<tr>
<th>表”描述”</th>
<th>写入时间</th>
<th>读取时间</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>默认</code></td>
<td>668214ms</td>
<td>4768452ms</td>
</tr>
<tr>
<td><code>BLOOMFILTER =&gt; &#39;ROW&#39;</code></td>
<td>683444ms</td>
<td>3175800ms</td>
</tr>
<tr>
<td><code>DATA_BLOCK_ENCODING =&gt; &#39;DIFF&#39;</code></td>
<td>633093ms</td>
<td>3452708ms</td>
</tr>
<tr>
<td><code>BLOCKSIZE =&gt; &#39;16384&#39;</code></td>
<td>670335ms</td>
<td>3472540ms</td>
</tr>
</tbody>
</table>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/07/22/HBase的日常维护/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/07/22/HBase的日常维护/" itemprop="url">HBase的日常维护</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-07-21T22:17:24-08:00">
                2014-07-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="常用维护命令"><a href="#常用维护命令" class="headerlink" title="常用维护命令"></a>常用维护命令</h1><p>1、major_compact ‘testtable’，通常生产环境会关闭自动major_compact(配置文件中hbase.hregion.majorcompaction设 为0)，选择一个晚上用户少的时间窗口手工major_compact，如果hbase更新不是太频繁，可以一个星期对所有表做一次 major_compact，这个可以在做完一次major_compact后，观看所有的storefile数量，如果storefile数量增加到 major_compact后的storefile的近二倍时，可以对所有表做一次major_compact，时间比较长，操作尽量避免高锋期。</p>
<p>2、flush ‘testtable’，将所有memstore刷新到hdfs，通常如果发现regionserver的内存使用过大，造成该机的regionserver很多线程block，可以执行一下flush操作，这个操作会造成hbase的storefile数量剧增，应尽量避免这个操 作，还有一种情况，在hbase进行迁移的时候，如果选择拷贝文件方式，可以先停写入，然后flush所有表，拷贝文件。</p>
<p>3、balance_switch true或者balance_switch flase，配置master是否执行平衡各个regionserver的region数量，当我们需要维护或者重启一个regionserver时，会 关闭balancer，这样就使得region在regionserver上的分布不均，这个时候需要手工的开启balance。</p>
<p>4、move一个region到另一个RS。</p>
<p>Move 要移动的regiong的id 目的机器，端口，startkey</p>
<h1 id="服务维护命令"><a href="#服务维护命令" class="headerlink" title="服务维护命令"></a>服务维护命令</h1><p>1、bin/graceful_stop.sh –restart –reload –debugnodename。这个操作是平滑的重启regionserver进程，对服务不会有影响，他会先将需要重启的regionserver上面的所有 region迁移到其它的服务器，然后重启，最后又会将之前的region迁移回来，但我们修改一个配置时，可以用这种方式重启每一台机子，这个命令会关 闭balancer，所以最后我们要在hbase shell里面执行一下balance_switch true，对于hbase regionserver重启，不要直接kill进程，这样会造成在zookeeper.session.timeout这个时间长的中断，也不要通过 bin/hbase-daemon.sh stop regionserver去重启，如果运气不太好，-ROOT-或者.META.表在上面的话，所有的请求会全部失败。</p>
<p>2、关闭下线一台regionserver bin/graceful_stop.sh –stop<strong>nodename</strong> <strong>。</strong>和上面一样，系统会在关闭之前迁移所有region，然后stop进程，同样最后我们要手工balance_switch true，开启master的region均衡。</p>
<p>3、检查region是否正常以及修复：bin/hbase hbck (检查) &amp;&amp; bin/hbase hbck -fix （修复）。执行修复的命令后会返回所有的region是否正常挂载，如没有正常挂载可以使用下一条命令修复，如果还是不能修复，那需要看日志为什么失败，手工处理，这个命令时间稍微有点时间长，慎用。</p>
<p>4、有时候，用户有必要绕过HBase并直接访问一个HFile，例如，检查它的健康程度，或者转存它的内容。hbase org.apache.hadoop.hbase.io.hfile.HFile -f /hbase/t1/e634b203a20933f8e13ad83ecb1511dc/f1/4f24802e1e8a47ba82c30d9de35589cc -v -m -p</p>
<p>5、检查复制的数据是否完整。hbase org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication –starttime=1265875194289 –stoptime=1265878794289 1 TestTable</p>
<p>6、更新所有meta的信息（慎用） hbase hbck -fixMeta -fixAssignments</p>
<h1 id="HBase的迁移"><a href="#HBase的迁移" class="headerlink" title="HBase的迁移"></a>HBase的迁移</h1><p>1，copytable方式</p>
<p>bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable –peer.adr=zookeeper1,zookeeper2,zookeeper3:/hbase ‘testtable’</p>
<p>目前0.92之前的版本的不支持多版本的复制，0.94已经支持多个版本的复制。当然这个操作需要添加hbase目录里的conf/mapred-site.xml，可以复制hadoop的过来。</p>
<p>2，Export/Import</p>
<p>bin/hbase org.apache.hadoop.hbase.mapreduce.Exporttesttable /user/testtable [versions] [starttime] [stoptime]</p>
<p>bin/hbase org.apache.hadoop.hbase.mapreduce.Importtesttable /user/testtable</p>
<p>跨版本的迁移，我觉得是一个不错的选择，而且copytable不支持多版本，而export支持多版本，比copytable更实用一些。</p>
<p>3，直接拷贝hdfs对应的文件</p>
<p>首先拷贝hdfs文件，如bin/hadoop distcp hdfs://srcnamenode:9000/hbase/testtable/ hdfs://distnamenode:9000/hbase/testtable/</p>
<p>然后在目的hbase上执行bin/hbase org.jruby.Main bin/add_table.rb /hbase/testtable</p>
<p>生成meta信息后，重启hbase</p>
<p>这个操作是简单的方式，操作之前可以关闭hbase的写入，执行flush所有表（上面有介绍）,再distcp拷贝，如果hadoop版本不一致，可以用hftp接口的方式，我推荐使用这种方式，成本低</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/07/16/Hadoop和HBase开启对LZO压缩的支持/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/07/16/Hadoop和HBase开启对LZO压缩的支持/" itemprop="url">Hadoop和HBase开启对LZO压缩的支持</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-07-15T21:19:12-08:00">
                2014-07-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这本来是想写一个小的系列，包括后续的压测，所以先从环境搭建开始.</p>
<p>PS:看了很多的文章后，总结出来的结论：网络上搭建方式各种坑，实践才是检验真理的唯一方式</p>
<h1 id="基础环境"><a href="#基础环境" class="headerlink" title="基础环境"></a>基础环境</h1><p>hadoop版本1.0.4,Hbase0.94.3，系统环境：Centos5X, 64位</p>
<h1 id="搭建过程"><a href="#搭建过程" class="headerlink" title="搭建过程"></a>搭建过程</h1><p> 一共需要准备4部分东西，这4个部分缺一不可。</p>
<h2 id="1、lzo的系统库文件"><a href="#1、lzo的系统库文件" class="headerlink" title="1、lzo的系统库文件"></a>1、lzo的系统库文件</h2><p>不用相信网上的方法，亲测了，和很多库一样，这个不需要编译环境和运行环境是同一套，不过最好本地和运行的基本库一样，否则会有一些系统库不是向下兼容的问题。</p>
<p>下载lzo-2.06.tar.gz</p>
<p>然后依次执行以下的命令编译系统库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">tar -zxvf lzo-2.06.tar.gz</div><div class="line"></div><div class="line">./configure --enable-shared --prefix /usr/local/lzo-2.06</div><div class="line"></div><div class="line">make</div><div class="line"></div><div class="line">make install</div></pre></td></tr></table></figure>
<p>生成的库文件如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">├── lib</div><div class="line">│   ├── liblzo2.a</div><div class="line">│   ├── liblzo2.la</div><div class="line">│   ├── liblzo2.so -&gt; liblzo2.so.2.0.0</div><div class="line">│   ├── liblzo2.so.2 -&gt; liblzo2.so.2.0.0</div><div class="line">│   └── liblzo2.so.2.0.0</div></pre></td></tr></table></figure>
<p>由于这些都是最基本的lzo压缩库，所以需要放到集群上的公共库中(我的系统是64位的，放在了/usr/lib64下面)，要求每个节点都有该库，因为所有的Map和Reduce都只依赖节点本身的库。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop131 ~]$ ls /usr/lib64/liblzo2.*</div><div class="line"></div><div class="line">/usr/lib64/liblzo2.a   /usr/lib64/liblzo2.so    /usr/lib64/liblzo2.so.2.0.0</div><div class="line"></div><div class="line">/usr/lib64/liblzo2.la  /usr/lib64/liblzo2.so.2</div></pre></td></tr></table></figure>
<h2 id="2、导入hadoop依赖lzo的jar包"><a href="#2、导入hadoop依赖lzo的jar包" class="headerlink" title="2、导入hadoop依赖lzo的jar包"></a>2、导入hadoop依赖lzo的jar包</h2><p>下载hadoop-gpl-packaging-0.6.1-1.x86_64.rpm</p>
<p>在系统上安装:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rpm -ivh hadoop-gpl-packaging-0.6.1-1.x86_64.rpm</div></pre></td></tr></table></figure>
<p>将会在本地安装以下的目录文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">├── lib</div><div class="line">│   ├── cdh4.0.1</div><div class="line">│   │   └── elephant-bird-gerritjvv</div><div class="line">│   │       └── elephant-bird-2.0.5.jar</div><div class="line">│   ├── guava-12.0.jar</div><div class="line">│   ├── hadoop-lzo-0.4.17.jar</div><div class="line">│   ├── hadoop-lzo.jar -&gt; /opt/hadoopgpl/lib/hadoop-lzo-0.4.17.jar</div><div class="line">│   ├── pig-0.10.0</div><div class="line">│   │   ├── elephant-bird-gerritjvv</div><div class="line">│   │   │   └── elephant-bird-2.0.5.jar</div><div class="line">│   │   ├── elephant-bird.jar -&gt; /opt/hadoopgpl/lib/pig-0.10.0/elephant-bird-gerritjvv/elephant-bird-2.0.5.jar</div><div class="line">│   │   ├── udfs-0.3.1.jar</div><div class="line">│   │   └── udfs.jar -&gt; /opt/hadoopgpl/lib/pig-0.10.0/udfs-0.3.1.jar</div><div class="line">│   ├── pig-0.6.0</div><div class="line">│   │   └── elephant-bird-1.0.jar</div><div class="line">│   ├── pig-0.7.0</div><div class="line">│   │   └── elephant-bird-1.0.jar</div><div class="line">│   ├── pig-0.8.0</div><div class="line">│   │   ├── elephant-bird-dvryaboy</div><div class="line">│   │   │   └── elephant-bird-2.0.jar</div><div class="line">│   │   └── elephant-bird-gerritjvv</div><div class="line">│   │       └── elephant-bird-2.0.jar</div><div class="line">│   ├── protobuf-java-2.4.1.jar</div><div class="line">│   ├── slf4j-api-1.5.8.jar</div><div class="line">│   ├── slf4j-log4j12-1.5.10.jar</div><div class="line">│   └── yamlbeans-0.9.3.jar</div><div class="line">└── native</div><div class="line">    └── Linux-amd64-64</div><div class="line">        ├── libgplcompression.a</div><div class="line">        ├── libgplcompression.la</div><div class="line">        ├── libgplcompression.so</div><div class="line">        ├── libgplcompression.so.0</div><div class="line">        ├── libgplcompression.so.0.0.0</div><div class="line">        ├── LzoCompressor.lo</div><div class="line">        ├── LzoCompressor.o</div><div class="line">        ├── LzoDecompressor.lo</div><div class="line">        └── LzoDecompressor.o</div></pre></td></tr></table></figure>
<p>现在只需要 hadoop-lzo.jar，需要将这个jar包拷贝到$HADOOP_HOME/lib和$HBAS_HOME/lib下，需要集群中所有的节点都有这个包。</p>
<h2 id="3、hadoop调用系统库的jni库文件"><a href="#3、hadoop调用系统库的jni库文件" class="headerlink" title="3、hadoop调用系统库的jni库文件"></a>3、hadoop调用系统库的jni库文件</h2><p>在上面的Linux-amd64-64目录下，存放了需要的所有的JNI库文件，需要把这些库放到          </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$HADOOP_HOME/lib/native/Linux-amd64-64</div></pre></td></tr></table></figure>
<p>拷贝其中所有的libgplcompression.* 到</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$HBASE_HOME/lib/native/Linux-amd64-64</div></pre></td></tr></table></figure>
<p>注意，一般Hbase中不存在该目录，直接创建就可以了。</p>
<h2 id="4、需要配置中，引用相关的库文件"><a href="#4、需要配置中，引用相关的库文件" class="headerlink" title="4、需要配置中，引用相关的库文件"></a>4、需要配置中，引用相关的库文件</h2><p>需要修改3个hadoop的配置：</p>
<h3 id="core-site-xml-中增加："><a href="#core-site-xml-中增加：" class="headerlink" title="core-site.xml 中增加："></a>core-site.xml 中增加：</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codec.lzo.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<h3 id="mapred-site-xml-中增加："><a href="#mapred-site-xml-中增加：" class="headerlink" title="mapred-site.xml 中增加："></a>mapred-site.xml 中增加：</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.output.compress<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.child.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>-Djava.library.path=$&#123;HADOOP_HOME&#125;/lib/native/Linux-amd64-64<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.map.output.compression.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<h3 id="hadoop-env-sh-中增加："><a href="#hadoop-env-sh-中增加：" class="headerlink" title="hadoop-env.sh 中增加："></a>hadoop-env.sh 中增加：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/lib:$HBASE_HOME/lib:</div><div class="line"></div><div class="line">export JAVA_LIBRARY_PATH=$JAVA_LIBRARY_PATH:$HADOOP_HOME/lib/native/Linux-amd64-64:$HADOOP_HOME/lib/native:$HADOOP_HOME/lib</div></pre></td></tr></table></figure>
<h1 id="结果测试"><a href="#结果测试" class="headerlink" title="结果测试"></a>结果测试</h1><p>完成上述的4步配置后，需要重启集群。使用下面的方式对结果进行测试</p>
<h2 id="hadoop测试使用lzo的压缩："><a href="#hadoop测试使用lzo的压缩：" class="headerlink" title="hadoop测试使用lzo的压缩："></a>hadoop测试使用lzo的压缩：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop jar $&#123;HADOOP_HOME&#125;/contrib/streaming/hadoop-streaming-1.0.4.jar -input /in/part-00009 -output /testalzo -mapper cat -reducer cat -jobconf mapred.output.compress=true -jobconf mapred.output.compression.codec=org.apache.hadoop.io.compress.LzoCodec</div></pre></td></tr></table></figure>
<p>测试成功的话会生成/in/part-00009的压缩文件/testalzo/part-00000.lzo_deflate</p>
<h2 id="hbase测试使用lzo压缩："><a href="#hbase测试使用lzo压缩：" class="headerlink" title="hbase测试使用lzo压缩："></a>hbase测试使用lzo压缩：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hbase org.apache.hadoop.hbase.util.CompressionTest hdfs://hadoop131/terasort/output/part-00005 lzo</div></pre></td></tr></table></figure>
<p>测试成功的话会输出SUCCESS   </p>
<p>修改表压缩方式为lzo，接下来以usertable为例子进行</p>
<p>1、describe ‘usertable’ 查看原来的压缩方式，默认情况下的压缩方式 COMPRESSION =&gt; ‘NONE’</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">hbase(main):003:0&gt; describe &apos;usertable&apos; </div><div class="line">DESCRIPTION                                                       ENABLED                           </div><div class="line">&#123;NAME =&gt; &apos;usertable&apos;, FAMILIES =&gt; [&#123;NAME =&gt; &apos;cf&apos;, DATA_BLOCK_ENC true                              </div><div class="line">ODING =&gt; &apos;NONE&apos;, BLOOMFILTER =&gt; &apos;NONE&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;                                   </div><div class="line">, VERSIONS =&gt; &apos;3&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, T                                   </div><div class="line">TL =&gt; &apos;2147483647&apos;, KEEP_DELETED_CELLS =&gt; &apos;false&apos;, BLOCKSIZE =&gt;                                    </div><div class="line">&apos;65536&apos;, IN_MEMORY =&gt; &apos;false&apos;, ENCODE_ON_DISK =&gt; &apos;true&apos;, BLOCKCA                                   </div><div class="line">CHE =&gt; &apos;true&apos;&#125;]&#125;  </div><div class="line">&gt;disable &apos;usertable&apos;</div><div class="line">&gt;alter &apos;usertable&apos;,&#123;NAME=&gt;&apos;info&apos;,COMPRESSION=&gt;&apos;LZO&apos;&#125;</div><div class="line">&gt;enable &apos;TestTable&apos;</div><div class="line">&gt;compact &apos;TestTable&apos;</div></pre></td></tr></table></figure>
<p>测试结果：</p>
<p>先用自带的工具生成1G的测试数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hbase org.apache.hadoop.hbase.PerformanceEvaluation  sequentialWrite 1</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">1059592589  hdfs://hadoop131/hbase/TestTable</div><div class="line">409232235   hdfs://hadoop131/hbase/TestTable</div></pre></td></tr></table></figure>
<p>原来1G的数据被压缩到了400M，这个压缩效果和数据有关系，但是从网上的经验看，至少20%的压缩比还是能达到的。</p>
<h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><p>2014-10-24日增加:</p>
<p>升级为hadoop2.4.1版本，为新版本增加LZO的压缩：</p>
<p>1、编译本地的LZO库同上</p>
<p>2、编译Hadoop-LZO依赖：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">git init </div><div class="line"></div><div class="line">git clone https://github.com/twitter/hadoop-lzo</div><div class="line"></div><div class="line">export C_INCLUDE_PATH=/usr/local/lzo/include</div><div class="line"></div><div class="line">export LIBRARY_PATH=/usr/local/lzo/lib</div><div class="line"></div><div class="line">mvn clean package -Dmaven.test.skip=true</div></pre></td></tr></table></figure>
<p>3、拷贝相应的库和包到集群上：</p>
<p>需要拷贝的包括(如果需要HBase支持ZLO，需要把所有的包拷贝到hbase下的lib下，没有的话创建</p>
<p>所有的lzo依赖库，把所有/usr/local/lzo/lib下的库文件拷贝到集群上，且通过ldd的工具设置为系统库。</p>
<p>拷贝maven编译完成后的target/hadoop-lzo-0.4.20-SNAPSHOT.jar拷贝到hadoop的share/hadoop/common下</p>
<p>拷贝target/native/Linux-amd64-64/lib下的所有库文件到hadoop的lib/native/下</p>
<p>4、配置集群文件同上<br>5、重启集群<br>6、测试压缩是否可以用<br>在本地安装lzop的工具，然后随便找一个文件，执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">lzop -U -9 testFile</div></pre></td></tr></table></figure></p>
<p> 然后上传到HDFS上，如果直接用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop fs -cat</div></pre></td></tr></table></figure>
<p> 可以看到文件是乱码，但是用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hdfs dfs -text</div></pre></td></tr></table></figure>
<p> 看到的是非乱码就证明lzo正确安装了。</p>
<h1 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h1><p><a href="http://www.quora.com/How-do-I-install-LZO-compression-with-Hbase-and-Hadoop" target="_blank" rel="external">http://www.quora.com/How-do-I-install-LZO-compression-with-Hbase-and-Hadoop</a><br><a href="http://shitouer.cn/2013/01/hadoop-hbase-snappy-setup-final-tutorial/" target="_blank" rel="external">http://shitouer.cn/2013/01/hadoop-hbase-snappy-setup-final-tutorial/</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/06/20/java多线程-Executor/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/06/20/java多线程-Executor/" itemprop="url">java多线程(Executor)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-06-19T19:42:58-08:00">
                2014-06-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Executor框架是指java 5中引入的一系列并发库中与executor相关的一些功能类，其中包括线程池，Executor，Executors，ExecutorService，CompletionService，Future，Callable等。</p>
<p>下面这个例子是为了证明一个问题：当一个ExecutorService在执行了shutdown后，并不会影响当前线程的执行，当前的线程会继续执行，只不过该对象不能再增加新的线程。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.sql.DatabaseMetaData;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.Date;</div><div class="line"><span class="keyword">import</span> java.util.concurrent.Callable;</div><div class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</div><div class="line"><span class="keyword">import</span> java.util.concurrent.ExecutorService;</div><div class="line"><span class="keyword">import</span> java.util.concurrent.Executors;</div><div class="line"><span class="keyword">import</span> java.util.concurrent.Future;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCallable</span> <span class="keyword">implements</span> <span class="title">Callable</span> </span>&#123;    </div><div class="line">    <span class="keyword">private</span> <span class="keyword">int</span> myNum = <span class="number">0</span>;</div><div class="line">    </div><div class="line">    MyCallable(<span class="keyword">int</span> threadNum)&#123;</div><div class="line">        <span class="keyword">this</span>.myNum = threadNum;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">call</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">        System.out.println(<span class="string">"&gt;&gt;&gt;"</span>+myNum+<span class="string">" thread start"</span>);</div><div class="line">        Date dateTmp1 = <span class="keyword">new</span> Date();</div><div class="line">        Thread.sleep(<span class="number">1000</span>);</div><div class="line">        Date dataTmp2 = <span class="keyword">new</span> Date();</div><div class="line">        <span class="keyword">long</span> time = dataTmp2.getTime()-dateTmp1.getTime();</div><div class="line">        System.out.println(<span class="string">"&gt;&gt;&gt;"</span>+myNum+<span class="string">" thread start, it has run time for "</span>+time);</div><div class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="meta">@SuppressWarnings</span>(<span class="string">"unchecked"</span>)</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThreadTest</span> </span>&#123;</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String []args)</span> <span class="keyword">throws</span> ExecutionException,InterruptedException</span>&#123;</div><div class="line">        System.out.println(<span class="string">"----Thread test stat-----"</span>);</div><div class="line">        Date dateNow = <span class="keyword">new</span> Date();</div><div class="line">        <span class="keyword">int</span> taskSize = <span class="number">5</span>;</div><div class="line">        </div><div class="line">       ExecutorService threadPool = Executors.newFixedThreadPool(taskSize);</div><div class="line">        List list = <span class="keyword">new</span> ArrayList();</div><div class="line">        <span class="keyword">for</span> ( <span class="keyword">int</span> i=<span class="number">0</span>;i</div><div class="line">            Callable callable = <span class="keyword">new</span> MyCallable(i);</div><div class="line">            Future future = threadPool.submit(callable);</div><div class="line">            list.add(future);</div><div class="line">        &#125;</div><div class="line">        threadPool.shutdown();</div><div class="line">        </div><div class="line">        <span class="keyword">for</span> (Future future : list)&#123;</div><div class="line">            <span class="keyword">if</span> (future.get() != <span class="keyword">null</span>)&#123;</div><div class="line">                System.out.println(<span class="string">"&gt;&gt;&gt;"</span> + future.get().toString());</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        </div><div class="line">        Date dataEnd = <span class="keyword">new</span> Date();</div><div class="line">        <span class="keyword">long</span> timeEnd = dateNow.getTime()-dataEnd.getTime();</div><div class="line">        System.out.println(<span class="string">"----Thread test "</span>+timeEnd+<span class="string">" end-----"</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在上面的例子中，用的是<strong>newFixedThreadPool</strong>来创建指定数量的线程。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutorService <span class="title">newFixedThreadPool</span><span class="params">(<span class="keyword">int</span> nThreads)</span></span></div></pre></td></tr></table></figure>
<p>创建固定数目线程的线程池。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutorService <span class="title">newCachedThreadPool</span><span class="params">()</span></span></div></pre></td></tr></table></figure>
<p>一个可缓存的线程池，调用<code>execute</code> 将重用以前构造的线程（如果线程可用）。如果现有线程没有可用的，则创建一个新线程并添加到池中。终止并从缓存中移除那些已有 60 秒钟未被使用的线程。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutorService <span class="title">newSingleThreadExecutor</span><span class="params">()</span></span></div></pre></td></tr></table></figure>
<p>一个单线程化的Executor。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ScheduledExecutorService <span class="title">newScheduledThreadPool</span><span class="params">(<span class="keyword">int</span> corePoolSize)</span></span></div></pre></td></tr></table></figure>
<p>一个支持定时及周期性的任务执行的线程池，多数情况下可用来替代Timer类。<br>上面的几个方法中，应该优先使用<strong>newCachedThreadPool</strong>，因为它创建与需求数量一致的线程数，只有当这个方法出问题的时候再考虑用<strong>newFixedThreadPool</strong>newSingleThreadExecutor对于希望在程序中连续运行的任何事物都是有用的。<br>在上面的例子中，使用的是callable，也可以使用runnable，这两个的区别就是如果想让线程执行有返回值，应该用callable。调用callable不是使用run，而是call，而且在executorserver中添加的时候，不是使用execute，而是使用submit，这个调用会返回一个futurer的对象，然后该对象可以调用get来获取结果<br>线程的yield方法是建议让具有相同优先级的其它线程可以运行了。</p>
<p>ExecutorService扩展了Executor并添加了一些生命周期管理的方法。一个Executor的生命周期有三种状态，<strong>运行</strong>，<strong>关闭</strong>，<strong>终止</strong>。Executor创建时处于运行状态。当调用ExecutorService.shutdown()后，处于关闭状态，isShutdown()方法返 回true。这时，不应该再想Executor中添加任务，所有已添加的任务执行完毕后，Executor处于终止状态，isTerminated()返 回true。</p>
<p>如果Executor处于关闭状态，往Executor提交任务会抛出unchecked exception RejectedExecutionException<br>线程的后台执行: 设置thread的对象setDaemon为true就可以设置为后台进程<br>使用join方法可以有一个特效，使得某个线程A在另一个线程B上调用A.join，则当前的B将会挂起，直到A执行完成后，B才会继续执行，也可以加一个超时时间，当然也可以中断挂起，用B的interrupt的方法.<br>join的例子：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.concurrent.Callable;</div><div class="line"><span class="keyword">import</span> java.util.concurrent.ExecutorService;</div><div class="line"><span class="keyword">import</span> java.util.concurrent.Executors;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sleeper</span> <span class="keyword">extends</span> <span class="title">Thread</span></span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> <span class="keyword">int</span> duriation;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Sleeper</span><span class="params">(String name, <span class="keyword">int</span> sleepTime)</span></span>&#123;</div><div class="line"></div><div class="line">        <span class="keyword">super</span>(name);</div><div class="line"></div><div class="line">        duriation = sleepTime;</div><div class="line"></div><div class="line">        start();</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</div><div class="line"></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line"></div><div class="line">            sleep(duriation);</div><div class="line"></div><div class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line"></div><div class="line">            <span class="comment">// <span class="doctag">TODO:</span> handle exception</span></div><div class="line"></div><div class="line">            System.out.println(getName()+<span class="string">"  was interrupted. "</span>+<span class="string">" is interrupete "</span>+isInterrupted());</div><div class="line"></div><div class="line">            <span class="keyword">return</span>;</div><div class="line"></div><div class="line">        &#125;</div><div class="line"></div><div class="line">        System.out.println(getName()+<span class="string">" has awakened"</span>);</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Joiner</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> Sleeper sleeper;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Joiner</span><span class="params">(String name, Sleeper sleeper)</span></span>&#123;</div><div class="line"></div><div class="line">        <span class="keyword">super</span>(name);</div><div class="line"></div><div class="line">        <span class="keyword">this</span>.sleeper = sleeper;</div><div class="line"></div><div class="line">        start();</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</div><div class="line"></div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line"></div><div class="line">            sleeper.join();</div><div class="line"></div><div class="line">            <span class="comment">//            sleep(1);</span></div><div class="line"></div><div class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line"></div><div class="line">            <span class="comment">// <span class="doctag">TODO:</span> handle exception</span></div><div class="line"></div><div class="line">            e.printStackTrace();</div><div class="line"></div><div class="line">        &#125;</div><div class="line"></div><div class="line">        System.out.println(getName() + <span class="string">" join completed"</span>);</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">threadTest</span></span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line"></div><div class="line">        <span class="comment">// TODO Auto-generated method stub</span></div><div class="line"></div><div class="line">        Sleeper sleeper = <span class="keyword">new</span> Sleeper(<span class="string">"a"</span>, <span class="number">15000</span>);</div><div class="line"></div><div class="line">        <span class="comment">//        Sleeper another = new Sleeper("b", 15000);</span></div><div class="line"></div><div class="line">        Joiner doc = <span class="keyword">new</span> Joiner(<span class="string">"doc"</span>, sleeper);</div><div class="line"></div><div class="line">        <span class="comment">//        sleeper.interrupt();</span></div><div class="line"></div><div class="line">        <span class="comment">//        another.interrupt();</span></div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>对于Executor管理的线程，如果发生了异常，需要捕获，尝试通过try catch的方法是无法正确捕获的，这时候可以通过Thread.UncaugthExceptionHandler.uncaughtException()来捕获临近死亡时候的状态。<br>为什么使用executor<br>构造器中启动线程是很危险的，因为另一个任务可能会在构造器结束之前开始执行，这意味着该任务能够访问处于不稳定状态的对象。<br>可以替代线程组更好的管理线程</p>
<p>参考博客：</p>
<p><a href="http://www.iteye.com/topic/366591" target="_blank" rel="external">http://www.iteye.com/topic/366591</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/06/18/Map到Reduce的主流程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/06/18/Map到Reduce的主流程/" itemprop="url">Map到Reduce的主流程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-06-17T15:09:40-08:00">
                2014-06-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一、Map"><a href="#一、Map" class="headerlink" title="﻿﻿一、Map"></a>﻿﻿一、Map</h1><p>1、spilit 和map是多对一或者一对一，split主要就是通过inutFormat程序对数据切片后的分块。<br>2、spill  这个过程就是在map过程中，把map的循环队列中的数据从内存写道硬盘的过程，写的时候会排序<br>3、merge Map端的Merge主要是，在完成了所有的map计算后，把所有的spill的块和当前内存中的块合并，然后作为reduce的输入，注意，如果设置了combined的函数，在这个阶段也会进行combine<br>4、combine  也叫pre-reduce，就是把所有的spill和内存中的map进行排序。</p>
<h1 id="二、Map结果怎么传送给Reduce"><a href="#二、Map结果怎么传送给Reduce" class="headerlink" title="二、Map结果怎么传送给Reduce"></a>二、Map结果怎么传送给Reduce</h1><p>1、shuffle整个从map端传送数据到reduce端的过程<br>2、partitioner 通过map输出的key/value个数和reduce的数量来决定哪个数据由哪个reduce来处理</p>
<h1 id="三、reduce"><a href="#三、reduce" class="headerlink" title="三、reduce"></a>三、reduce</h1><p>1、mergeReduce阶段的Merge就是通过shuffle从不同的map端下载的数据进行合并，注意，这部分数据不会直接写入磁盘，而是先在内存中，到一定数量再写入。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/06/14/通过MR读取Kafka的数据/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/06/14/通过MR读取Kafka的数据/" itemprop="url">通过MR读取Kafka的数据</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-06-13T15:13:27-08:00">
                2014-06-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="为什么要用MR读取Kafka"><a href="#为什么要用MR读取Kafka" class="headerlink" title="为什么要用MR读取Kafka"></a>为什么要用MR读取Kafka</h1><p>通过Mapreduce读取Kafka，有几个原因：</p>
<p>1、防止数据量大的读取，导致单台机器承载能力不够</p>
<p>2、利用MR的优势，更方便的处理数据</p>
<p>3、可以根据数据的类型，把数据直接存储到HDFS的不同路径上</p>
<h1 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h1><p>1、数据来源是kafka的consumer，所以不能用类似于TextInputFormat等自带的传统的数据切分的方法。<br>2、数据的Map过程需要对数据进行处理，为了减少对数据处理的次数，在Map的时候就需要把后面拼接路径的有效信息提取到<br>3、不需要有Reduce，只需要有直接的OutputStream就可以直接把不同路径的数据分发到不同的HDFS上<br>4、需要基本的ZK管理类，来直接读写偏移的信息</p>
<h1 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h1><h2 id="提交任务主类"><a href="#提交任务主类" class="headerlink" title="提交任务主类"></a>提交任务主类</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div></pre></td><td class="code"><pre><div class="line">writeNewOffsetFromConfig(conf,TOPIC,GROUP_ID,OFFSETSTART,<span class="keyword">true</span>);</div><div class="line">Job job = <span class="keyword">new</span> Job(conf, <span class="string">"Kafka.Consumer"</span>);</div><div class="line">job.setJarByClass(getClass());</div><div class="line">job.setMapperClass(KafkaMapper.class);</div><div class="line"></div><div class="line"><span class="comment">// input</span></div><div class="line">job.setInputFormatClass(KafkaInputFormat.class);</div><div class="line"></div><div class="line"><span class="comment">// output</span></div><div class="line">job.setOutputKeyClass(Text.class);</div><div class="line">job.setOutputValueClass(Text.class);</div><div class="line">job.setOutputFormatClass(KafkaOutputFormat.class);</div><div class="line">job.setNumReduceTasks(<span class="number">0</span>);</div><div class="line"></div><div class="line">KafkaOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(HDFS_PATH + <span class="string">"/"</span> + TOPIC));</div><div class="line"><span class="comment">//上面这部分的代码，展示了整个的流程：writeNewOffsetFromConfig是实现了一个从配置读取偏移，不使用上次默认偏移的方法。</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">writeNewOffsetFromConfig</span><span class="params">(Configuration conf,String topic,String groupId,String offsetStr,<span class="keyword">boolean</span> needBackUp)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">    ZkUtils zk = <span class="keyword">new</span> ZkUtils(conf);</div><div class="line">    String errorStr = <span class="string">""</span>;</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (needBackUp)&#123;</div><div class="line">        backupLastOffset(zk, conf, topic, groupId); <span class="comment">//这个是备份之前的配置，主要通过topic来读取相应的brocker、partiotion、lastOffset的信息，然后存储到ZK。</span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">        <span class="keyword">if</span> (offsetStr == <span class="keyword">null</span> || offsetStr == <span class="string">""</span>)&#123; <span class="comment">//如果配置中未指定默认的起始地址，则接着上次的偏移开始读</span></div><div class="line">            <span class="keyword">return</span>;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        String [] splitStrs = offsetStr.split(<span class="string">","</span>);</div><div class="line">        logger.info(<span class="string">"Get Start message:"</span> + offsetStr);</div><div class="line">        <span class="keyword">if</span> (splitStrs.length &lt; <span class="number">1</span>)&#123;</div><div class="line">            logger.info(<span class="string">"INFO, when start message get splitStr: "</span>+offsetStr+<span class="string">" length :"</span> + splitStrs.length);</div><div class="line">            <span class="keyword">return</span>;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">for</span> ( String splitStr : splitStrs)&#123;</div><div class="line">            String [] offsetDetailInfos = splitStr.split(<span class="string">":"</span>);</div><div class="line">            <span class="keyword">if</span> (ffsetDetailInfos.length != <span class="number">2</span>)&#123;</div><div class="line">                errorStr = <span class="string">"Error: when change offset from config, get error system config (OFFSETSTART):"</span> + offsetStr;</div><div class="line">                logger.error(errorStr);</div><div class="line">            &#125;  <span class="keyword">else</span> &#123;</div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                    String partition = offsetDetailInfos[<span class="number">0</span>];</div><div class="line">                    <span class="keyword">long</span> offset = Long.valueOf(offsetDetailInfos[<span class="number">1</span>]);</div><div class="line">                    logger.info(<span class="string">"Info: update group:"</span> + groupId + <span class="string">" topic:"</span></div><div class="line">                        + topic + <span class="string">" parti:"</span> + partition + <span class="string">" offset:"</span>+ offset);</div><div class="line">                    zk.setLastCommit(groupId, topic, partition, offset,<span class="keyword">false</span>); <span class="comment">//把当前的偏移设置为从配置中读取到的默认偏移</span></div><div class="line">                &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">                    errorStr = <span class="string">"When set offset which is read from config OFFSETSTART:"</span> + offsetStr + <span class="string">". Get Error:"</span> + e.toString();</div><div class="line">                    logger.error(errorStr);</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125; <span class="keyword">catch</span> (Exception e)&#123;</div><div class="line">        String lastOffset = zk.getLastOffset(groupId, topic);</div><div class="line">        <span class="keyword">if</span> (lastOffset != <span class="keyword">null</span>)&#123;</div><div class="line">            writeNewOffsetFromConfig(conf, topic, groupId, lastOffset,<span class="keyword">false</span>);</div><div class="line">        &#125;</div><div class="line">        errorStr = <span class="string">"Error: when change offset from config, get exception:"</span> + e.toString();</div><div class="line">        logger.error(errorStr);</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">        zk.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>其它的MR的方法，接下来会依次介绍，可以看到，这里主要实现了InputFormat、 Mapper、OutputFormat的方法来实现从Kafka消费数据，然后根据数据的类型写到不同的HDFS上。</p>
<h2 id="关于InputSplit"><a href="#关于InputSplit" class="headerlink" title="关于InputSplit"></a>关于InputSplit</h2><p>﻿﻿在 MapReduce过程中，Map之前要进行的是Inputsplit，通过数据的切分，然后才能把数据分发到不同的节点上进行计算。</p>
<p>InputSplit整个过程包含了个重要的方法getSplits和getRecorder方法。</p>
<p>getSplits主要返回了一个InputSplit的对象，这个对象在逻辑上对整个数据进行了切片(注意，没有进行实际的物理切分)</p>
<p>切分的过程中，会涉及到分片大小的问题：</p>
<p>在默认的切分中，会通过下面的方式计算分片的大小：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">goalSize = totalSize / mapred.map.tasks <span class="comment">// mapred.map.tasks默认的配置为2</span></div><div class="line">minSize = max &#123;mapred.min.split.size, minSplitSize&#125;  <span class="comment">//mapred.min.split.size的默认配置为1</span></div><div class="line">splitSize = max (minSize, min(goalSize, dfs.block.size))  <span class="comment">//dfs.block.size默认为64M</span></div></pre></td></tr></table></figure>
<p>通过最后一个计算公式，可以看到，一般正常配置的情况下分片最大不会超过dfs.block.size(minSize一般小于分块的大小)</p>
<p>接下来分析我使用的分片方式</p>
<h3 id="1、定义kafka-consumer的数据进行了分片结构："><a href="#1、定义kafka-consumer的数据进行了分片结构：" class="headerlink" title="1、定义kafka consumer的数据进行了分片结构："></a>1、定义kafka consumer的数据进行了分片结构：</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> String brokerId;  <span class="comment">//数据所在的broker的Id</span></div><div class="line"><span class="keyword">private</span> String broker;  <span class="comment">// 数据所在的broker</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">int</span> partition; <span class="comment">//具体的每个brocker的某个partition</span></div><div class="line"><span class="keyword">private</span> String topic; <span class="comment">//读取的topic</span></div><div class="line"><span class="keyword">private</span> <span class="keyword">long</span> lastCommit; <span class="comment">//记录上一次的偏移量</span></div></pre></td></tr></table></figure>
<h3 id="2、构建逻辑分片"><a href="#2、构建逻辑分片" class="headerlink" title="2、构建逻辑分片"></a>2、构建逻辑分片</h3><p>​     要想使用上面的数据结构来构建splits，需要从Kafka的consumer组中，查找指定的topic的各个数据。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">List splits = <span class="keyword">new</span> ArrayList();</div><div class="line">List partitions = zk.getPartitions(topic);  <span class="comment">//获取指定的topic的分区情况</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> (String partition : partitions) &#123;</div><div class="line">    String[] sp = partition.split(<span class="string">"-"</span>); <span class="comment">//在消费队列中，存储的格式为brocker-partition的，通过切分获取相应的brocker和partition</span></div><div class="line">    <span class="keyword">long</span> last = zk.getLastCommit(group, topic, partition); <span class="comment">//获取对应的brocker-partition的最后便宜量</span></div><div class="line">    InputSplit split = <span class="keyword">new</span> KafkaSplit(sp[<span class="number">0</span>], zk.getBroker(sp[<span class="number">0</span>]),topic, Integer.valueOf(sp[<span class="number">1</span>]), last); <span class="comment">//构建split分片对象</span></div><div class="line">    splits.add(split); <span class="comment">//在总的分片列表中加入刚创建的分片对象</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>最后返回splits，这样就获取到了所有的逻辑分片,这样的分片，可以保证，每个Map读取对应的brocker的数据(因为Map的数量是由split的个数决定的)。</p>
<h3 id="3、完成Reader"><a href="#3、完成Reader" class="headerlink" title="3、完成Reader"></a>3、完成Reader</h3><p>﻿﻿﻿﻿完成数据的逻辑分片后，需要对具体的某个分片，获取相应的数据，接下来就涉及到了InputSplit的另一个重要的方法。</p>
<p>RecordReader方法的使命是通过逻辑分片，读取相应的数据，然后传递给Map。</p>
<p>为什么用了使命呢，因为这部分是比较棘手的一部分。</p>
<p>这部分是返回一个RecordReader的对象，需要重写以下几个方法：</p>
<h4 id="1、initialize"><a href="#1、initialize" class="headerlink" title="1、initialize"></a>1、initialize</h4><p>初始化RecordReader对象，这个过程中，我会新开一个线程去从Kafka读取数据：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">kafka.api.FetchRequest request = <span class="keyword">new</span> FetchRequest(topic, partition, offset, fetchSize);</div><div class="line">messages = consumer.fetch(request);这里需要判断一下返回结构是否正确</div><div class="line"><span class="keyword">int</span> code = messages.getErrorCode();</div><div class="line"><span class="keyword">if</span> (code == <span class="number">0</span>) &#123;    </div><div class="line">  hasData = <span class="keyword">true</span>;  <span class="comment">//设置是否还有数据    </span></div><div class="line">  offset += messages.validBytes(); <span class="comment">// 把offset设置为当前的offset加上读出的数据的长度，为下次读取的偏移</span></div><div class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (hasData &amp;&amp; code == ErrorMapping.OffsetOutOfRangeCode()) &#123;    </div><div class="line">  <span class="comment">//表示已经没有数据了        </span></div><div class="line">  stop = <span class="keyword">true</span>; <span class="comment">//停止读取数据</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>但是这里访问kafka的时候，可能会有并发的问题，所以我加入了ArrayBlockingQueue队列来存储所有的message。在初始化阶段，还需要把偏移的start和end获取到：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">startOffset = SimpleConsumer kafka.consumer.KafkaContext.consumer.getOffsetsBefore(topic, partition, -<span class="number">2L</span>, <span class="number">1</span>)[<span class="number">0</span>];</div><div class="line">lastOffset = SimpleConsumer kafka.consumer.KafkaContext.consumer.getOffsetsBefore(topic, partition, -<span class="number">1L</span>, <span class="number">1</span>)[<span class="number">0</span>];</div></pre></td></tr></table></figure>
<h4 id="2、nextKeyValue"><a href="#2、nextKeyValue" class="headerlink" title="2、nextKeyValue"></a>2、nextKeyValue</h4><p>这个方法是最为重要的一个方法，主要是把获取到的逻辑分片，返回一个key value的数据对，传递给Map。</p>
<p>作为最重要的处理方法，在我们这里其实很简单，就是读取当前的message，把offset作为key，获取到的message作为value返回。</p>
<p>通过 Iterator iterator.hasNext判断是否还有新的message，如果有的话，调用Iterator iterator.message获取到当前的消息，然后设置key和value返回。</p>
<h4 id="3、getProgress"><a href="#3、getProgress" class="headerlink" title="3、getProgress"></a>3、getProgress</h4><p>这个函数主要是通过初始化的时候获取到的起始偏移来返回当前的进度，</p>
<h4 id="4、close"><a href="#4、close" class="headerlink" title="4、close"></a>4、close</h4><p>在关闭当前的读取之后，需要把读取完之后的offset写回，</p>
<p>和split的时候获取的方法想对应，这里调用的是：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">zk.setLastCommit(group, ksplit.getTopic(), partition, pos, <span class="keyword">true</span>);</div></pre></td></tr></table></figure>
<p>写回的偏移保证下次的split的时候能从新的偏移开始读数据。</p>
<p>通过createRecordReader(旧版的API里面为RecorderReader)返回上面的一个recorder对象，就可以提供在Map的时候处理的KV。</p>
<h2 id="实现Map类"><a href="#实现Map类" class="headerlink" title="实现Map类"></a>实现Map类</h2><p>﻿其实Map类很简单，</p>
<p>分到的key为上一次的offset</p>
<p>分到的 value 为上一次读取到的message</p>
<p>为什么是上一次呢，因为在上次读取完之后，已经修改了offset，对于每个map，下一次要处理的数据，不再是同一个。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"> Text out = <span class="keyword">new</span> Text();</div><div class="line">out.set(value.getBytes(), <span class="number">0</span>, value.getLength());</div></pre></td></tr></table></figure>
<p>上面的两次可以获取到具体的message，然后可以进行处理，这里的处理不再细说。</p>
<h2 id="实现output"><a href="#实现output" class="headerlink" title="实现output"></a>实现output</h2><p>﻿﻿首先我们的OutPut类继承了FileOutputFormat方法，这样就可以实现不同的数据存储到不同路径的需求。    </p>
<p>然后需要重写方法getRecordWriter，这个方法需要返回一个RecordWriter的数据，这个数据中指定了不同的数据存储到不同的节点上。</p>
<p>这里需要补充一个问题，因为我们的HDFS上需要存储数据的基本路径可能是固定的，比如/user/XXX的数据存放在相应的目录下。我们可以在主类中执行最基本的路径：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">OutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(HDFS_PATH + <span class="string">"/"</span> + TOPIC));</div></pre></td></tr></table></figure>
<p>然后可以通过一下方法在output的时候获取到相应的路径：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Path outputPath = <span class="keyword">super</span>.getOutputPath(conf);</div></pre></td></tr></table></figure>
<p>基本路径取定后，我们需要实现一个RecordWriter类，这个类中可以定义构造函数，指定要写入的job名称和基础路径。<br>而RecordWriter需要重写以下的方法：</p>
<p>在write中是将指定的数据写入到指定的HDFS上，基本路径在之前已经获取到，然后需要指定写入的文件名，我们可以简单的使用key.toString()作为文件名称。</p>
<p>然后通过Path file = new Path(workPath,key.toString() );拼接出来新的路径。</p>
<p>接下来创建一个HDFS的输出流：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">FSDataOutputStream fileOut = file.getFileSystem(conf).create(file, <span class="keyword">true</span>);</div></pre></td></tr></table></figure>
<p>调用out.write方法，把刚才获取到的value值写到流中。</p>
<p> 这里的write实现了数据的写入。</p>
<p>在close的时候需要做的是，把所有打开的RecordWriter都关闭。</p>
<p>我在实现的时候使用了private HashMap&gt; recordWriters 类存储所有的对象，然后轮寻所有hashmap,</p>
<p>依次调用close方法关闭相应的输出流，最后应该清空hashMap,这样可以最快的释放内存。</p>
<h2 id="ZK偏移量的管理"><a href="#ZK偏移量的管理" class="headerlink" title="ZK偏移量的管理"></a>ZK偏移量的管理</h2><p>﻿zk的使用，可以使用ZkClient或者使用org.apache.zookeeper.ZooKeeper，实现，本文使用的是前者。</p>
<p>zk的创建：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ZkClient client = <span class="keyword">new</span> ZkClient(zk, stimeout, ctimeout, <span class="keyword">new</span> StringSerializer() );</div></pre></td></tr></table></figure>
<p>zk获取目录的列表：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">client.getChildren(path);</div></pre></td></tr></table></figure>
<p>zk获取节点的值：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">client.readData(znode, <span class="keyword">true</span>);</div></pre></td></tr></table></figure>
<p>zk删除节点：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">client.delete(path);</div></pre></td></tr></table></figure>
<p>zk创建路径：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">client.createPersistent(path, <span class="keyword">true</span>);</div></pre></td></tr></table></figure>
<p>zk写值</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">client.writeData(path, value);</div></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">aiping.lap</p>
              <p class="site-description motion-element" itemprop="description">aiping.liang s home</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">22</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">aiping.lap</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  








  





  

  

  

  

  

  

</body>
</html>
