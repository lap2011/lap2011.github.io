<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="aiping.liang s home">
<meta property="og:type" content="website">
<meta property="og:title" content="Aiping.LAP">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Aiping.LAP">
<meta property="og:description" content="aiping.liang s home">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Aiping.LAP">
<meta name="twitter:description" content="aiping.liang s home">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/2/"/>





  <title>Aiping.LAP</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Aiping.LAP</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">认真工作，快乐生活</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/09/04/HBase的一些元数据操作/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/09/04/HBase的一些元数据操作/" itemprop="url">HBase的一些元数据操作</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-09-03T20:26:52-08:00">
                2014-09-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="NameSpace"><a href="#NameSpace" class="headerlink" title="NameSpace"></a>NameSpace</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>NameSpace 命名空间在很多地方都会用，主要其实是在指定一个规则，或者一个环境变量。在Hbase0.96后，增加了命名空间的概念。这个NameSpace是存储在ZK上的，类似与RDBMS里面的database的概念，主要似乎用来做权限管理和安全控制的。 </p>
<h2 id="收益"><a href="#收益" class="headerlink" title="收益"></a>收益</h2><p> 命名空间，是一个逻辑分组的概念，可见这个概念是为了多用户准备的，为的就是资源隔离。资源的隔离直接带来了几个特性： </p>
<p>1、资源在Hbase中，逻辑上可以用的资源是table，在物理上可以用的资源是region(当然还有更细力度的storeFile) </p>
<p>2、资源的分配主要是用来控制不同的namespace可以使用的资源</p>
<p>3、资源的存储 Hbase对于一个逻辑的资源table，如何能根据其所在的空间，被固定的分配到一个或者个多个regionserver上 </p>
<p>4、为不同的用户提供不同的资源 </p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><p>在系统中，有两个默认的命名空间，一个是hbase，是系统表.meta.使用的；另一个是default，主要是提供所有没有指定命名空间表用的。 </p>
<p>接下来分析一个自定义的命名空间的创建和使用</p>
<p>创建一个命名空间 最简单的办法是在hbase shell中创建，例如：create_namespace ‘lap’， </p>
<p>在创建完后，最直观的是在/hbase/data/下多了一个文件夹lap，同时，在zk的/hbase/namespace下也会多一个lap的节点。</p>
<p>在命名空间下创建一个表 create ‘lap:lapTable’,’info’。</p>
<p>删除一个命名空间 drop_namespace ‘lap’ </p>
<h3 id="使用namespace实现资源隔离"><a href="#使用namespace实现资源隔离" class="headerlink" title="使用namespace实现资源隔离"></a>使用namespace实现资源隔离</h3><p>每一个Table都隶属于一个NameSpace之下，每个NameSpace都有自己的Quota信息。例如Region的个数、hfile存储空间、甚至HBase之上的RPC的流量控制等。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">NamespaceDescriptor namespaceDescriptor = admin.getNamespaceDescriptor(tableName);</div><div class="line">namespaceDescriptor.setConfiguration(<span class="string">"hbase.namespace.quota.maxregion"</span>, <span class="string">"10"</span>);</div><div class="line">namespaceDescriptor.setConfiguration(<span class="string">"hbase.namespace.quota.maxtables"</span>, <span class="string">"10"</span>);</div></pre></td></tr></table></figure>
<p>这一部分在0.10中还未实现，相信在新的版本中，一定能给出好的解决方案。</p>
<h1 id="Table元数据操作"><a href="#Table元数据操作" class="headerlink" title="Table元数据操作"></a>Table元数据操作</h1><h2 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h2><p>table的操作 对表的操作主要包括增、删、改、查，接下来用一段代码来展示</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">voidcreateTable</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> Exception</span>&#123;</div><div class="line"> 	Configuration conf = HBaseConfiguration.create();</div><div class="line">	HBaseAdmin admin = <span class="keyword">new</span> HBaseAdmin(conf);</div><div class="line">	<span class="comment">//创建一个新的命名空间</span></div><div class="line">	admin.createNamespace(NamespaceDescriptor.create(<span class="string">"lap"</span>).build());</div><div class="line">	<span class="comment">//申请新的表到该命名空间</span></div><div class="line">	HTableDescriptor tableDescriptor = <span class="keyword">new</span> HTableDescriptor(TableName.valueOf(<span class="string">"lap:"</span>+tableName));</div><div class="line">	<span class="comment">//设置表的属性，比如setDurability是设置acl的属性，而后一个直接设置所有的属性，当然这部分还包括列簇的属性</span></div><div class="line">	tableDescriptor.setDurability(Durability.SYNC_WAL);</div><div class="line">	tableDescriptor.setConfiguration(<span class="string">"BLOOMFILTER"</span>, <span class="string">"ROW"</span>);</div><div class="line"></div><div class="line">  	<span class="comment">//创建一个列簇并且加入到表中</span></div><div class="line">	HColumnDescriptor hColumnDescriptor = <span class="keyword">new</span> HcolumnDescriptor(<span class="string">"cf"</span>);</div><div class="line">	<span class="comment">//设置列簇的属性</span></div><div class="line">	hColumnDescriptor.setBloomFilterType(BloomType.ROW);</div><div class="line">	tableDescriptor.addFamily(hColumnDescriptor);</div><div class="line">	<span class="comment">//创建表</span></div><div class="line">	admin.createTable(tableDescriptor);</div><div class="line">	<span class="comment">//关闭链接</span></div><div class="line">	admin.close();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>从上面可以看出来，表有两个Descriptor需要设置，一个是namespace的，一个是本身的。</p>
<p>表的命名空间的描述短时间内应该不会有很大的应用，因为不是很完善。而本身的属性会有很多需要设置的，比如：DURABILIT（设置写日志的方式）、MEMSTORE_FLUSHSIZE（memstore刷到硬盘上的大小）、BLOOMFILTER（roekey的头压缩）、DATA_BLOCK_ENCODING（数据部分的压缩）、TTL（过期时间）、IN_MEMORY（是否在内存中）等一系列的属性。这些属性会直接影响到表的查询和写入速度等特性，所以需要根据实际的数据设置。</p>
<h2 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deleteTable</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> Exception</span>&#123;</div><div class="line">	Configuration conf = HBaseConfiguration.create();</div><div class="line">	HBaseAdmin admin = <span class="keyword">new</span> HBaseAdmin(conf);</div><div class="line">	<span class="keyword">if</span>(admin.tableExists(tableName))&#123;</div><div class="line">		<span class="keyword">try</span>&#123;</div><div class="line">			<span class="comment">//必须要禁用表</span></div><div class="line">			admin.disableTable(tableName);</div><div class="line">			<span class="comment">//删除表</span></div><div class="line">			admin.deleteTable(tableName);</div><div class="line">		&#125;<span class="keyword">catch</span>(Exception e)&#123;</div><div class="line">			e.printStackTrace();</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	admin.close();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>可见，删除表的时候就比较简单了，直接用admin的对象，禁用表（这里必须禁用，防止别人正在操作该表，出现异常的问题），然后直接删除就可以了，在命令行是同样的道理。</p>
<h2 id="更新表"><a href="#更新表" class="headerlink" title="更新表"></a>更新表</h2><p>更新表分为两个部分，一部分是更新表的属性，一部分是更新表中的数据，所以这里要分两个部分讲： 对于更新表的属性，这里依旧要用表的描述符来修改，主要的操作包括，增加删除一个列簇，修改一个列簇的属性，具体如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">alterTable</span><span class="params">(String tableName)</span> <span class="keyword">throws</span> Exception</span>&#123;</div><div class="line">	Configuration conf = HBaseConfiguration.create();</div><div class="line">	HBaseAdmin admin= <span class="keyword">new</span> HBaseAdmin(conf);</div><div class="line">	<span class="keyword">if</span>(admin.tableExists(tableName))&#123;</div><div class="line">		<span class="comment">//和删除同样的道理，在更改表的描述符之前，必须先disable掉表</span></div><div class="line">		admin.disableTable(tableName);</div><div class="line">		<span class="comment">//获取当前表的描述符</span></div><div class="line">      	HTableDescriptor descriptor = admin.getTableDescriptor(Bytes.toBytes(tableName));</div><div class="line">		<span class="comment">//判断如果存在info的列簇，删除</span></div><div class="line">		<span class="keyword">if</span>(descriptor.hasFamily(Bytes.toBytes(<span class="string">"info"</span>)))&#123;</div><div class="line">			descriptor.removeFamily(Bytes.toBytes(<span class="string">"info"</span>));</div><div class="line">         &#125;</div><div class="line">		<span class="comment">//创建一个新的列簇</span></div><div class="line">		HColumnDescriptor hColumnDescriptor = <span class="keyword">new</span> HColumnDescriptor(Bytes.toBytes(<span class="string">"lapNew"</span>));</div><div class="line">		<span class="comment">//设置列簇的属性</span></div><div class="line">		hColumnDescriptor.setCompressionType(Algorithm.GZ);</div><div class="line">		<span class="comment">//为表增加一个新的列簇</span></div><div class="line">      	descriptor.addFamily(hColumnDescriptor);</div><div class="line">		<span class="comment">//获取到原先表中的列簇对象</span></div><div class="line">		HColumnDescriptor changeInfo = descriptor.getFamily(Bytes.toBytes(<span class="string">"clomun"</span>));</div><div class="line">		<span class="comment">//修改该对象的属性</span></div><div class="line">		changeInfo.setBlockCacheEnabled(<span class="keyword">true</span>);</div><div class="line">		<span class="comment">//让新的表描述符生效</span></div><div class="line">		admin.modifyTable(tableName, descriptor);</div><div class="line">		<span class="comment">//启用表</span></div><div class="line">		admin.enableTable(tableName);</div><div class="line">      	admin.close();</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>对于表内容的修改，其实就是增加一行，因为Hbase的存储直接依赖与HDFS，而HDFS是只支持创建和追加的，不支持某一行的单独修改，所以Hbase对表的增删改的操作，事实上都是在对表增加，不同的是，如果是删除，会在表中删除字段标记为true，在表进行major compaction的时候，会删除该行。对表的修改实际是，增加一个新的时间戳或者新版本的行，在查询的时候，会按照时间戳排序，保证能查到最新的修改。鉴于上面这些，这里不会单独讲表内容的修改，会在接下来文章中详细将对与表内容的所有动作。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/08/17/HBase读写性能相关-请求不均匀的问题优化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/08/17/HBase读写性能相关-请求不均匀的问题优化/" itemprop="url">HBase读写性能相关-请求不均匀的问题优化</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-08-16T21:39:13-08:00">
                2014-08-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="影响到随机读的速度的主要因素"><a href="#影响到随机读的速度的主要因素" class="headerlink" title="影响到随机读的速度的主要因素"></a>影响到随机读的速度的主要因素</h1><p>1、请求次数的分布均衡、</p>
<p>2、StoreFile数量、</p>
<p>3、表属性的设置</p>
<p>4、Cache大小以及命中率</p>
<p>今天主要讨论的是请求不均匀的问题</p>
<h1 id="Hbase的负载均衡的判断"><a href="#Hbase的负载均衡的判断" class="headerlink" title="Hbase的负载均衡的判断"></a>Hbase的负载均衡的判断</h1><p>各region server的region数是否均衡</p>
<p>读写请求是否均衡分布。</p>
<h1 id="保证Hbase集群LB的方法"><a href="#保证Hbase集群LB的方法" class="headerlink" title="保证Hbase集群LB的方法"></a>保证Hbase集群LB的方法</h1><p>对于region的个数，Hbase的balance策略会保证</p>
<p>对于table的region分布均衡，读请求仍然不均衡分布的情况，说明应用的请求有热点的状况，如这种状况造成了读速度的不OK，可以手工将region进行拆分，并分配到不同的region server上，这是hbase很简单的一种应对热点的解决方法。</p>
<h1 id="统计方法"><a href="#统计方法" class="headerlink" title="统计方法"></a>统计方法</h1><p>先用execel把相应表的RS列和TPS列单独取出来放到一个文件中，用如下统计脚本进行统计</p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/perl -w </span></div><div class="line"><span class="keyword">use</span> Data::Dumper; </div><div class="line"><span class="keyword">open</span>(FILE,<span class="string">"&lt;"</span>,$ARGV[<span class="number">0</span>]) <span class="keyword">or</span> <span class="keyword">die</span> <span class="string">"can not open file"</span>; </div><div class="line"><span class="keyword">my</span> %allStatus = (); </div><div class="line"><span class="keyword">while</span> ()&#123; </div><div class="line">    <span class="keyword">if</span> ($_ =~ <span class="regexp">m/(\S+)\s+(\d+)/</span>)&#123; </div><div class="line">	     $allStatus&#123;$1&#125; += $2; </div><div class="line">	&#125;   </div><div class="line">&#125; </div><div class="line"><span class="keyword">foreach</span> <span class="keyword">my</span> $key (<span class="keyword">keys</span>%allStatus)&#123; </div><div class="line">    <span class="keyword">print</span> <span class="string">"$key "</span>.$allStatus&#123;$key&#125;.<span class="string">"\n"</span>; </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>选择了TPS比较高，又经常访问的表CleanOfflineUserProfileTempV1，进行了查询。结果如下(这里省略)</p>
<p>可以明显看到分成了4个量级别的请求数目，不同的之间TPS相差了50W的请求量，可见不同的regionserver之间的请求差别很大。</p>
<h1 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h1><p> 通过实验对比因为访问不均衡导致的时间差别。</p>
<p> 1、先随机的写入100w条的数据           </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">time ./hbase org.apache.hadoop.hbase.PerformanceEvaluation  randomWrite 1</div></pre></td></tr></table></figure>
<p>2、查看regionserver的分布状态</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">TestTable,,1405596957945.af6be5a5763589d054d77ab533af721f. hadoop141:60030</div><div class="line">TestTable,0000262285,1405597137509.f376305528ef3570901120a457d80a2f. hadoop141:60030</div><div class="line">TestTable,0000393515,1405597137509.4a9738e4f29128709a0b2cd63ed8d39d. hadoop141:60030</div><div class="line">TestTable,0000524451,1405597162495.47c7234c3d48a1b75a521eaaedae72b4. hadoop141:60030</div><div class="line">TestTable,0000654788,1405597162495.53c2647b5f707032dc3839cdea26965e. hadoop141:60030</div><div class="line">TestTable,0000785234,1405596978669.810195f07f006d900913898abf4385e1. hadoop141:60030</div></pre></td></tr></table></figure>
<p>3、接下来进行实验的重点部分</p>
<p>需要用到的命令</p>
<p>a、测试100w条的随机读</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">time ./hbase org.apache.hadoop.hbase.PerformanceEvaluation  randomRead 1</div></pre></td></tr></table></figure>
<p>b、关闭balancer</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hbase shell &gt; balance_switch flase</div></pre></td></tr></table></figure>
<p>c、手动move region</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&quot;move &apos;96b1f2b1569aec38030d2c0c95eb9dc5&apos;,&apos;dw-perf-11,60020,1328178878605&apos;</div></pre></td></tr></table></figure>
<p>其中第一个参数为要复制的region的startKey，第二个参数分别是要迁移到的机器，端口号，要插入的起始地址</p>
<p>调整后的测试结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">TestTable,,1405596957945.af6be5a5763589d054d77ab533af721f. hadoop131:60030</div><div class="line">TestTable,0000262285,1405597137509.f376305528ef3570901120a457d80a2f. hadoop132:60030</div><div class="line">TestTable,0000393515,1405597137509.4a9738e4f29128709a0b2cd63ed8d39d. hadoop142:60030</div><div class="line">TestTable,0000524451,1405597162495.47c7234c3d48a1b75a521eaaedae72b4. hadoop141:60030</div><div class="line">TestTable,0000654788,1405597162495.53c2647b5f707032dc3839cdea26965e. hadoop141:60030</div><div class="line">TestTable,0000785234,1405596978669.810195f07f006d900913898abf4385e1. hadoop132:60030</div></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">调整后</th>
<th style="text-align:center">调整前</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">内部执行时间</td>
<td style="text-align:center">1156830ms</td>
<td style="text-align:center">1741539ms</td>
</tr>
<tr>
<td style="text-align:center">在用户态的耗时</td>
<td style="text-align:center">3m38.686s</td>
<td style="text-align:center">3m34.142s</td>
</tr>
<tr>
<td style="text-align:center">程序本身的耗时</td>
<td style="text-align:center">19m20.268s</td>
<td style="text-align:center">29m5.021s</td>
</tr>
</tbody>
</table>
<p>可见，在这个实验中，对于不同的分布，对请求的结果差会有30%到40%的性能差。</p>
<h1 id="修复方法"><a href="#修复方法" class="headerlink" title="修复方法"></a>修复方法</h1><p>​      手动的把表进行split，表被拆分到小表后，会使访问更加均匀，也可以用其它方法。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/08/15/HBase读写性能相关-客户端的程序优化建议/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/08/15/HBase读写性能相关-客户端的程序优化建议/" itemprop="url">HBase读写性能相关-客户端的程序优化建议</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-08-14T21:35:16-08:00">
                2014-08-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="对于rowkey的设计"><a href="#对于rowkey的设计" class="headerlink" title="对于rowkey的设计"></a>对于rowkey的设计</h1><p>a、表的设计，尽量要把一起访问的放到一个cf里面</p>
<p>b、借助HBase单行操作的原子性。一个用户的所有信息存储成一行,这包括消息本身、Message Index、Search Index、以及相关Meta Data(Actions:addMessage、markAsRead,etc)</p>
<p>c、存储时，数据按照Row key的字典 序(byte order)排序存储。设计key时，要充分排序存储这个特性，将经常一起读取的行存储放到一起。(位置相关性)</p>
<p>d、推介RowKey：md5sum(userid)+userid。</p>
<h2 id="关于表的scan操作"><a href="#关于表的scan操作" class="headerlink" title="关于表的scan操作"></a>关于表的scan操作</h2><p>a、每次遍历完Scan的返回结果集后，都不要忘了关闭ResultScanner对象，否则会引发问题。</p>
<p>b、如果生产环境有mapred任务去scan hbase的时候，如果读取的是热点数据，可以在mapred scan类中加一个</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scan.setCacheBlocks(<span class="keyword">true</span>)</div></pre></td></tr></table></figure>
<p>c、几个重要的方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setBatch</span><span class="params">(<span class="keyword">int</span> batch)</span> ：<span class="comment">//为设置获取记录的列个数，默认无限制，也就是返回所有的列</span></span></div><div class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setCaching</span><span class="params">(<span class="keyword">int</span> caching)</span>：<span class="comment">//每次从服务器端读取的行数，默认为配置文件中设置的值,可以减少RPC的次数：</span></div><div class="line">HTable.<span class="title">setScannerCaching</span><span class="params">(<span class="keyword">int</span> scannerCaching)</span></div></pre></td></tr></table></figure>
<p>注意：有三个地方可以进行配置(上面三个参数)：</p>
<p>1）在HBase的conf配置文件中进行配置；</p>
<p>2）通过调用HTable.setScannerCaching(int scannerCaching)进行配置；</p>
<p>3）通过调用Scan.setCaching(int caching)进行配置。三者的优先级越来越高。</p>
<p> RPCs=(Rows* Cols per Row) / Min(Cols per Row, Batch size) / Scanner caching</p>
<p>d、默认情况下，HBase的自动刷新功能是处于激活状态的。则样每一个单个的Put都会自动调用一次RPC向server发送一次请求，为性能计，可以关闭自动刷新功能(setAutoFlush(false))，这样会客户端的write buffer已“满”或客户端强制flushComits()刷新再或者调用HTable的close()时才调用RPC一次把多个Put操作发送给服务器。</p>
<h2 id="关于put操作"><a href="#关于put操作" class="headerlink" title="关于put操作"></a>关于put操作</h2><p>a、在Put操作保存数据时，可以考虑关闭WAL的功能——调用writeToWAL(false)，这意味着在写数据时不会讲数据先写到WAL日志文件，直接写入memstore，这可以提高一些性能。但是者会带来一些副作用，那就是当Region Server出问题时，没有写log的数据会丢失，这在保存重要数据时会带来很大的危险，只有在保存不南无重要的数据时才考虑这样做。</p>
<p>或者设置setDeferredLogFlush为true，服务端将采用异步写hlog的方式，客户端写响应时间会降到1ms以下。如果regionserver挂掉会丢失1s的数据。同样有数据丢失风险，这种方式会小很多很多（rs被kill -9或者物理机器直接宕机），而且写入的数据马上可以被读取到，客户端也不需要采取特别的策略。</p>
<p>b、setWriteBufferSize可以设置table的客户端缓存。</p>
<p>c、通过batch 方式来put：HTable.put(List)</p>
<h1 id="其它的一些技术"><a href="#其它的一些技术" class="headerlink" title="其它的一些技术"></a>其它的一些技术</h1><p> 使用filter <a href="http://mysun.iteye.com/blog/1584635" target="_blank" rel="external">http://mysun.iteye.com/blog/1584635</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/07/30/HBase读写性能相关-关于缓存/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/07/30/HBase读写性能相关-关于缓存/" itemprop="url">HBase读写性能相关-关于缓存</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-07-29T19:24:06-08:00">
                2014-07-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="命中缓存的重要性"><a href="#命中缓存的重要性" class="headerlink" title="命中缓存的重要性"></a>命中缓存的重要性</h1><p>做一个简单的小实验，读取同样5条数据，命中catch和不命中catch的对比：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[hadoop@bjlg-49p21-hadoop-client03 hbase]$ ./start.sh </div><div class="line">83bd782bcb74fca2000032170000503e4fe29093 57848 </div><div class="line">a0225452001ccb15000055ce0004c15d538f05d9 895 </div><div class="line">a0225452001ccb150000793e00001258538efb1c 606 </div><div class="line">b12ed4ae526364b50007d61e0024a45652ee6a4e 189248 </div><div class="line">92fbd4ae526c85c80008fb96000997fa514811d6 3387</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[hadoop@bjlg-49p21-hadoop-client03 hbase]$ ./start.sh </div><div class="line">83bd782bcb74fca2000032170000503e4fe29093 4928 </div><div class="line">a0225452001ccb15000055ce0004c15d538f05d9 660 </div><div class="line">a0225452001ccb150000793e00001258538efb1c 762 </div><div class="line">b12ed4ae526364b50007d61e0024a45652ee6a4e 1227 </div><div class="line">92fbd4ae526c85c80008fb96000997fa514811d6 1218</div></pre></td></tr></table></figure>
<h1 id="提高缓存命中率的理论"><a href="#提高缓存命中率的理论" class="headerlink" title="提高缓存命中率的理论"></a>提高缓存命中率的理论</h1><p>1、总的catch的大小，这个调整主要通过提高Regionserver的堆内存和提高hfile.block.cache.size的百分比来提高</p>
<p>2、数据本身的合理设计，把可能会一起访问的数据设计到一起，把需要整行读取的数据设计到同一个CF里面。如果数据本身不存在热点，就会导致访问完全随机，就只能靠加机器来提高命中率了。</p>
<p>3、block块的大小，由于每次存放到catch中的数据是一整个block，而不是某个key-value。所以一般来说，不同的数据，有着不同的效果。对于非重复有热点的数据来说，如果block块设置大的话，catch命中率会随着块的增大而增加；对于重复的无热点数据来说，刚好相反。(这里要插一句，block本身的大小很相影响随机读的效果，如果块大的话，索引就相对较小，随机读效果就差)</p>
<h1 id="关于Hbase的catch本身的实现"><a href="#关于Hbase的catch本身的实现" class="headerlink" title="关于Hbase的catch本身的实现"></a>关于Hbase的catch本身的实现</h1><p>在Hbase的每个Regionserver中，一共维护了3个catch队列：</p>
<p>Single：如果一个Block第一次被访问，则放在这一优先级队列中；</p>
<p>Multi：如果一个Block被多次访问，则从Single队列移到Multi队列中；</p>
<p>InMemory：如果一个Block是inMemory的，则放到这个队列中，例如-ROOT-表和.META.表。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">hbase(main):002:0&gt; describe &apos;.META.&apos; </div><div class="line">DESCRIPTION                                                       ENABLED                           </div><div class="line"> &#123;NAME =&gt; &apos;.META.&apos;, IS_META =&gt; &apos;true&apos;, FAMILIES =&gt; [&#123;NAME =&gt; &apos;inf true                              </div><div class="line"> o&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, BLOOMFILTER =&gt; &apos;NONE&apos;, REPLIC                                   </div><div class="line"> ATION_SCOPE =&gt; &apos;0&apos;, COMPRESSION =&gt; &apos;NONE&apos;, VERSIONS =&gt; &apos;10&apos;, TTL                                   </div><div class="line">  =&gt; &apos;2147483647&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, KEEP_DELETED_CELLS =&gt; &apos;fa                                   </div><div class="line"> lse&apos;, BLOCKSIZE =&gt; &apos;8192&apos;, ENCODE_ON_DISK =&gt; &apos;true&apos;, IN_MEMORY =                                   </div><div class="line"> &gt; &apos;true&apos;, BLOCKCACHE =&gt; &apos;true&apos;&#125;]&#125;                                                                  </div><div class="line">1 row(s) in 0.0550 seconds </div><div class="line">hbase(main):003:0&gt; describe &apos;t3&apos; </div><div class="line">DESCRIPTION                                                       ENABLED                           </div><div class="line"> &#123;NAME =&gt; &apos;t3&apos;, FAMILIES =&gt; [&#123;NAME =&gt; &apos;f3&apos;, DATA_BLOCK_ENCODING = true                              </div><div class="line"> &gt; &apos;NONE&apos;, BLOOMFILTER =&gt; &apos;NONE&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;, VERSI                                   </div><div class="line"> ONS =&gt; &apos;3&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, TTL =&gt; &apos;                                   </div><div class="line"> 2147483647&apos;, KEEP_DELETED_CELLS =&gt; &apos;false&apos;, BLOCKSIZE =&gt; &apos;65536&apos;                                   </div><div class="line"> , IN_MEMORY =&gt; &apos;false&apos;, ENCODE_ON_DISK =&gt; &apos;true&apos;, BLOCKCACHE =&gt;                                    </div><div class="line"> &apos;true&apos;&#125;, &#123;NAME =&gt; &apos;f4&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, BLOOMFILT                                   </div><div class="line"> ER =&gt; &apos;NONE&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;, VERSIONS =&gt; &apos;3&apos;, COMPRES                                   </div><div class="line"> SION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, TTL =&gt; &apos;2147483647&apos;, KEEP_D                                   </div><div class="line"> ELETED_CELLS =&gt; &apos;false&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, IN_MEMORY =&gt; &apos;fal                                   </div><div class="line"> se&apos;, ENCODE_ON_DISK =&gt; &apos;true&apos;, BLOCKCACHE =&gt; &apos;true&apos;&#125;]&#125;                                             </div><div class="line">1 row(s) in 0.0470 seconds</div></pre></td></tr></table></figure>
<p>这三个队列Single、Multi、InMemory分别占用了总大小的0.25、0.50和0.25。队列如何生成的呢？对于第一次访问到集群的数据，如果不是被标记为IN_MEMORY，会将它写入single队列，否则写入memory队列。当再次访问该数据并且在single中读到了该数据时，single会升级为multi。通过将缓存分级的好处是避免Cache之间相互影响，尤其是对HBase来说像Meta表这样的Cache应该保证高优先级。</p>
<h1 id="统计命中率的方法"><a href="#统计命中率的方法" class="headerlink" title="统计命中率的方法"></a>统计命中率的方法</h1><p>目前在ganglia中没有提供特别有效的命中率的详细统计，主要通过一下方式获取,接下来以获取访问命中率的例子来进行分析。提供一个脚本通过日志来获取7天内的历史命中率</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">#!/bin/bash </div><div class="line">baseDir=$(cd $(dirname $0); pwd) </div><div class="line">hbaseLogDir=&quot;/opt/hbase_logs/&quot; </div><div class="line">checkServerList=$(cat /opt/hbase/conf/regionservers) </div><div class="line">checkUser=&quot;hadoop&quot; </div><div class="line">checkStr=&quot;org.apache.hadoop.hbase.io.hfile.LruBlockCache&quot; </div><div class="line">logPrefix=&quot;hbase-hadoop-regionserver-&quot; </div><div class="line">resultDir=$&#123;baseDir&#125;/result </div><div class="line">max=7 </div><div class="line">if [ ! -d $resultDir ];then </div><div class="line">    mkdir $resultDir </div><div class="line">fi </div><div class="line">for checkServer in $&#123;checkServerList&#125; </div><div class="line">    do </div><div class="line">        for ((i=0 ; i &lt; $max ; i++)) </div><div class="line">           do </div><div class="line">               logFile= </div><div class="line">                if [ $i -ne 0 ];then </div><div class="line">                    day=.$(date +%Y-%m-%d --date &quot;$i days ago&quot;) </div><div class="line">                fi </div><div class="line">                hostname=$(ssh $&#123;checkUser&#125;@$&#123;checkServer&#125; &quot;hostname&quot;) </div><div class="line">                logFile=$&#123;hbaseLogDir&#125;$&#123;logPrefix&#125;$&#123;hostname&#125;.log$&#123;day&#125; </div><div class="line">                echo $logFile </div><div class="line">                ssh $&#123;checkUser&#125;@$&#123;checkServer&#125; &quot;grep $&#123;checkStr&#125; -r $&#123;logFile&#125;&quot; &gt;&gt; $&#123;resultDir&#125;/$&#123;checkServer&#125; </div><div class="line">            done </div><div class="line">        #echo $result &gt;&gt;$&#123;resultDir&#125;/$&#123;checkServer&#125; </div><div class="line">    done</div></pre></td></tr></table></figure>
<p>通过awk呈现历史的命中率：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">2014-07-15 00:18:36,290 DEBUG org.apache.hadoop.hbase.io.hfile.LruBlockCache: LRU Stats: total1.86 GB, free=367.23 MB, max=2.22 GB, blocks=29427, accesses=109550765, hits=84075136, hitRatio=76.74%, cachingAccesses=88885825, cachingHits=83592639, cachingHitsRatio=94.04%, evictions=1248, evicted=4889752, evictedPerRun=3918.070556640625</div></pre></td></tr></table></figure>
<p>上面展示了一个LRU日志的内容，接下来简单解析一下LRU日志的各个参数的含义：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">total 当前catch正在使用的内存大小</div><div class="line">free 系统可用的catch的大小</div><div class="line">max  总的可以用于catch的内存大小</div><div class="line">blocks   当前系统的catch中存储的block的个数</div><div class="line">accesses  catch被访问的总的次数</div><div class="line">hits    catch被访问，而且命中的次数</div><div class="line">hitRatio   当前的catch的总的命中率</div><div class="line">catchAccesses 总共请求查找块的次数</div><div class="line">catchingHits 请求查找块的命中次数</div><div class="line">catchHistsRatio 查找块的位置的命中率</div><div class="line">evictions  清除catch发成的次数</div><div class="line">evicted    被清除的block的总数</div><div class="line">evictedPerRun  平均每次清除的个数</div></pre></td></tr></table></figure>
<p>我们需要的是hitRatio的数据，这部分数据显示的是总的请求的命中率</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat $file| grep &quot;cachingHitsRatio&quot; |awk -F &quot;,&quot; &apos;&#123;print $1 $8&#125;&apos;</div></pre></td></tr></table></figure>
<p>通过这个命令，可以看到系统每隔5分钟会进行，平均的命中率是75%</p>
<p>通过perl脚本获取到单个节点的平均命中率：</p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/perl </span></div><div class="line"><span class="keyword">my</span> $file = $ARGV[<span class="number">0</span>]; </div><div class="line"><span class="keyword">my</span> $ratioBefore; </div><div class="line"><span class="keyword">my</span> $ratioLast; </div><div class="line"><span class="keyword">my</span> $count = <span class="number">0</span>; </div><div class="line"><span class="keyword">if</span> ( -e $file )&#123; </div><div class="line">    <span class="keyword">open</span>(FILE,<span class="string">"&lt;"</span>,$file)||<span class="keyword">die</span> <span class="string">"can not open $file\n"</span>; </div><div class="line">    <span class="keyword">while</span>()&#123; </div><div class="line">        <span class="keyword">if</span> ( $_ =~ <span class="regexp">m/hitRatio=(\d+.\d+)%/</span>)&#123; </div><div class="line">            $ratioBefore += $1; </div><div class="line">            $count += <span class="number">1</span>; </div><div class="line">        &#125;   </div><div class="line">    &#125;   </div><div class="line">&#125;</div><div class="line"><span class="keyword">print</span> $ratioBefore/$count;</div></pre></td></tr></table></figure>
<h1 id="提高catch的命中率"><a href="#提高catch的命中率" class="headerlink" title="提高catch的命中率"></a>提高catch的命中率</h1><p>1、在系统堆内存总量不边的情况下，修改了以下的参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hbase.regionserver.global.memstore.upperLimit  0.35</div></pre></td></tr></table></figure>
<p>在Region服务器中所有的memstore所占Java虚拟机比例的最大值  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hbase.regionserver.global.memstore.lowerLimit  0.30</div></pre></td></tr></table></figure>
<p>在Region服务器中所有的memstore所占Java虚拟机比例的最小值 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hfile.block.cache.size  0.40</div></pre></td></tr></table></figure>
<p>HFile/StoreFile缓存所占java虚拟机堆的大小的百分比 </p>
<p> 修改了这部分参数意义上是减少了写缓存的数量，提高了读缓存的数量。</p>
<p>2、使用使用LRU和BucketCatche结合的方式时候</p>
<p>3、修改表的属性，包括DATA_BLOCK_ENCODING、BLOCKSIZE对命中率有一定的影响</p>
<p>4、合理的设置rowkey，建议用md5等方式，保证分布足够均匀</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/07/28/HBase读写性能相关-基本优化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/07/28/HBase读写性能相关-基本优化/" itemprop="url">HBase读写性能相关-基本优化</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-07-27T21:52:42-08:00">
                2014-07-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="split触发时机"><a href="#split触发时机" class="headerlink" title="split触发时机"></a>split触发时机</h1><p>a、compact之后会调用CompactSplitThread.requestSplit(HRegion)</p>
<p>b、flush之前会检测区域中HStoreFile数目是否超hbase.hstore.blockingStoreFiles，如果超过且没有等待超时会调用CompactSplitThread.requestSplit(HRegion)</p>
<p>c、flush之后会调用HRegion.checkSplit()检测是否需要split，如果需要则调用  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">CompactSplitThread.requestSplit(HRegion)</div></pre></td></tr></table></figure>
<p>d、手动的split</p>
<h1 id="compact发生的时间"><a href="#compact发生的时间" class="headerlink" title="compact发生的时间"></a>compact发生的时间</h1><p>a、Memstoreflush时；</p>
<p>b、HRegionServer定期做Compaction Checker时；</p>
<p>c、HBaseAdmin客户端发起的请求；</p>
<p>d、CompactTool发起。</p>
<h1 id="影响写速度的因素"><a href="#影响写速度的因素" class="headerlink" title="影响写速度的因素"></a>影响写速度的因素</h1><p>1、rowKey是否足够合理</p>
<p>因为hbase是按rowKey连续存储的，因此如应用写入数据时rowKey是连续的，那么就会造成写的压力会集中在单台region server上，这样造成了负载不均衡。因此应用在设计rowKey时，要尽可能的保证写入是分散的，当然，这可能会对有连续读需求的场景产生一定的冲击。</p>
<p>2、写阻塞等待</p>
<p>造成这个现象的原因是各个region的memstore使用的大小加起来超过了总的阈值，于是阻塞并开始进行flush，这个过程会需要消耗掉一些时间，通常来说造成这个的原因是单台RegionServer上region数太多了，因此其实单台RegionServer上最好不要放置过多的region，一种调节方法是调大split的fileSize，这样可以适当的减少region数，但需要关注调整后读性能的变化。一般会报错：Flush thread woke up with memory above low water。或者在memstore的大小超过其阈值的时候，也会进行flush，错误日志：delaying flush up to。</p>
<p>3、compact带来的影响</p>
<p>通常是store file太多compact造成的，错误日志： has too many store files; delaying flush up to</p>
<p> 4、 split造成的</p>
<p>split会造成读写短暂的失败，如写的数据比较大的话，可能会有频繁的split出现，sptlit期间会让region下线，然后block住该region的写请求。</p>
<h1 id="随机写的测试工具YCSB"><a href="#随机写的测试工具YCSB" class="headerlink" title="随机写的测试工具YCSB"></a>随机写的测试工具YCSB</h1><p> ycsb是一个非常方便的针对分布式文件系统的测试工具,它的特点是：1 可以任意设置读写比例、线程数量，打印结果比较详细；2 它是hbase等nosql官方jira上面的测试标准，与人交流时ycsb的测试数据最能说明问题。</p>
<p>下载0.1.3版本，编译hbase的测试类，把hbase的配置文件拷贝到db/hbase/conf下，依次执行ant &amp;&amp; ant dbcompile-hbase</p>
<p>执行随机写的测试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">java -cp build/ycsb.jar:db/hbase/lib/* com.yahoo.ycsb.Client -load -db com.yahoo.ycsb.db.HBaseClient -P workloads/workloada -p columnfamily=family -p table=hello -p recordcount=100000000 -p threadcount=4  -s 2&gt;&amp;1 &gt;result</div></pre></td></tr></table></figure>
<p> 执行随机读的测试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">java -cp build/ycsb.jar:db/hbase/lib/* com.yahoo.ycsb.Client -t -db com.yahoo.ycsb.db.HBaseClient -P workloads/workloada -p columnfamily=info -p table=hee -p operationcount=100000 -s -threads 10 -target 100 &gt; transactions.dat</div></pre></td></tr></table></figure>
<p> workload[a-f]依次代表的百分比为：</p>
<p>结果分析(以插入1000000条数据的结果做分析)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">3 [OVERALL], RunTime(ms), 2459212.0  //总的运行时间</div><div class="line"></div><div class="line">4 [OVERALL], Throughput(ops/sec), 406.63432026193755  //每秒内插入的数据 </div><div class="line"></div><div class="line">5 [INSERT], Operations, 1000000 //插入的总数量</div><div class="line"></div><div class="line">6 [INSERT], AverageLatency(ms), 9.27774  //每条数据的插入时间</div><div class="line"></div><div class="line">7 [INSERT], MinLatency(ms), 0 //最短插入间隔</div><div class="line"></div><div class="line">8 [INSERT], MaxLatency(ms), 265720 //最长的插入间隔</div><div class="line"></div><div class="line">9 [INSERT], 95thPercentileLatency(ms), 0 </div><div class="line"></div><div class="line">10 [INSERT], 99thPercentileLatency(ms), 0 </div><div class="line"></div><div class="line">11 [INSERT], Return=0, 1000000 //插入1000000万条的返回值</div><div class="line"></div><div class="line">12 [INSERT], 0, 994510 //有994510条数据在0-1s内插入</div></pre></td></tr></table></figure>
<h1 id="表的信息影响随机读写性能的测试"><a href="#表的信息影响随机读写性能的测试" class="headerlink" title="表的信息影响随机读写性能的测试"></a>表的信息影响随机读写性能的测试</h1><p>1、DEFERRED_LOG_FLUSH的设置：</p>
<p>设置数据表的DEFERRED_LOG_FLUSH属性为true，服务端将采用异步写hlog的方式，客户端写响应时间会降到1ms以下。如果regionserver挂掉会丢失1s的数据。同样有数据丢失风险，这种方式会小很多很多（rs被kill -9或者物理机器直接宕机），而且写入的数据马上可以被读取到，客户端也不需要采取特别的策略。</p>
<p>2、BLOOMFILTER 的设置：</p>
<p>对于某个region的随机读，HBase会遍历读memstore及storefile（按照一定的顺序），将结果合并返回给客户端。如果你设置了bloomfilter，那么在遍历读storefile时，就可以利用bloomfilter，忽略某些storefile</p>
<p>有两种类型的bloomfilter类型：</p>
<p> a)ROW, 根据KeyValue中的row来过滤storefile </p>
<p>举例：假设有2个storefile文件sf1和sf2， </p>
<p>sf1包含kv1（r1 cf:q1 v）、kv2（r2 cf:q1 v） </p>
<p>sf2包含kv3（r3 cf:q1 v）、kv4（r4 cf:q1 v） </p>
<p>如果设置了CF属性中的bloomfilter为ROW，那么get(r1)时就会过滤sf2，get(r3)就会过滤sf1 </p>
<p>b)ROWCOL,根据KeyValue中的row+qualifier来过滤storefile </p>
<p>举例：假设有2个storefile文件sf1和sf2， </p>
<p>sf1包含kv1（r1 cf:q1 v）、kv2（r2 cf:q1 v） </p>
<p>sf2包含kv3（r1 cf:q2 v）、kv4（r2 cf:q2 v） </p>
<p>如果设置了CF属性中的bloomfilter为ROW，无论get(r1,q1)还是get(r1,q2)，都会读取sf1+sf2； 而如果设置了CF属性中的bloomfilter为ROWCOL，那么get(r1,q1)就会过滤sf2，get(r1,q2)就会过滤sf1 </p>
<p>3、DATA_BLOCK_ENCODING的设置：</p>
<p> Data Block Encoding是0.94的重要特性，可以很大程度上改善lrucache的使用率，从而提高get scan性能。从测试结果来看设置DATA_BLOCK_ENCODING =&gt; ‘DIFF’可以获得最有性价比的性能提升。可以选择的类型包括：</p>
<p>PREFIX(这是一个比较公用的压缩算法，其压缩过程也比较简单快速，通用于存在相同的Row前缀的情景);</p>
<p>DIFF(相比于PREFIX，同样是需要应用在存在相同的Row前缀的情景，但其只写一份family内容，写入timestamp的差值，比较长度是否一样等特性，使其压缩幅度会更大，当然压缩的CPU开销也会稍大);    </p>
<p>FAST_DIFF(和DIFF类似，但是对于存在相同Value内容的场景,那肯定是使用它了)</p>
<p>更加详细的内容请参考   <a href="http://blog.cloudera.com/blog/2012/06/hbase-io-hfile-input-output/" target="_blank" rel="external">http://blog.cloudera.com/blog/2012/06/hbase-io-hfile-input-output/</a></p>
<p>4、BLOCKSIZE的设置(HBase-3864)：</p>
<p>在配置中有一个hbase.mapreduce.hfileoutputformat.blocksize，这个参数和表描述中的一样，主要是用来初始化MR的。配置中的采纳数主要是从HFileOutputFormat写hfile的时候，会强制改成这个值。但是表中的配置会覆盖这个配置，所以这个配置一般没有用。</p>
<p>5、COMPRESSION的设置：</p>
<p>设置压缩格式，可以间接的提高查询效率，但是会影响写入的效率。</p>
<p>接下来测试开启该配置和不开启的区别：</p>
<p>我们用4线程，写入100w条记录做实验，查看平均的插入时间</p>
<table>
<thead>
<tr>
<th>表”描述”</th>
<th>写入时间</th>
<th>读取时间</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>默认</code></td>
<td>668214ms</td>
<td>4768452ms</td>
</tr>
<tr>
<td><code>BLOOMFILTER =&gt; &#39;ROW&#39;</code></td>
<td>683444ms</td>
<td>3175800ms</td>
</tr>
<tr>
<td><code>DATA_BLOCK_ENCODING =&gt; &#39;DIFF&#39;</code></td>
<td>633093ms</td>
<td>3452708ms</td>
</tr>
<tr>
<td><code>BLOCKSIZE =&gt; &#39;16384&#39;</code></td>
<td>670335ms</td>
<td>3472540ms</td>
</tr>
</tbody>
</table>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2014/07/16/Hadoop和HBase开启对LZO压缩的支持/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="aiping.lap">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Aiping.LAP">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2014/07/16/Hadoop和HBase开启对LZO压缩的支持/" itemprop="url">Hadoop和HBase开启对LZO压缩的支持</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2014-07-15T21:19:12-08:00">
                2014-07-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这本来是想写一个小的系列，包括后续的压测，所以先从环境搭建开始.</p>
<p>PS:看了很多的文章后，总结出来的结论：网络上搭建方式各种坑，实践才是检验真理的唯一方式</p>
<h1 id="基础环境"><a href="#基础环境" class="headerlink" title="基础环境"></a>基础环境</h1><p>hadoop版本1.0.4,Hbase0.94.3，系统环境：Centos5X, 64位</p>
<h1 id="搭建过程"><a href="#搭建过程" class="headerlink" title="搭建过程"></a>搭建过程</h1><p> 一共需要准备4部分东西，这4个部分缺一不可。</p>
<h2 id="1、lzo的系统库文件"><a href="#1、lzo的系统库文件" class="headerlink" title="1、lzo的系统库文件"></a>1、lzo的系统库文件</h2><p>不用相信网上的方法，亲测了，和很多库一样，这个不需要编译环境和运行环境是同一套，不过最好本地和运行的基本库一样，否则会有一些系统库不是向下兼容的问题。</p>
<p>下载lzo-2.06.tar.gz</p>
<p>然后依次执行以下的命令编译系统库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">tar -zxvf lzo-2.06.tar.gz</div><div class="line"></div><div class="line">./configure --enable-shared --prefix /usr/local/lzo-2.06</div><div class="line"></div><div class="line">make</div><div class="line"></div><div class="line">make install</div></pre></td></tr></table></figure>
<p>生成的库文件如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">├── lib</div><div class="line">│   ├── liblzo2.a</div><div class="line">│   ├── liblzo2.la</div><div class="line">│   ├── liblzo2.so -&gt; liblzo2.so.2.0.0</div><div class="line">│   ├── liblzo2.so.2 -&gt; liblzo2.so.2.0.0</div><div class="line">│   └── liblzo2.so.2.0.0</div></pre></td></tr></table></figure>
<p>由于这些都是最基本的lzo压缩库，所以需要放到集群上的公共库中(我的系统是64位的，放在了/usr/lib64下面)，要求每个节点都有该库，因为所有的Map和Reduce都只依赖节点本身的库。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[hadoop@hadoop131 ~]$ ls /usr/lib64/liblzo2.*</div><div class="line"></div><div class="line">/usr/lib64/liblzo2.a   /usr/lib64/liblzo2.so    /usr/lib64/liblzo2.so.2.0.0</div><div class="line"></div><div class="line">/usr/lib64/liblzo2.la  /usr/lib64/liblzo2.so.2</div></pre></td></tr></table></figure>
<h2 id="2、导入hadoop依赖lzo的jar包"><a href="#2、导入hadoop依赖lzo的jar包" class="headerlink" title="2、导入hadoop依赖lzo的jar包"></a>2、导入hadoop依赖lzo的jar包</h2><p>下载hadoop-gpl-packaging-0.6.1-1.x86_64.rpm</p>
<p>在系统上安装:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rpm -ivh hadoop-gpl-packaging-0.6.1-1.x86_64.rpm</div></pre></td></tr></table></figure>
<p>将会在本地安装以下的目录文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">├── lib</div><div class="line">│   ├── cdh4.0.1</div><div class="line">│   │   └── elephant-bird-gerritjvv</div><div class="line">│   │       └── elephant-bird-2.0.5.jar</div><div class="line">│   ├── guava-12.0.jar</div><div class="line">│   ├── hadoop-lzo-0.4.17.jar</div><div class="line">│   ├── hadoop-lzo.jar -&gt; /opt/hadoopgpl/lib/hadoop-lzo-0.4.17.jar</div><div class="line">│   ├── pig-0.10.0</div><div class="line">│   │   ├── elephant-bird-gerritjvv</div><div class="line">│   │   │   └── elephant-bird-2.0.5.jar</div><div class="line">│   │   ├── elephant-bird.jar -&gt; /opt/hadoopgpl/lib/pig-0.10.0/elephant-bird-gerritjvv/elephant-bird-2.0.5.jar</div><div class="line">│   │   ├── udfs-0.3.1.jar</div><div class="line">│   │   └── udfs.jar -&gt; /opt/hadoopgpl/lib/pig-0.10.0/udfs-0.3.1.jar</div><div class="line">│   ├── pig-0.6.0</div><div class="line">│   │   └── elephant-bird-1.0.jar</div><div class="line">│   ├── pig-0.7.0</div><div class="line">│   │   └── elephant-bird-1.0.jar</div><div class="line">│   ├── pig-0.8.0</div><div class="line">│   │   ├── elephant-bird-dvryaboy</div><div class="line">│   │   │   └── elephant-bird-2.0.jar</div><div class="line">│   │   └── elephant-bird-gerritjvv</div><div class="line">│   │       └── elephant-bird-2.0.jar</div><div class="line">│   ├── protobuf-java-2.4.1.jar</div><div class="line">│   ├── slf4j-api-1.5.8.jar</div><div class="line">│   ├── slf4j-log4j12-1.5.10.jar</div><div class="line">│   └── yamlbeans-0.9.3.jar</div><div class="line">└── native</div><div class="line">    └── Linux-amd64-64</div><div class="line">        ├── libgplcompression.a</div><div class="line">        ├── libgplcompression.la</div><div class="line">        ├── libgplcompression.so</div><div class="line">        ├── libgplcompression.so.0</div><div class="line">        ├── libgplcompression.so.0.0.0</div><div class="line">        ├── LzoCompressor.lo</div><div class="line">        ├── LzoCompressor.o</div><div class="line">        ├── LzoDecompressor.lo</div><div class="line">        └── LzoDecompressor.o</div></pre></td></tr></table></figure>
<p>现在只需要 hadoop-lzo.jar，需要将这个jar包拷贝到$HADOOP_HOME/lib和$HBAS_HOME/lib下，需要集群中所有的节点都有这个包。</p>
<h2 id="3、hadoop调用系统库的jni库文件"><a href="#3、hadoop调用系统库的jni库文件" class="headerlink" title="3、hadoop调用系统库的jni库文件"></a>3、hadoop调用系统库的jni库文件</h2><p>在上面的Linux-amd64-64目录下，存放了需要的所有的JNI库文件，需要把这些库放到          </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$HADOOP_HOME/lib/native/Linux-amd64-64</div></pre></td></tr></table></figure>
<p>拷贝其中所有的libgplcompression.* 到</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$HBASE_HOME/lib/native/Linux-amd64-64</div></pre></td></tr></table></figure>
<p>注意，一般Hbase中不存在该目录，直接创建就可以了。</p>
<h2 id="4、需要配置中，引用相关的库文件"><a href="#4、需要配置中，引用相关的库文件" class="headerlink" title="4、需要配置中，引用相关的库文件"></a>4、需要配置中，引用相关的库文件</h2><p>需要修改3个hadoop的配置：</p>
<h3 id="core-site-xml-中增加："><a href="#core-site-xml-中增加：" class="headerlink" title="core-site.xml 中增加："></a>core-site.xml 中增加：</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codec.lzo.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<h3 id="mapred-site-xml-中增加："><a href="#mapred-site-xml-中增加：" class="headerlink" title="mapred-site.xml 中增加："></a>mapred-site.xml 中增加：</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.output.compress<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.child.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>-Djava.library.path=$&#123;HADOOP_HOME&#125;/lib/native/Linux-amd64-64<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.map.output.compression.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<h3 id="hadoop-env-sh-中增加："><a href="#hadoop-env-sh-中增加：" class="headerlink" title="hadoop-env.sh 中增加："></a>hadoop-env.sh 中增加：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/lib:$HBASE_HOME/lib:</div><div class="line"></div><div class="line">export JAVA_LIBRARY_PATH=$JAVA_LIBRARY_PATH:$HADOOP_HOME/lib/native/Linux-amd64-64:$HADOOP_HOME/lib/native:$HADOOP_HOME/lib</div></pre></td></tr></table></figure>
<h1 id="结果测试"><a href="#结果测试" class="headerlink" title="结果测试"></a>结果测试</h1><p>完成上述的4步配置后，需要重启集群。使用下面的方式对结果进行测试</p>
<h2 id="hadoop测试使用lzo的压缩："><a href="#hadoop测试使用lzo的压缩：" class="headerlink" title="hadoop测试使用lzo的压缩："></a>hadoop测试使用lzo的压缩：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop jar $&#123;HADOOP_HOME&#125;/contrib/streaming/hadoop-streaming-1.0.4.jar -input /in/part-00009 -output /testalzo -mapper cat -reducer cat -jobconf mapred.output.compress=true -jobconf mapred.output.compression.codec=org.apache.hadoop.io.compress.LzoCodec</div></pre></td></tr></table></figure>
<p>测试成功的话会生成/in/part-00009的压缩文件/testalzo/part-00000.lzo_deflate</p>
<h2 id="hbase测试使用lzo压缩："><a href="#hbase测试使用lzo压缩：" class="headerlink" title="hbase测试使用lzo压缩："></a>hbase测试使用lzo压缩：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hbase org.apache.hadoop.hbase.util.CompressionTest hdfs://hadoop131/terasort/output/part-00005 lzo</div></pre></td></tr></table></figure>
<p>测试成功的话会输出SUCCESS   </p>
<p>修改表压缩方式为lzo，接下来以usertable为例子进行</p>
<p>1、describe ‘usertable’ 查看原来的压缩方式，默认情况下的压缩方式 COMPRESSION =&gt; ‘NONE’</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">hbase(main):003:0&gt; describe &apos;usertable&apos; </div><div class="line">DESCRIPTION                                                       ENABLED                           </div><div class="line">&#123;NAME =&gt; &apos;usertable&apos;, FAMILIES =&gt; [&#123;NAME =&gt; &apos;cf&apos;, DATA_BLOCK_ENC true                              </div><div class="line">ODING =&gt; &apos;NONE&apos;, BLOOMFILTER =&gt; &apos;NONE&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;                                   </div><div class="line">, VERSIONS =&gt; &apos;3&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, T                                   </div><div class="line">TL =&gt; &apos;2147483647&apos;, KEEP_DELETED_CELLS =&gt; &apos;false&apos;, BLOCKSIZE =&gt;                                    </div><div class="line">&apos;65536&apos;, IN_MEMORY =&gt; &apos;false&apos;, ENCODE_ON_DISK =&gt; &apos;true&apos;, BLOCKCA                                   </div><div class="line">CHE =&gt; &apos;true&apos;&#125;]&#125;  </div><div class="line">&gt;disable &apos;usertable&apos;</div><div class="line">&gt;alter &apos;usertable&apos;,&#123;NAME=&gt;&apos;info&apos;,COMPRESSION=&gt;&apos;LZO&apos;&#125;</div><div class="line">&gt;enable &apos;TestTable&apos;</div><div class="line">&gt;compact &apos;TestTable&apos;</div></pre></td></tr></table></figure>
<p>测试结果：</p>
<p>先用自带的工具生成1G的测试数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hbase org.apache.hadoop.hbase.PerformanceEvaluation  sequentialWrite 1</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">1059592589  hdfs://hadoop131/hbase/TestTable</div><div class="line">409232235   hdfs://hadoop131/hbase/TestTable</div></pre></td></tr></table></figure>
<p>原来1G的数据被压缩到了400M，这个压缩效果和数据有关系，但是从网上的经验看，至少20%的压缩比还是能达到的。</p>
<h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><p>2014-10-24日增加:</p>
<p>升级为hadoop2.4.1版本，为新版本增加LZO的压缩：</p>
<p>1、编译本地的LZO库同上</p>
<p>2、编译Hadoop-LZO依赖：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">git init </div><div class="line"></div><div class="line">git clone https://github.com/twitter/hadoop-lzo</div><div class="line"></div><div class="line">export C_INCLUDE_PATH=/usr/local/lzo/include</div><div class="line"></div><div class="line">export LIBRARY_PATH=/usr/local/lzo/lib</div><div class="line"></div><div class="line">mvn clean package -Dmaven.test.skip=true</div></pre></td></tr></table></figure>
<p>3、拷贝相应的库和包到集群上：</p>
<p>需要拷贝的包括(如果需要HBase支持ZLO，需要把所有的包拷贝到hbase下的lib下，没有的话创建</p>
<p>所有的lzo依赖库，把所有/usr/local/lzo/lib下的库文件拷贝到集群上，且通过ldd的工具设置为系统库。</p>
<p>拷贝maven编译完成后的target/hadoop-lzo-0.4.20-SNAPSHOT.jar拷贝到hadoop的share/hadoop/common下</p>
<p>拷贝target/native/Linux-amd64-64/lib下的所有库文件到hadoop的lib/native/下</p>
<p>4、配置集群文件同上<br>5、重启集群<br>6、测试压缩是否可以用<br>在本地安装lzop的工具，然后随便找一个文件，执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">lzop -U -9 testFile</div></pre></td></tr></table></figure></p>
<p> 然后上传到HDFS上，如果直接用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop fs -cat</div></pre></td></tr></table></figure>
<p> 可以看到文件是乱码，但是用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hdfs dfs -text</div></pre></td></tr></table></figure>
<p> 看到的是非乱码就证明lzo正确安装了。</p>
<h1 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h1><p><a href="http://www.quora.com/How-do-I-install-LZO-compression-with-Hbase-and-Hadoop" target="_blank" rel="external">http://www.quora.com/How-do-I-install-LZO-compression-with-Hbase-and-Hadoop</a><br><a href="http://shitouer.cn/2013/01/hadoop-hbase-snappy-setup-final-tutorial/" target="_blank" rel="external">http://shitouer.cn/2013/01/hadoop-hbase-snappy-setup-final-tutorial/</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">aiping.lap</p>
              <p class="site-description motion-element" itemprop="description">aiping.liang s home</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">aiping.lap</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  








  





  

  

  

  

  

  

</body>
</html>
